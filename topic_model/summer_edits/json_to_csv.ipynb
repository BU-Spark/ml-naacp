{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "223eed56-3e12-4d71-aeb3-3029c12138e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = \"improve_model/openai_label_three_keywords.json\"\n",
    "\n",
    "# Specify the path to save the CSV file\n",
    "\n",
    "csv_file_path = \"improve_model/labels_3kw.csv\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract the data and write to CSV\n",
    "with open(csv_file_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = [\"topic_number\", \"Name\"]\n",
    "    max_keywords = 0  # Variable to track the maximum number of keywords\n",
    "    for topic_number, topic_data in data.items():\n",
    "        name = topic_data[\"Name\"]\n",
    "        openai_labels = topic_data[\"OpenAI_label\"].split(\",\")  # Split the OpenAI_Label by comma\n",
    "        num_keywords = len(openai_labels)  # Get the number of keywords for the current row\n",
    "        if num_keywords > max_keywords:\n",
    "            max_keywords = num_keywords\n",
    "        row = [topic_number, name] + openai_labels  # Append all the keywords to the row\n",
    "        writer.writerow(row)\n",
    "    \n",
    "    # Generate header for the keyword columns\n",
    "    header += [f\"OpenAI_Label_{i+1}\" for i in range(max_keywords)]\n",
    "    writer.writerow(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc4ff29a-2a91-4526-867e-8f96f20f48ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 5 fields in line 550, saw 6\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# OpenAI label value counts\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimprove_model/labels_3kw.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m target_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenAI_Label_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenAI_Label_2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenAI_Label_3\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Combine all keyword columns into a single Series\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/sparkgrp/dyxu/env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projectnb/sparkgrp/dyxu/env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projectnb/sparkgrp/dyxu/env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/projectnb/sparkgrp/dyxu/env/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m/projectnb/sparkgrp/dyxu/env/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:812\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/projectnb/sparkgrp/dyxu/env/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:873\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/projectnb/sparkgrp/dyxu/env/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:848\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/projectnb/sparkgrp/dyxu/env/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:859\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/projectnb/sparkgrp/dyxu/env/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:2025\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 5 fields in line 550, saw 6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# OpenAI label value counts\n",
    "\n",
    "df = pd.read_csv(\"improve_model/labels_3kw.csv\")\n",
    "\n",
    "target_columns = ['OpenAI_Label_1', 'OpenAI_Label_2', 'OpenAI_Label_3']\n",
    "# Combine all keyword columns into a single Series\n",
    "all_keywords = pd.concat([df[column] for column in target_columns]).dropna()\n",
    "\n",
    "# Compute the value counts for all keywords\n",
    "keyword_counts = all_keywords.value_counts()\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(16, 6))  # Adjust figure size as per your preference\n",
    "plt.bar(keyword_counts.index, keyword_counts.values)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Keyword')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Combined Keyword Frequency')\n",
    "\n",
    "# Rotate x-axis labels if needed\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Show the bar chart\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01f7a854-57f4-488a-b983-761c2effd822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of                 neighborhoods position_section            tracts  \\\n",
       "0                    [Fenway]        Education          [010300]   \n",
       "1     [Downtown, Beacon Hill]         Politics  [981700, 020302]   \n",
       "2      [Dorchester, Mattapan]         Politics  [100900, 100700]   \n",
       "3                  [Downtown]             News          [030302]   \n",
       "4             [Jamaica Plain]        Education          [120400]   \n",
       "...                       ...              ...               ...   \n",
       "3812                      NaN              NaN               NaN   \n",
       "3813                      NaN              NaN               NaN   \n",
       "3814                      NaN              NaN               NaN   \n",
       "3815                      NaN              NaN               NaN   \n",
       "3816                      NaN              NaN               NaN   \n",
       "\n",
       "                 author                                               body  \\\n",
       "0     Esteban Bustillos  Thomas White, a senior at Boston Latin School,...   \n",
       "1           Mike Deehan  A wave of blue votes could wash over the Massa...   \n",
       "2           Adam Reilly  It’s unlikely Donald Trump will win Massachuse...   \n",
       "3         Craig LeMoult  The state Department of Public Health released...   \n",
       "4        Kirk Carapezza  A couple of years ago, Daymian Mejia, a senior...   \n",
       "...                 ...                                                ...   \n",
       "3812                NaN                                                NaN   \n",
       "3813                NaN                                                NaN   \n",
       "3814                NaN                                                NaN   \n",
       "3815                NaN                                                NaN   \n",
       "3816                NaN                                                NaN   \n",
       "\n",
       "                                content_id  \\\n",
       "0     00000175-7583-d779-a575-779f0f6b0001   \n",
       "1     00000175-75fe-d5c8-a775-f7fe5a7f0001   \n",
       "2     00000175-7aad-d944-a9fd-7aed30970002   \n",
       "3     00000175-7b20-d944-a9fd-7be1d4bf0001   \n",
       "4     00000175-7b24-d5c8-a775-fb2c49a40001   \n",
       "...                                    ...   \n",
       "3812                                   NaN   \n",
       "3813                                   NaN   \n",
       "3814                                   NaN   \n",
       "3815                                   NaN   \n",
       "3816                                   NaN   \n",
       "\n",
       "                                                    hl1  \\\n",
       "0     For High School Athletes, The Pandemic Has Led...   \n",
       "1     Mass. Republicans Don't Fear Trump-Fueled Blue...   \n",
       "2     Trump Won't Win Boston — But He Might Win This...   \n",
       "3     Household 'Clusters' Are A Problem In Massachu...   \n",
       "4     'A Tint Over Everything': College Students Of ...   \n",
       "...                                                 ...   \n",
       "3812                                                NaN   \n",
       "3813                                                NaN   \n",
       "3814                                                NaN   \n",
       "3815                                                NaN   \n",
       "3816                                                NaN   \n",
       "\n",
       "                                                    hl2             pub_date  \\\n",
       "0     For High School Athletes, The Pandemic Has Led...  2020-11-11 00:00:00   \n",
       "1     Mass. Republicans Don't Fear Trump-Fueled Blue...  2020-11-03 00:00:00   \n",
       "2     Trump Won't Win Boston — But He Might Win This...  2020-11-02 00:00:00   \n",
       "3     Household 'Clusters' Are A Problem In Massachu...  2020-11-01 00:00:00   \n",
       "4     'A Tint Over Everything': College Students Of ...  2020-11-10 00:00:00   \n",
       "...                                                 ...                  ...   \n",
       "3812                                                NaN                  NaN   \n",
       "3813                                                NaN                  NaN   \n",
       "3814                                                NaN                  NaN   \n",
       "3815                                                NaN                  NaN   \n",
       "3816                                                NaN                  NaN   \n",
       "\n",
       "     pub_name                                               link  \n",
       "0         GBH  https://wgbh.org/news/education/2020/11/11/for...  \n",
       "1         GBH  https://wgbh.org/news/politics/2020/11/03/mass...  \n",
       "2         GBH  https://wgbh.org/news/politics/2020/11/02/trum...  \n",
       "3         GBH  https://wgbh.org/news/local-news/2020/11/01/ho...  \n",
       "4         GBH  https://wgbh.org/news/education/2020/11/10/a-t...  \n",
       "...       ...                                                ...  \n",
       "3812      NaN                                                NaN  \n",
       "3813      NaN                                                NaN  \n",
       "3814      NaN                                                NaN  \n",
       "3815      NaN                                                NaN  \n",
       "3816      NaN                                                NaN  \n",
       "\n",
       "[3817 rows x 11 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = \"GBH_SUMMER_DEMO_DATA.json\"\n",
    "\n",
    "df = pd.read_json(json_file_path, orient='columns')\n",
    "\n",
    "# Extract the information in the \"articles\" section and make keys in that dict the DataFrame's columns\n",
    "# articles_df = pd.json_normalize(df['articles'], 'neighborhoods', 'position_section', 'tracts', 'author', 'body', 'content_id', 'hl1', 'hl2', 'pub_date', 'pub_name', 'link')\n",
    "articles_df = pd.json_normalize(df['articles'])\n",
    "# df = pd.concat([df, articles_df], axis=1)\n",
    "# df.drop(columns=['articles'], inplace=True)\n",
    "\n",
    "articles_df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4b71789-6fdd-498c-b5fa-3a285b689c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df.dropna(axis=0)\n",
    "articles_df.to_csv(\"geocoded_articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a371dc73-036f-405e-b9c0-f93924a66670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper code to merge two dataframes \n",
    "import pandas as pd\n",
    "\n",
    "smaller_df = pd.read_csv(\"geocoded_articles.csv\")\n",
    "\n",
    "smaller_df.rename(columns={\"body\": \"Body\", \"content_id\": \"Tagging\"}, inplace=True)\n",
    "\n",
    "larger_df = pd.read_csv(\"/projectnb/sparkgrp/mvoong/gbh-rss-articles/Articles Nov 2020 - March 2023.csv\", usecols=['Tagging', 'Body'])\n",
    "\n",
    "# larger_df.describe\n",
    "merged_df = larger_df.merge(smaller_df[['Tagging', 'Body']], on='Tagging', how='left')[['Tagging', 'Body_x']]\n",
    "\n",
    "merged_df = merged_df.dropna(subset=['Body_x'])\n",
    "merged_df['Tagging'].iloc[240]\n",
    "merged_df.to_csv('large_gbh_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4efb17-ef28-41a5-a3e2-13654cab9893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
