{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60151c9e-f349-4d57-92d3-799134dd0f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import openai\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7486ed6-72b1-46b3-b823-dd39d8fa7d55",
   "metadata": {},
   "source": [
    "### A clean inference pipeline where all experiments are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09f49313-3242-4e54-9618-316cfbbdae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent cell to define embedding function, the embedding model is set to OpenAI's latest text-embedding-3-small by default\n",
    "\n",
    "openai.api_key =  # GitHub version does not include the OpenAI API key, please replace with your own key\n",
    "\n",
    "import openai\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "\n",
    "# Retry up to 6 times with exponential backoff, starting at 1 second and maxing out at 20 seconds delay\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def get_embedding(text: str, model='text-embedding-3-small') -> list[float]:\n",
    "    # print(text)\n",
    "    return openai.Embedding.create(input=text, model=model)[\"data\"][0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3931e85-fa58-46f0-a067-2cdbfac91bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text           race_label  \\\n",
      "0  It has been nearly three years since auto plan...  N/A - not specified   \n",
      "1  The Boston City Council Wednesday approved pro...  N/A - not specified   \n",
      "2  So far this year, 16 people have been murdered...  N/A - not specified   \n",
      "\n",
      "   race_discussed link  \n",
      "0               0  NaN  \n",
      "1               0  NaN  \n",
      "2               0  NaN  \n"
     ]
    }
   ],
   "source": [
    "# Load GBH articles\n",
    "unseen_articles = pd.read_csv('../datasets/combined_dataset_new_labels.csv')\n",
    "# unseen_articles = pd.read_csv('../datasets/Articles Nov 2020 - March 2023.csv', usecols=range(12))\n",
    "# unseen_articles = unseen_articles.dropna(subset=['Body'])\n",
    "# unseen_articles = unseen_articles.sample(n=5000, random_state=1)\n",
    "unseen_articles.reset_index(drop=True, inplace=True)\n",
    "print(unseen_articles.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac5ac023-7443-4c5f-bb26-7f1f559dcb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text           race_label  \\\n",
      "0  It has been nearly three years since auto plan...  N/A - not specified   \n",
      "1  The Boston City Council Wednesday approved pro...  N/A - not specified   \n",
      "2  So far this year, 16 people have been murdered...  N/A - not specified   \n",
      "\n",
      "   race_discussed link                                     processed_body  \\\n",
      "0               0  NaN  It has been nearly three years since auto plan...   \n",
      "1               0  NaN  The Boston City Council Wednesday approved pro...   \n",
      "2               0  NaN  So far this year, 16 people have been murdered...   \n",
      "\n",
      "                                       ada_embedding  \n",
      "0  [-0.017918458208441734, -0.022890731692314148,...  \n",
      "1  [0.028521470725536346, 0.002924327738583088, 0...  \n",
      "2  [0.048949964344501495, 0.06095076724886894, 0....  \n"
     ]
    }
   ],
   "source": [
    "# Updated token count & truncating preprocessing, using OpenAI's tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def truncate_to_max_tokens(string: str, encoding_name: str, max_tokens: int = 8192) -> str:\n",
    "    \"\"\"Truncate the string to not exceed max_tokens when encoded.\"\"\"\n",
    "    # Initially assume the full string is okay\n",
    "    end = len(string)\n",
    "    start = 0\n",
    "    while start < end:\n",
    "        # Check the middle of the current text portion\n",
    "        mid = (start + end) // 2\n",
    "        current_slice = string[:mid]\n",
    "        tokens_count = num_tokens_from_string(current_slice, encoding_name)\n",
    "        \n",
    "        if tokens_count > max_tokens:\n",
    "            # Too many tokens, need to reduce the size of the text\n",
    "            end = mid - 1\n",
    "        else:\n",
    "            # Not too many tokens, but can we include more?\n",
    "            if tokens_count == max_tokens or num_tokens_from_string(string[:mid + 1], encoding_name) > max_tokens:\n",
    "                return current_slice\n",
    "            else:\n",
    "                start = mid + 1\n",
    "\n",
    "    # In case the string is shorter than the max tokens or exactly max tokens, return the original string\n",
    "    return string\n",
    "\n",
    "encoding_name = \"cl100k_base\"\n",
    "\n",
    "unseen_articles['text'] = unseen_articles['text'].apply(lambda x: re.sub(re.compile('<.*?>'), '', x))\n",
    "unseen_articles['processed_body'] = unseen_articles['text'].apply(lambda x: truncate_to_max_tokens(x, \"cl100k_base\", 8192))\n",
    "unseen_articles = unseen_articles.dropna(subset=['processed_body'])\n",
    "unseen_articles['ada_embedding'] = unseen_articles['processed_body'].apply(lambda x: get_embedding(x))\n",
    "\n",
    "print(unseen_articles.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccb62aa2-3871-443d-8418-2dffc2f16193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative taxonomy: client's list of topics\n",
    "\n",
    "client_taxonomy_df = pd.read_csv('../datasets/asad_topic_list.csv', names=['label'])\n",
    "# client_taxonomy_df = pd.read_excel('../datasets/singers_topics_list.xlsx', names=['label'])\n",
    "client_taxonomy_df['ada_embedding'] = client_taxonomy_df['label'].map(get_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fd9348f-3372-4a14-9adc-8d0766d8bcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text           race_label  \\\n",
      "0  It has been nearly three years since auto plan...  N/A - not specified   \n",
      "1  The Boston City Council Wednesday approved pro...  N/A - not specified   \n",
      "2  So far this year, 16 people have been murdered...  N/A - not specified   \n",
      "3  Recently I spent the better part of two days i...  N/A - not specified   \n",
      "4  Communities across the nation are beginning th...                Black   \n",
      "5  When Carrie Kissell learned that her employer ...  N/A - not specified   \n",
      "6  Advisory: This story includes descriptions of ...                Black   \n",
      "7  On this week’s edition of the Joy Beat, theAll...  N/A - not specified   \n",
      "8  Nearly two years after its executive director ...  N/A - not specified   \n",
      "9  As a highly respected astronomer, Harvard prof...  N/A - not specified   \n",
      "\n",
      "   race_discussed link                                     processed_body  \\\n",
      "0               0  NaN  It has been nearly three years since auto plan...   \n",
      "1               0  NaN  The Boston City Council Wednesday approved pro...   \n",
      "2               0  NaN  So far this year, 16 people have been murdered...   \n",
      "3               0  NaN  Recently I spent the better part of two days i...   \n",
      "4               1  NaN  Communities across the nation are beginning th...   \n",
      "5               0  NaN  When Carrie Kissell learned that her employer ...   \n",
      "6               1  NaN  Advisory: This story includes descriptions of ...   \n",
      "7               0  NaN  On this week’s edition of the Joy Beat, theAll...   \n",
      "8               0  NaN  Nearly two years after its executive director ...   \n",
      "9               0  NaN  As a highly respected astronomer, Harvard prof...   \n",
      "\n",
      "                                       ada_embedding  \\\n",
      "0  [-0.017918458208441734, -0.022890731692314148,...   \n",
      "1  [0.028521470725536346, 0.002924327738583088, 0...   \n",
      "2  [0.048949964344501495, 0.06095076724886894, 0....   \n",
      "3  [-0.026381686329841614, 0.016754629090428352, ...   \n",
      "4  [0.04470303654670715, 0.0445072166621685, 0.09...   \n",
      "5  [0.000276861188467592, 0.017688486725091934, 0...   \n",
      "6  [0.04413130506873131, -0.0013868125388398767, ...   \n",
      "7  [0.06033196672797203, -0.038470253348350525, 0...   \n",
      "8  [0.01740531623363495, 0.017317336052656174, 0....   \n",
      "9  [0.026775673031806946, 0.021893411874771118, -...   \n",
      "\n",
      "                                closest_topic_client  \n",
      "0                         [Economy, Shopping, Taxes]  \n",
      "1  [Local Politics, Housing/Homelessness, Gentrif...  \n",
      "2                  [Guns, Housing/Homelessness, GBH]  \n",
      "3  [Addiction/Substance Use, Shopping, Small Busi...  \n",
      "4  [Equity & Justice, Civil Rights, Philanthropy/...  \n",
      "5  [Immigration, Housing/Homelessness, Labor/Work...  \n",
      "6           [Native Americans, Race, Local Politics]  \n",
      "7                                              Other  \n",
      "8      [Law Enforcement, Local Politics, Government]  \n",
      "9                                              Other  \n"
     ]
    }
   ],
   "source": [
    "# Find most similar taxonomy (out of client's list of topics) to news body\n",
    "\n",
    "# Column of embeddings to list\n",
    "\n",
    "client_topic_embedding_list = client_taxonomy_df['ada_embedding'].to_list()\n",
    "client_topic_embedding_list = client_taxonomy_df['ada_embedding'].to_list()\n",
    "client_topic_list = client_taxonomy_df['label'].to_list()\n",
    "similarity_arr = []\n",
    "\n",
    "closest_topic_list_client = []\n",
    "for index, row in unseen_articles.iterrows():\n",
    "    target_embedding = row['ada_embedding']\n",
    "    similarities = [cosine_similarity(np.array(target_embedding).reshape(1, -1), np.array(topic).reshape(1, -1))[0][0] for topic in client_topic_embedding_list]\n",
    "    \n",
    "    if max(similarities) > 0.2:    \n",
    "        # Find the index of the topic with the highest similarity\n",
    "        # closest_topic_index = np.argmax(similarities)\n",
    "        top3_indices = np.argsort(similarities)[-3:][::-1]  # This reverses the slice to ensure highest similarities are first\n",
    "\n",
    "        # Retrieve the closest topic embedding\n",
    "        # closest_topic = client_topic_list[closest_topic_index]\n",
    "        # closest_topic_list_client.append(closest_topic)\n",
    "        top3_topics = [client_topic_list[index] for index in top3_indices]\n",
    "        closest_topic_list_client.append(top3_topics)\n",
    "\n",
    "    else:\n",
    "        closest_topic_list_client.append('Other')\n",
    "    similarity_arr.append(max(similarities))\n",
    "    \n",
    "unseen_articles['closest_topic_client'] = closest_topic_list_client\n",
    "print(unseen_articles.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd36ebbe-569c-4a5c-afe0-b84d996e1cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs all columns but ada_embedding\n",
    "\n",
    "# unseen_articles.to_csv('../output/pure_embedding_all_gbh_5000.csv', columns=['Headline', 'Section', 'Body', 'closest_topic_all', 'closest_topic_selected', 'closest_topic_client'])\n",
    "columns_to_output = unseen_articles.columns.difference(['ada_embedding', 'Unnamed: 0', 'processed_body'])\n",
    "unseen_articles.to_csv('../output/pure_embedding_three_labels_anulika.csv', columns=columns_to_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}