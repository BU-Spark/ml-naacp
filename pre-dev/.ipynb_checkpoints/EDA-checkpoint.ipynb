{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f25bb486",
   "metadata": {},
   "source": [
    "Some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a40b169d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#doc2vec\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc2vec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc2Vec\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc2vec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TaggedDocument\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#wordnet\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim.models'"
     ]
    }
   ],
   "source": [
    "#pandas & numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "#spacy & nltk\n",
    "import spacy\n",
    "import nltk \n",
    "\n",
    "#doc2vec\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "#wordnet\n",
    "import wn\n",
    "#en = wn.Wordnet('oewn:2021') \n",
    "#wn.similarity.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae49f24",
   "metadata": {},
   "source": [
    "- Corpus $\\rightarrow$ Article\n",
    "- Article $\\rightarrow$ Summary?\n",
    "- Article $\\rightarrow$ doc2vec?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "607ebaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_path = \"topicdata/TBG_unique_raw.csv\"\n",
    "tag_path = \"topicdata/topictags.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bce037c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'topicdata/topictags.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tags \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.8/site-packages/pandas/io/excel/_base.py:457\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    456\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    460\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    462\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.8/site-packages/pandas/io/excel/_base.py:1376\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1374\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1376\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1381\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1382\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m         )\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.8/site-packages/pandas/io/excel/_base.py:1250\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1248\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[0;32m-> 1250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1252\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m   1253\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   1254\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.8/site-packages/pandas/io/common.py:795\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    787\u001b[0m             handle,\n\u001b[1;32m    788\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    791\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    798\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'topicdata/topictags.xlsx'"
     ]
    }
   ],
   "source": [
    "tags = pd.read_excel(tag_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a553cd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Addiction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aging in Prison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Public Safety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Architecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Arts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Biotech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Boston Marathon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bruins</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Addiction\n",
       "0            Aging \n",
       "1   Aging in Prison\n",
       "2            Health\n",
       "3     Public Safety\n",
       "4      Architecture\n",
       "5              Arts\n",
       "6           Biotech\n",
       "7             Books\n",
       "8   Boston Marathon\n",
       "9            Bruins"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53187e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pub_type</th>\n",
       "      <th>position_section</th>\n",
       "      <th>position_subsection</th>\n",
       "      <th>hl1</th>\n",
       "      <th>hl2</th>\n",
       "      <th>author</th>\n",
       "      <th>lede</th>\n",
       "      <th>body</th>\n",
       "      <th>language</th>\n",
       "      <th>word_count</th>\n",
       "      <th>copyright</th>\n",
       "      <th>content-id</th>\n",
       "      <th>volume</th>\n",
       "      <th>issue_number</th>\n",
       "      <th>edition</th>\n",
       "      <th>pub_name</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>licensor_indexing_terms</th>\n",
       "      <th>indexing_terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5863</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SPORTS; Pg. E16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O'Brien not enough as HC bows to BC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WORCESTER - Amy O'Brien did everything she c...</td>\n",
       "      <td>['ENGLISH']</td>\n",
       "      <td>216.0</td>\n",
       "      <td>Copyright 1998 Globe Newspaper Company</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>,City Edition</td>\n",
       "      <td>The Boston Globe</td>\n",
       "      <td>{'month': '11', 'day': '20', 'year': '1998'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'subject': [{'score': '77', 'classCode': 'ST0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9061</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NATIONAL/FOREIGN; Pg. A11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15,000-strong media force seeks battle;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It's too hot outside. It's too cold inside. Th...</td>\n",
       "      <td>Ted Koppel set the tone in San Diego, stomping...</td>\n",
       "      <td>['ENGLISH']</td>\n",
       "      <td>713.0</td>\n",
       "      <td>Copyright 1996 Globe Newspaper Company</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Boston Globe</td>\n",
       "      <td>{'month': '08', 'day': '27', 'year': '1996'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'legal': [{'className': 'Transportation Law',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4637</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ECONOMY; Pg. E2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Region;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The CML Group, maker of the NordicTrack exer...</td>\n",
       "      <td>['ENGLISH']</td>\n",
       "      <td>1048.0</td>\n",
       "      <td>Copyright 1998 Globe Newspaper Company</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>,City Edition</td>\n",
       "      <td>The Boston Globe</td>\n",
       "      <td>{'month': '11', 'day': '08', 'year': '1998'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'subject': [{'score': '92', 'classCode': 'STX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>NaN</td>\n",
       "      <td>METRO/REGION; Pg. B4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 killed, 6 hurt in Methuen crash;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>METHUEN - One person is dead and six others ...</td>\n",
       "      <td>['ENGLISH']</td>\n",
       "      <td>83.0</td>\n",
       "      <td>Copyright 1998 Globe Newspaper Company</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>,City Edition</td>\n",
       "      <td>The Boston Globe</td>\n",
       "      <td>{'month': '10', 'day': '21', 'year': '1998'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'subject': [{'score': '88', 'classCode': 'STX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8462</th>\n",
       "      <td>Newspaper, Newspapers</td>\n",
       "      <td>LIVING ARTS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Inspired by MFA visit, this entrepreneur makes...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Artwork for JIGGY Puzzles' summer collection, ...</td>\n",
       "      <td>BODY The experience inspired Marcotte to found...</td>\n",
       "      <td>['ENGLISH']</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>Copyright 2021 Globe Newspaper Company All Rig...</td>\n",
       "      <td>BGLOBE-7da1e4ced77c11ebabb88a6787fac1dc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Boston Globe</td>\n",
       "      <td>{'day': '28', 'month': '06', 'year': '2021'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'subject': [{'score': '91', 'classCode': 'ST0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   pub_type           position_section position_subsection  \\\n",
       "5863                    NaN            SPORTS; Pg. E16                 NaN   \n",
       "9061                    NaN  NATIONAL/FOREIGN; Pg. A11                 NaN   \n",
       "4637                    NaN            ECONOMY; Pg. E2                 NaN   \n",
       "1052                    NaN       METRO/REGION; Pg. B4                 NaN   \n",
       "8462  Newspaper, Newspapers                LIVING ARTS                 NaN   \n",
       "\n",
       "                                                    hl1  hl2 author  \\\n",
       "5863                O'Brien not enough as HC bows to BC  NaN    NaN   \n",
       "9061            15,000-strong media force seeks battle;  NaN    NaN   \n",
       "4637                                        The Region;  NaN    NaN   \n",
       "1052                 1 killed, 6 hurt in Methuen crash;  NaN    NaN   \n",
       "8462  Inspired by MFA visit, this entrepreneur makes...  NaN    NaN   \n",
       "\n",
       "                                                   lede  \\\n",
       "5863                                                NaN   \n",
       "9061  It's too hot outside. It's too cold inside. Th...   \n",
       "4637                                                NaN   \n",
       "1052                                                NaN   \n",
       "8462  Artwork for JIGGY Puzzles' summer collection, ...   \n",
       "\n",
       "                                                   body     language  \\\n",
       "5863    WORCESTER - Amy O'Brien did everything she c...  ['ENGLISH']   \n",
       "9061  Ted Koppel set the tone in San Diego, stomping...  ['ENGLISH']   \n",
       "4637    The CML Group, maker of the NordicTrack exer...  ['ENGLISH']   \n",
       "1052    METHUEN - One person is dead and six others ...  ['ENGLISH']   \n",
       "8462  BODY The experience inspired Marcotte to found...  ['ENGLISH']   \n",
       "\n",
       "      word_count                                          copyright  \\\n",
       "5863       216.0             Copyright 1998 Globe Newspaper Company   \n",
       "9061       713.0             Copyright 1996 Globe Newspaper Company   \n",
       "4637      1048.0             Copyright 1998 Globe Newspaper Company   \n",
       "1052        83.0             Copyright 1998 Globe Newspaper Company   \n",
       "8462      1130.0  Copyright 2021 Globe Newspaper Company All Rig...   \n",
       "\n",
       "                                   content-id  volume  issue_number  \\\n",
       "5863                                      NaN     NaN           NaN   \n",
       "9061                                      NaN     NaN           NaN   \n",
       "4637                                      NaN     NaN           NaN   \n",
       "1052                                      NaN     NaN           NaN   \n",
       "8462  BGLOBE-7da1e4ced77c11ebabb88a6787fac1dc     NaN           NaN   \n",
       "\n",
       "            edition          pub_name  \\\n",
       "5863  ,City Edition  The Boston Globe   \n",
       "9061            NaN  The Boston Globe   \n",
       "4637  ,City Edition  The Boston Globe   \n",
       "1052  ,City Edition  The Boston Globe   \n",
       "8462            NaN  The Boston Globe   \n",
       "\n",
       "                                          pub_date licensor_indexing_terms  \\\n",
       "5863  {'month': '11', 'day': '20', 'year': '1998'}                     NaN   \n",
       "9061  {'month': '08', 'day': '27', 'year': '1996'}                     NaN   \n",
       "4637  {'month': '11', 'day': '08', 'year': '1998'}                     NaN   \n",
       "1052  {'month': '10', 'day': '21', 'year': '1998'}                     NaN   \n",
       "8462  {'day': '28', 'month': '06', 'year': '2021'}                     NaN   \n",
       "\n",
       "                                         indexing_terms  \n",
       "5863  {'subject': [{'score': '77', 'classCode': 'ST0...  \n",
       "9061  {'legal': [{'className': 'Transportation Law',...  \n",
       "4637  {'subject': [{'score': '92', 'classCode': 'STX...  \n",
       "1052  {'subject': [{'score': '88', 'classCode': 'STX...  \n",
       "8462  {'subject': [{'score': '91', 'classCode': 'ST0...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get 10k rows from .csv\n",
    "#total of 164 chunks\n",
    "#if r!=None \n",
    "import sys\n",
    "import csv\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "def get_ten_k(nth, large_path):\n",
    "    chunksize = 10 ** 4\n",
    "    chunk_count = 0\n",
    "    for chunk in pd.read_csv(large_path, chunksize=chunksize):\n",
    "        if(chunk_count==nth):\n",
    "            #return(chunk)\n",
    "            #print(chunk_count)\n",
    "            chunk_count+=1\n",
    "            pass\n",
    "        else:\n",
    "            chunk_count+=1\n",
    "        print(chunk_count)\n",
    "\n",
    "# parameters: number of chunks, sample size in rows\n",
    "def random_sample_corpus(chunkmax, samplesize):\n",
    "    from_each_row = int(samplesize/chunkmax)\n",
    "    chunksize = 10 ** 4\n",
    "    chunk_count = 0\n",
    "    for chunk in pd.read_csv(large_path, chunksize=chunksize):\n",
    "        if(chunk_count == 0):\n",
    "            s = chunk.sample(n = from_each_row)\n",
    "            chunk_count += 1\n",
    "        else:\n",
    "            s2 = chunk.sample(n = from_each_row)\n",
    "            s = pd.concat([s, s2])\n",
    "            chunk_count+=1\n",
    "    return(s)\n",
    "    \n",
    "sample = random_sample_corpus(164, 15000)\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea69bf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from time import sleep\n",
    "import sys\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "#least efficient code ever written\n",
    "def mask_non_entity(corpus, field):\n",
    "    nlp = spacy.load(\"en_core_web_trf\")\n",
    "    corpusentified = []\n",
    "    count = 0\n",
    "    with alive_bar(corpus.shape[0], force_tty=True, title=f'Entifying...') as bar:\n",
    "        for index, row in corpus.iterrows():\n",
    "\n",
    "                row[field] = str(row[field])\n",
    "                doc = nlp(str(row[field]))\n",
    "                newstring = \"\"\n",
    "                entities = []\n",
    "                for ent in doc.ents:\n",
    "                    #print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "                    entities.append(ent.text)\n",
    "                corpusentified.append(\" \".join(entities))\n",
    "\n",
    "                #print(str(count) + \" of \" + str(corpus.shape[0]))\n",
    "                count+=1\n",
    "                bar()\n",
    "    return(corpusentified)\n",
    "        #print(row[field])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc86f2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on 0: /home/aidan/.local/lib/python3.8/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "        warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on 290: Token indices sequence length is longer than the specified maximum sequence length for this model (647 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entifying... |███████████████▋⚠︎                       | (!) 5821/14924 [39%] in 37:38.6 (2.58/s)                        \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample2 \u001b[38;5;241m=\u001b[39m \u001b[43mmask_non_entity\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbody\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m sample2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(sample2)\n\u001b[1;32m      3\u001b[0m sample2\u001b[38;5;241m.\u001b[39mrename(columns \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m,}, inplace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mmask_non_entity\u001b[0;34m(corpus, field)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m corpus\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     14\u001b[0m         row[field] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(row[field])\n\u001b[0;32m---> 15\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m         newstring \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m         entities \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/language.py:1014\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1014\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/spacy_transformers/pipeline_component.py:192\u001b[0m, in \u001b[0;36mTransformer.__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m\"\"\"Apply the pipe to one document. The document is modified in place,\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03mand returned. This usually happens under the hood when the nlp object\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03mis called on a text and all components are applied to the Doc.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mDOCS: https://spacy.io/api/transformer#call\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m install_extensions()\n\u001b[0;32m--> 192\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_annotations([doc], outputs)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/spacy_transformers/pipeline_component.py:228\u001b[0m, in \u001b[0;36mTransformer.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    226\u001b[0m     activations \u001b[38;5;241m=\u001b[39m FullTransformerBatch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(docs))\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 228\u001b[0m     activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m batch_id \u001b[38;5;241m=\u001b[39m TransformerListener\u001b[38;5;241m.\u001b[39mget_batch_id(docs)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m listener \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlisteners:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/spacy_transformers/layers/transformer_model.py:185\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, docs, is_train)\u001b[0m\n\u001b[1;32m    181\u001b[0m align \u001b[38;5;241m=\u001b[39m get_alignment(flat_spans, wordpieces\u001b[38;5;241m.\u001b[39mstrings, tokenizer\u001b[38;5;241m.\u001b[39mall_special_tokens)\n\u001b[1;32m    182\u001b[0m wordpieces, align \u001b[38;5;241m=\u001b[39m truncate_oversize_splits(\n\u001b[1;32m    183\u001b[0m     wordpieces, align, tokenizer\u001b[38;5;241m.\u001b[39mmodel_max_length\n\u001b[1;32m    184\u001b[0m )\n\u001b[0;32m--> 185\u001b[0m model_output, bp_tensors \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwordpieces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogger\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mattrs:\n\u001b[1;32m    187\u001b[0m     log_gpu_memory(model\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogger\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter forward\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/layers/pytorchwrapper.py:134\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m    131\u001b[0m convert_outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    133\u001b[0m Xtorch, get_dX \u001b[38;5;241m=\u001b[39m convert_inputs(model, X, is_train)\n\u001b[0;32m--> 134\u001b[0m Ytorch, torch_backprop \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshims\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtorch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m Y, get_dYtorch \u001b[38;5;241m=\u001b[39m convert_outputs(model, (X, Ytorch), is_train)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dY: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/shims/pytorch.py:56\u001b[0m, in \u001b[0;36mPyTorchShim.__call__\u001b[0;34m(self, inputs, is_train)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbegin_update(inputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mlambda\u001b[39;00m a: \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/shims/pytorch.py:66\u001b[0m, in \u001b[0;36mPyTorchShim.predict\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mixed_precision):\n\u001b[0;32m---> 66\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:853\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    844\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    846\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    847\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    848\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    851\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    852\u001b[0m )\n\u001b[0;32m--> 853\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    866\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:526\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    517\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    518\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    519\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 526\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:412\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    402\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    409\u001b[0m ):\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    411\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:339\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    331\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    338\u001b[0m ):\n\u001b[0;32m--> 339\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    349\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:225\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    223\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m1\u001b[39m], value_layer], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    226\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(hidden_states))\n\u001b[1;32m    228\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample2 = mask_non_entity(sample, 'body')\n",
    "sample2 = pd.DataFrame(sample2)\n",
    "sample2.rename(columns = {0:'body',}, inplace = True)\n",
    "\n",
    "sample2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c4984",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2['indexing_terms'] = sample['indexing_terms']\n",
    "sample2['hl1'] = sample['hl1']\n",
    "sample2.to_csv(\"entities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0811b66",
   "metadata": {},
   "source": [
    "## doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "148ac864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tag count:  1379\n",
      "Total tag count:  8956\n",
      "Text Processing Exceptions:  527\n",
      "Unique tag count:  1380\n",
      "Total tag count:  9010\n",
      "Text Processing Exceptions:  4\n",
      "7586\n",
      "1897\n",
      "TaggedDocument<['gstaad', 'switzerland', 'to', 'make', 'matters', 'more', 'interesting', 'on', 'this', 'my', 'annual', 'visit', 'to', 'the', 'alps', 'wasn', 'planning', 'to', 'just', 'ski', 'down', 'the', 'mountains', 'like', 'any', 'tom', 'dick', 'or', 'herminator', 'but', 'to', 'fly', 'over', 'them', 'like', 'soap', 'bubble', 'over', 'picket', 'fence', 'mean', 'fly', 'in', 'hot', 'air', 'balloon', 'just', 'as', 'did', 'the', 'first', 'passengers', 'in', 'montgolfier', 'balloon', 'in', 'of', 'those', 'passengers', 'sheep', 'rooster', 'and', 'duck', 'only', 'the', 'duck', 'could', 'fly', 'without', 'assistance', 'but', 'all', 'of', 'them', 'made', 'it', 'why', 'not', 'try', 'it', 'myself', 'so', 'my', 'wife', 'julie', 'and', 'came', 'here', 'to', 'southwest', 'switzerland', 'where', 'she', 'could', 'ski', 'and', 'could', 'fly', 'in', 'the', 'annual', 'hot', 'air', 'balloon', 'festival'], MOUNTAINS>\n"
     ]
    }
   ],
   "source": [
    "#get cores\n",
    "import multiprocessing\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "cores = multiprocessing.cpu_count()\n",
    "import ast\n",
    "taglist = []\n",
    "\n",
    "\n",
    "def read_corpus(pdf, tokens_only=False):\n",
    "    it=0\n",
    "    exception_count = 0\n",
    "    for article in pdf['body'].values:\n",
    "        #print('\\n')\n",
    "        #print(pdf.iloc[it]['body'])\n",
    "        #print('equals?')\n",
    "        #print(article)\n",
    "        #print(\"\\n\")\n",
    "        tagging = pdf.iloc[it]['indexing_terms']\n",
    "        #print(tagging)\n",
    "        if(isinstance(article, str) and isinstance(tagging, str)):\n",
    "            if(\";\" in tagging):\n",
    "                tagging = tagging.split(\";\")[0]\n",
    "                #print(tagging)\n",
    "            try:\n",
    "                tagging = tagging.replace(\"'\", '\"')\n",
    "                y = json.loads(tagging)\n",
    "                y = ast.literal_eval(tagging)\n",
    "                if('subject' in y):\n",
    "                    yt = y['subject'][0]['className']\n",
    "                elif('legal' in y):\n",
    "                    yt = y['legal'][0]['className']\n",
    "                tagging = yt\n",
    "                taglist.append(tagging)\n",
    "            except Exception as inst:\n",
    "                exception_count +=1\n",
    "                \n",
    "\n",
    "                #print(y)\n",
    "        \n",
    "                tagging = \"-\"\n",
    "                #print(inst)\n",
    "            tokens = gensim.utils.simple_preprocess(article)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "             \n",
    "            else:\n",
    "                #print(tagging)\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, tagging)\n",
    "        else:\n",
    "            pass\n",
    "        it+=1\n",
    "    \n",
    "    print(\"Unique tag count: \", len(set(taglist)))\n",
    "    print(\"Total tag count: \", len(taglist))\n",
    "    print(\"Text Processing Exceptions: \", exception_count)\n",
    "    #print(taglist[10],taglist[0],taglist[59])\n",
    "    #395 tags\n",
    "\n",
    "corpus = list(read_corpus(sample, tokens_only=False))\n",
    "\n",
    "#                                                                              |\n",
    "#THAT IS WHY THE ACCURACY WAS SO HIGH! THIS IS JUST MAKING AN IDENTICAL CORPUS v\n",
    "\n",
    "\n",
    "#test_corpus = list(read_corpus(sample, tokens_only=False))\n",
    "\n",
    "\n",
    "#bugfix!\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_corpus, test_corpus = train_test_split(corpus, test_size=0.2)\n",
    "\n",
    "corpus_masked = list(read_corpus(sample2, tokens_only=False))\n",
    "\n",
    "#test_corpus_masked = list(read_corpus(sample2, tokens_only=False))\n",
    "train_corpus_masked, test_corpus_masked = train_test_split(corpus_masked, test_size=0.2)\n",
    "\n",
    "\n",
    "import pickle \n",
    "with open('sample_test.pkl', 'wb') as f:\n",
    "    pickle.dump(test_corpus, f)\n",
    "with open('sample_train.pkl', 'wb') as f:\n",
    "    pickle.dump(train_corpus, f)\n",
    "#load doc2vec model\n",
    "#dm: distributed bag of words flag\n",
    "#vector_size: # of features\n",
    "#negative: # of noise words\n",
    "#hs: sampling\n",
    "print(len(train_corpus))\n",
    "print(len(test_corpus))\n",
    "print(test_corpus[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93d8fcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Unmasked training complete -\n",
      "- Masked training complete -\n"
     ]
    }
   ],
   "source": [
    "d2v_model = Doc2Vec(vector_size=50, min_count=1, epochs=40)\n",
    "d2v_model.build_vocab(train_corpus)\n",
    "d2v_model.train(train_corpus, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)\n",
    "print(\"- Unmasked training complete -\")\n",
    "d2v_model_masked = Doc2Vec(vector_size=10, min_count=1, epochs=40)\n",
    "d2v_model_masked.build_vocab(train_corpus_masked)\n",
    "d2v_model_masked.train(train_corpus_masked, total_examples=d2v_model_masked.corpus_count, epochs=d2v_model_masked.epochs)\n",
    "print(\"- Masked training complete -\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9d556f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model.save(\"trained121\")\n",
    "d2v_model_masked.save(\"trainednotactuallymasked121\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c8114ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating feature vectors...\n",
      "Creating feature vectors...\n",
      "Creating feature vectors...\n",
      "Creating feature vectors...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import utils\n",
    "\n",
    "def vector_for_learning(model, input_docs):\n",
    "    print(\"Creating feature vectors...\")\n",
    "    sents = input_docs\n",
    "    targets, feature_vectors = zip(*[(doc.tags, model.infer_vector(doc.words)) for doc in sents])\n",
    "    return targets, feature_vectors\n",
    "\n",
    "y_train, X_train = vector_for_learning(d2v_model, train_corpus)\n",
    "y_test, X_test = vector_for_learning(d2v_model, test_corpus)\n",
    "\n",
    "\n",
    "y_train_masked, X_train_masked = vector_for_learning(d2v_model_masked, train_corpus_masked)\n",
    "y_test_masked, X_test_masked = vector_for_learning(d2v_model_masked, test_corpus_masked)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7856007c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fitting logistic regression -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        65331     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  5.42808D+04    |proj g|=  1.29301D+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  1.52073D+04    |proj g|=  3.44122D+01\n",
      "\n",
      "At iterate  100    f=  1.45561D+04    |proj g|=  1.80426D+01\n",
      "\n",
      "At iterate  150    f=  1.43416D+04    |proj g|=  9.39923D+00\n",
      "\n",
      "At iterate  200    f=  1.42655D+04    |proj g|=  1.30146D+01\n",
      "\n",
      "At iterate  250    f=  1.42326D+04    |proj g|=  2.31312D+00\n",
      "\n",
      "At iterate  300    f=  1.42173D+04    |proj g|=  4.78064D+00\n",
      "\n",
      "At iterate  350    f=  1.42110D+04    |proj g|=  1.49573D+00\n",
      "\n",
      "At iterate  400    f=  1.42081D+04    |proj g|=  4.20430D-01\n",
      "\n",
      "At iterate  450    f=  1.42068D+04    |proj g|=  5.98866D-01\n",
      "\n",
      "At iterate  500    f=  1.42061D+04    |proj g|=  2.20801D-01\n",
      "\n",
      "At iterate  550    f=  1.42058D+04    |proj g|=  6.31400D-01\n",
      "\n",
      "At iterate  600    f=  1.42056D+04    |proj g|=  2.54564D-01\n",
      "\n",
      "At iterate  650    f=  1.42055D+04    |proj g|=  1.34859D-01\n",
      "\n",
      "At iterate  700    f=  1.42055D+04    |proj g|=  1.22041D-01\n",
      "\n",
      "At iterate  750    f=  1.42054D+04    |proj g|=  5.93569D-02\n",
      "\n",
      "At iterate  800    f=  1.42054D+04    |proj g|=  1.23021D-01\n",
      "\n",
      "At iterate  850    f=  1.42054D+04    |proj g|=  8.41105D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "65331    862    905      1     0     0   1.624D-01   1.421D+04\n",
      "  F =   14205.413597570259     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          451     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.70824D+02    |proj g|=  1.14833D+01\n",
      "\n",
      "At iterate   50    f=  1.50437D+02    |proj g|=  1.55041D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  451     87     96      1     0     0   3.669D-03   1.504D+02\n",
      "  F =   150.43560987877115     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      " - Predicting test values -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 50 features, but LogisticRegression is expecting 10 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [92]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - Predicting test values -\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m logreg\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m----> 8\u001b[0m y_pred_masked \u001b[38;5;241m=\u001b[39m \u001b[43mlogreg_masked\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTesting accuracy for tag: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat( accuracy_score(y_test, y_pred)))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTesting F1 score for tag: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(f1_score(y_test, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/linear_model/_base.py:425\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;124;03m    Predict class labels for samples in X.\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m        Predicted class label per sample.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    427\u001b[0m         indices \u001b[38;5;241m=\u001b[39m (scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/linear_model/_base.py:407\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03mPredict confidence scores for samples.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;124;03m    class would be predicted.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    405\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 407\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mravel() \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py:580\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    577\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 580\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py:395\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 395\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    398\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 50 features, but LogisticRegression is expecting 10 features as input."
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=0.5, max_iter=1000, verbose=1)\n",
    "logreg_masked = LogisticRegression(n_jobs=1, C=0.5, max_iter=1000, verbose=1)\n",
    "print(\"- Fitting logistic regression -\")\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg_masked.fit(X_train_masked, y_train_masked)\n",
    "print(\" - Predicting test values -\")\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_pred_masked = logreg_masked.predict(X_test)\n",
    "print('Testing accuracy for tag: {}'.format( accuracy_score(y_test, y_pred)))\n",
    "print('Testing F1 score for tag: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "\n",
    "#[key for key in model.dv.index_to_key if type(key) is not int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6776543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = d2v_model.infer_vector(['the', 'election', 'results', 'will', 'be', 'returned', 'tomorrow', 'afternoon', 'with', 'the', 'incumbent', 'expected'])\n",
    "print(logreg.predict([d2v_model.infer_vector(['the', 'election', 'results', 'will', 'be', 'returned', 'tomorrow', 'afternoon', 'with', 'the', 'incumbent', 'expected'])]))\n",
    "print(d2v_model.dv.similar_by_vector(v, 1)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0bda33",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3b852e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- fitting unmasked SVM -\n",
      "- fitting masked SVM -\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(decision_function_shape='ovo')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "print(\"- fitting unmasked SVM -\")\n",
    "clf.fit(X_train, y_train)\n",
    "clf2 = svm.SVC(decision_function_shape='ovo')\n",
    "print(\"- fitting masked SVM -\")\n",
    "clf2.fit(X_train_masked, y_train_masked)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb3a406e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "[-2.7968981  -0.9605918  -0.94964266 -0.90632695 -1.7649145   0.8024218\n",
      "  0.45919192 -0.60518366 -0.33086327  0.45882818  2.6214075   1.9308689\n",
      " -0.29829636  1.398833    0.4059418   0.25561228 -0.36959714  0.1431916\n",
      "  0.3989197  -0.25259843 -2.0321      0.8634097   0.32008106 -1.1782335\n",
      "  1.4668249  -0.6557659   1.1808115  -0.00587212 -1.5586793  -0.11530534\n",
      "  0.729539    2.096556    0.41142276  1.7147954   1.3945565   1.3192946\n",
      " -1.5917963  -1.157754    0.1122666  -0.5435854  -1.0736178  -1.4485372\n",
      " -2.367786    0.54825896 -0.25985727  0.624689    0.71359086  0.329459\n",
      "  0.22761214  1.6441754 ]\n",
      "(Unmasked) Predicted:  STADIUMS & ARENAS  Actual:  MOUNTAINS\n",
      "(Masked) Predicted:  LEGISLATIVE BODIES  Actual:  US REPUBLICAN PARTY\n",
      "------\n",
      "\n",
      "------\n",
      "[-1.795023   -1.9685853  -0.6079043  -1.1959746  -1.0177357   0.8293192\n",
      " -0.4036998  -0.1717674  -0.72922534  0.11506678  0.5380773   0.10588539\n",
      "  0.3361628   1.2624158   0.12865584  0.08584485  1.0957191   0.22013864\n",
      " -0.1595036   0.19663064 -0.6183021  -0.5584727  -0.31494346  0.5359567\n",
      "  0.57276416 -0.4690684  -0.4469947   1.0947863  -1.4567444  -0.9114281\n",
      "  0.6554227   1.9480485   0.284137    1.0638318   0.2702168   1.6981905\n",
      "  0.11986572 -1.2636449  -0.19723356 -0.14291511 -0.630183    0.6522912\n",
      " -0.688769   -0.4836162   0.7363817  -0.22837047  0.01451954 -0.0302165\n",
      "  1.4851688   0.56832117]\n",
      "(Unmasked) Predicted:  DEATHS & OBITUARIES  Actual:  WORLD WAR I\n",
      "(Masked) Predicted:  LEGISLATIVE BODIES  Actual:  -\n",
      "------\n",
      "\n",
      "------\n",
      "[-0.62451124 -0.8649864  -0.36242357 -0.14570282 -0.649379    0.30302012\n",
      " -0.01183496 -0.13746627 -0.2589482  -0.16056806  0.5680221   0.5833338\n",
      "  0.17179407  0.82422495  0.2574261  -0.5913952   0.9192381   0.47070163\n",
      "  0.26918364  0.29985324 -0.24815775 -0.36418015 -0.12453361  0.4369237\n",
      "  0.55930555 -0.30777133 -0.22532459  0.67557985 -0.6222145  -0.5861917\n",
      "  0.54335     0.89455277 -0.12011968  0.6964344   0.05202919  1.0553117\n",
      " -0.24814956 -0.6514895   0.06445172  0.30702984 -0.52561295  0.36762598\n",
      " -0.8086055  -0.33380333  0.86930275 -0.07545679  0.34080672  0.16081126\n",
      "  1.0082384   0.40927953]\n",
      "(Unmasked) Predicted:  DEATHS & OBITUARIES  Actual:  DEATHS & OBITUARIES\n",
      "(Masked) Predicted:  LEGISLATIVE BODIES  Actual:  US Selective Service System\n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred_masked = clf2.predict(X_test_masked)\n",
    "for y_i in range(0, 3):\n",
    "    print(\"------\")\n",
    "    print(X_test[y_i])\n",
    "    print(\"(Unmasked) Predicted: \", y_pred[y_i], \" Actual: \", y_test[y_i])\n",
    "    print(\"(Masked) Predicted: \", y_pred_masked[y_i], \" Actual: \", y_test_masked[y_i])\n",
    "    print(\"------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c298cfec",
   "metadata": {},
   "source": [
    "# Dimensionality Reduced SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a1a1b3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=3, n_iter=100, random_state=42)\n",
    "svd_unmasked = TruncatedSVD(n_components=3, n_iter=100, random_state=42)\n",
    "\n",
    "dimredx = svd.fit_transform(X_train)\n",
    "dimredx_masked = svd.fit_transform(X_train_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5b005838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- fitting unmasked SVM -\n",
      "- fitting masked SVM -\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(decision_function_shape='ovo')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "print(\"- fitting unmasked SVM -\")\n",
    "clf.fit(dimredx, y_train)\n",
    "clf2 = svm.SVC(decision_function_shape='ovo')\n",
    "print(\"- fitting masked SVM -\")\n",
    "clf2.fit(dimredx_masked, y_train_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "22a4988b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "[-2.7968981  -0.9605918  -0.94964266 -0.90632695 -1.7649145   0.8024218\n",
      "  0.45919192 -0.60518366 -0.33086327  0.45882818  2.6214075   1.9308689\n",
      " -0.29829636  1.398833    0.4059418   0.25561228 -0.36959714  0.1431916\n",
      "  0.3989197  -0.25259843 -2.0321      0.8634097   0.32008106 -1.1782335\n",
      "  1.4668249  -0.6557659   1.1808115  -0.00587212 -1.5586793  -0.11530534\n",
      "  0.729539    2.096556    0.41142276  1.7147954   1.3945565   1.3192946\n",
      " -1.5917963  -1.157754    0.1122666  -0.5435854  -1.0736178  -1.4485372\n",
      " -2.367786    0.54825896 -0.25985727  0.624689    0.71359086  0.329459\n",
      "  0.22761214  1.6441754 ]\n",
      "(Unmasked) Predicted:  FILM  Actual:  MOUNTAINS\n",
      "(Masked) Predicted:  LEGISLATIVE BODIES  Actual:  US REPUBLICAN PARTY\n",
      "------\n",
      "\n",
      "------\n",
      "[-1.795023   -1.9685853  -0.6079043  -1.1959746  -1.0177357   0.8293192\n",
      " -0.4036998  -0.1717674  -0.72922534  0.11506678  0.5380773   0.10588539\n",
      "  0.3361628   1.2624158   0.12865584  0.08584485  1.0957191   0.22013864\n",
      " -0.1595036   0.19663064 -0.6183021  -0.5584727  -0.31494346  0.5359567\n",
      "  0.57276416 -0.4690684  -0.4469947   1.0947863  -1.4567444  -0.9114281\n",
      "  0.6554227   1.9480485   0.284137    1.0638318   0.2702168   1.6981905\n",
      "  0.11986572 -1.2636449  -0.19723356 -0.14291511 -0.630183    0.6522912\n",
      " -0.688769   -0.4836162   0.7363817  -0.22837047  0.01451954 -0.0302165\n",
      "  1.4851688   0.56832117]\n",
      "(Unmasked) Predicted:  DEATHS & OBITUARIES  Actual:  WORLD WAR I\n",
      "(Masked) Predicted:  LEGISLATIVE BODIES  Actual:  -\n",
      "------\n",
      "\n",
      "------\n",
      "[-0.62451124 -0.8649864  -0.36242357 -0.14570282 -0.649379    0.30302012\n",
      " -0.01183496 -0.13746627 -0.2589482  -0.16056806  0.5680221   0.5833338\n",
      "  0.17179407  0.82422495  0.2574261  -0.5913952   0.9192381   0.47070163\n",
      "  0.26918364  0.29985324 -0.24815775 -0.36418015 -0.12453361  0.4369237\n",
      "  0.55930555 -0.30777133 -0.22532459  0.67557985 -0.6222145  -0.5861917\n",
      "  0.54335     0.89455277 -0.12011968  0.6964344   0.05202919  1.0553117\n",
      " -0.24814956 -0.6514895   0.06445172  0.30702984 -0.52561295  0.36762598\n",
      " -0.8086055  -0.33380333  0.86930275 -0.07545679  0.34080672  0.16081126\n",
      "  1.0082384   0.40927953]\n",
      "(Unmasked) Predicted:  DEATHS & OBITUARIES  Actual:  DEATHS & OBITUARIES\n",
      "(Masked) Predicted:  LEGISLATIVE BODIES  Actual:  US Selective Service System\n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(svd.fit_transform(X_test))\n",
    "y_pred_masked = clf2.predict(svd_unmasked.fit_transform(X_test_masked))\n",
    "for y_i in range(0, 3):\n",
    "    print(\"------\")\n",
    "    print(X_test[y_i])\n",
    "    print(\"(Unmasked) Predicted: \", y_pred[y_i], \" Actual: \", y_test[y_i])\n",
    "    print(\"(Masked) Predicted: \", y_pred_masked[y_i], \" Actual: \", y_test_masked[y_i])\n",
    "    print(\"------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e97abdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt \n",
    "#merge back upwards\n",
    "cmap = plt.get_cmap('cool')\n",
    "xg = svd.fit_transform(X_test)\n",
    "\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "ax = plt.axes(projection='3d')\n",
    "#ax2 = plt.axes(projection='3d')\n",
    "\n",
    "#colorsmn = cmap(np.linspace(0, 1, len(set(y_pred))))\n",
    "#clr = {y_pred[i]: colorsmn[y_pred[i]] for i in range(len(y_pred))}\n",
    "\n",
    "colors = [float(hash(s) % 256) / 256 for s in y_pred]      \n",
    "colors_actual = [float(hash(s) % 256) / 256 for s in y_test]    \n",
    "\n",
    "ax.scatter3D(xg[:, 0], xg[:, 1], xg[:, 2], c=colors)\n",
    "\n",
    "#ax2.scatter3D(xg[:, 0], xg[:, 1], xg[:, 2], c=colors_actual)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e65f2971",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt \n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(xg[:, 0], xg[:, 1], xg[:, 2], c=colors_actual)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a2511bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = [key for key in d2v_model.dv.index_to_key]\n",
    "doc_vectors = np.array([d2v_model.dv[key] for key in doc_ids])\n",
    "\n",
    "doc_ids_masked = [key for key in d2v_model_masked.dv.index_to_key]\n",
    "doc_vectors_masked = np.array([d2v_model_masked.dv[key] for key in doc_ids_masked])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "186de505",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import umap.umap_ as umap\n",
    "\n",
    "umap_args = {\n",
    "    'n_neighbors': 10,\n",
    "    'n_components': 10,\n",
    "    'metric': 'cosine'\n",
    "}\n",
    "\n",
    "umap_model = umap.UMAP(**umap_args).fit(doc_vectors)\n",
    "umap_model_masked = umap.UMAP(**umap_args).fit(doc_vectors_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b3d08371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "hdbscan_args = {\n",
    "    'min_cluster_size': 10,\n",
    "    'metric': 'euclidean',\n",
    "    'cluster_selection_method': 'eom'\n",
    "}\n",
    "\n",
    "clusters = hdbscan.HDBSCAN(**hdbscan_args).fit(umap_model.embedding_)\n",
    "clusters_masked = hdbscan.HDBSCAN(**hdbscan_args).fit(umap_model_masked.embedding_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a14eb891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48,)\n",
      "(67,)\n"
     ]
    }
   ],
   "source": [
    "doc_cluster_labels = clusters.labels_\n",
    "doc_cluster_labels_masked = clusters_masked.labels_\n",
    "doc_unique_cluster_labels = set(doc_cluster_labels)\n",
    "doc_unique_cluster_labels_masked = set(doc_cluster_labels_masked)\n",
    "print(doc_cluster_labels_masked.shape)\n",
    "print(doc_cluster_labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6d30b7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 50)\n",
      "(43, 50)\n",
      "(29, 10)\n"
     ]
    }
   ],
   "source": [
    "topics = {}\n",
    "print(doc_vectors.shape)\n",
    "\n",
    "for label in doc_unique_cluster_labels:\n",
    "    label_mask = np.where(doc_cluster_labels == label)\n",
    "    label_doc_vectors = doc_vectors[label_mask]\n",
    "\n",
    "    label_doc_vectors_centroid = np.mean(label_doc_vectors, axis=0)\n",
    "    \n",
    "    topics[f'{label}'] = {\n",
    "        'similar_words': d2v_model.wv.similar_by_vector(label_doc_vectors_centroid, 10)\n",
    "    }\n",
    "topics_masked = {}\n",
    "\n",
    "for label in doc_unique_cluster_labels_masked:\n",
    "    label_mask = np.where(doc_cluster_labels_masked == label)\n",
    "    label_doc_vectors_masked = doc_vectors_masked[label_mask]\n",
    "    \n",
    "    label_doc_vectors_centroid_masked = np.mean(label_doc_vectors_masked, axis=0)\n",
    "    \n",
    "    topics_masked[f'{label}'] = {\n",
    "        'similar_words': d2v_model_masked.wv.similar_by_vector(label_doc_vectors_centroid_masked, 10)\n",
    "    }\n",
    "\n",
    "print(label_doc_vectors.shape)\n",
    "print(label_doc_vectors_masked.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "651efea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'similar_words': [('guilty', 0.6942793130874634),\n",
       "   ('phillipos', 0.6906381249427795),\n",
       "   ('unidentified', 0.688586950302124),\n",
       "   ('victim', 0.6834298968315125),\n",
       "   ('tmz', 0.6524671316146851),\n",
       "   ('murder', 0.6524639129638672),\n",
       "   ('stabbing', 0.6481800675392151),\n",
       "   ('convicted', 0.6476503610610962),\n",
       "   ('woman', 0.6360318660736084),\n",
       "   ('cab', 0.6349772214889526)]},\n",
       " '1': {'similar_words': [('sogoodsong', 0.5967389941215515),\n",
       "   ('org', 0.5939915180206299),\n",
       "   ('passim', 0.5926750302314758),\n",
       "   ('barnbilly', 0.5920282602310181),\n",
       "   ('fricke', 0.5892524719238281),\n",
       "   ('touring', 0.589024007320404),\n",
       "   ('nrg', 0.5869570374488831),\n",
       "   ('edm', 0.5845231413841248),\n",
       "   ('cafeartscience', 0.576126754283905),\n",
       "   ('com', 0.5734462738037109)]},\n",
       " '-1': {'similar_words': [('vile', 0.6671569347381592),\n",
       "   ('pour', 0.6660515069961548),\n",
       "   ('vanengelen', 0.6426787376403809),\n",
       "   ('umbriaprime', 0.6340689659118652),\n",
       "   ('coefficient', 0.6256619691848755),\n",
       "   ('ricocheted', 0.6254613995552063),\n",
       "   ('nicholls', 0.6231626272201538),\n",
       "   ('philandering', 0.6208568811416626),\n",
       "   ('stills', 0.6207634210586548),\n",
       "   ('cycles', 0.6180403828620911)]}}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "31fca53d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'similar_words': [('that', 0.9875426292419434),\n",
       "   ('mad', 0.9873052835464478),\n",
       "   ('hamlet', 0.983853816986084),\n",
       "   ('th', 0.9826173186302185),\n",
       "   ('john', 0.9820388555526733),\n",
       "   ('red', 0.979408323764801),\n",
       "   ('the', 0.977973997592926),\n",
       "   ('bennett', 0.9768017530441284),\n",
       "   ('ten', 0.9760429859161377),\n",
       "   ('mcdonald', 0.9728329181671143)]},\n",
       " '1': {'similar_words': [('john', 0.9917251467704773),\n",
       "   ('karol', 0.9897347688674927),\n",
       "   ('hamlet', 0.988791286945343),\n",
       "   ('cup', 0.9884663224220276),\n",
       "   ('mcdonald', 0.9876260757446289),\n",
       "   ('martin', 0.9853977560997009),\n",
       "   ('letter', 0.9851770401000977),\n",
       "   ('ago', 0.9841151237487793),\n",
       "   ('yale', 0.9833624958992004),\n",
       "   ('the', 0.9823281168937683)]}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b3c5700f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aidan/.local/lib/python3.8/site-packages/umap/umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43, 50)\n",
      "(29, 10)\n",
      "(67, 2)\n",
      "(48, 2)\n"
     ]
    }
   ],
   "source": [
    "#nonlinear umap \n",
    "\n",
    "import umap.umap_ as umap\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#print(label_doc_vectors.shape)\n",
    "#reducer = PCA(n_components=3)\n",
    "#reducer.fit(X=label_doc_vectors)\n",
    "#pca_transformed = reducer.transform(label_doc_vectors)\n",
    "\n",
    "\n",
    "umapped = umap.UMAP(n_neighbors=50, random_state=42).fit(doc_vectors)\n",
    "umapped_masked = umap.UMAP(n_neighbors=50, random_state=4).fit(doc_vectors_masked)\n",
    "\n",
    "print(label_doc_vectors.shape)\n",
    "print(label_doc_vectors_masked.shape)\n",
    "\n",
    "print(umapped.embedding_.shape)\n",
    "print(umapped_masked.embedding_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4a6f582d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -5.3481154   -0.8192531  -10.439599   ...   9.4186535   12.0515\n",
      "   -8.7208805 ]\n",
      " [ -0.02329331  -1.7041844   -5.280875   ...   2.6817906    4.975572\n",
      "    0.45308542]\n",
      " [  1.5554686    0.24535349  -1.3979199  ...  -0.47098267  -2.2448773\n",
      "   -6.8244452 ]\n",
      " ...\n",
      " [  3.9112766    1.0878955    1.1413082  ...   2.5401616   -2.4846683\n",
      "   -1.9111472 ]\n",
      " [  0.6864695    1.1443138   -0.19280544 ...   0.50281316   0.17396323\n",
      "    0.4484266 ]\n",
      " [  1.5864064    0.03104437   0.36713883 ...   0.56538594   0.02059977\n",
      "   -0.4417035 ]]\n"
     ]
    }
   ],
   "source": [
    "len(label_doc_vectors)\n",
    "print(label_doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "daad44f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67,)\n",
      "(48,)\n",
      "[-1 -1 -1 -1  0 -1 -1  0 -1 -1 -1  1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1  0\n",
      "  1 -1 -1  1 -1 -1 -1 -1  1 -1 -1 -1 -1  0 -1 -1  1  0  0  1  0 -1 -1 -1\n",
      "  0  0  1  1  1 -1 -1 -1  0 -1 -1 -1  0 -1  0  1  0 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "print(doc_cluster_labels.shape)\n",
    "print(doc_cluster_labels_masked.shape)\n",
    "print(doc_cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "99aedfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(28.5, 20.5)\n",
    "\n",
    "y = umapped.embedding_[:, 1]\n",
    "z = umapped.embedding_[:, 0]\n",
    "ax.scatter(y, z, s= 10, c=doc_cluster_labels[0:len(umapped.embedding_)], cmap='Spectral')\n",
    "for i in range(0, len(y)):\n",
    "    ax.annotate(d2v_model.wv.similar_by_vector(doc_vectors[i],1), (y[i], z[i]), fontsize=4)\n",
    "fig.savefig('test.png', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1edadc93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fda5de41460>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.scatter(umapped_masked.embedding_[:, 0], umapped_masked.embedding_[:, 1], s= 10, c=doc_cluster_labels_masked[0:len(umapped_masked.embedding_)], cmap='Spectral')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2667aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload a set of tags -> match clustering to tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39007239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
