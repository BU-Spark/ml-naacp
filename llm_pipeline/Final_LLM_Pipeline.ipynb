{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06660f5-9970-43fa-950d-40d2672158d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6728cb8-d5d6-4c82-9151-e28077e55645",
   "metadata": {},
   "source": [
    "## Llama 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94b9693-c2e9-476c-8b45-e729e0451166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "# llm\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# Prompt\n",
    "from langchain.chains.prompt_selector import ConditionalPromptSelector\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Parser\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa485a-fbb8-4fda-8f12-922ef0797a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"headline\", \"body\"],\n",
    "    template=\"\"\"<<SYS>> \\n You are an assistant tasked in geo-locating \\\n",
    "this news article. \\n <</SYS>> \\n\\n [INST] Generate a SHORT response \\\n",
    "of where you think this article is talking about. BE SPECIFIC AS POSSIBLE. IT IS IMPERATIVE THAT YOU HIGHLIGHT THE MOST SPECIFIC LOCATION. Give your response in the following format: \\\n",
    "1.Y/N indicating whether the article is talking about a region of Boston. \\n 2.The specific location within the city you got if you got Y in the first question. \\\n",
    "3. The involved specific locations or organizations EXPLICITLY FOUND WITHIN THE ARTICLE that influenced your decision. \\\n",
    "If you do not know, PLEASE GIVE THE BEST GUESS AS POSSIBLE. \\n\\n\n",
    "Headline: \\n\\n {headline} \\n\\n Body: \\n\\n {body} \\n\\n [/INST]\"\"\",\n",
    ")\n",
    "\n",
    "# prompt = PromptTemplate(\n",
    "#     input_variables=[\"headline\"],\n",
    "#     template=\"\"\"<<SYS>> \\n You are an assistant tasked in geo-locating \\\n",
    "# this news article. \\n <</SYS>> \\n\\n [INST] Generate a SHORT response \\\n",
    "# of where you think this article is talking about. BE SPECIFIC AND CONCISE AS POSSIBLE. IT IS IMPERATIVE THAT YOU HIGHLIGHT THE MOST SPECIFIC LOCATION. PLEASE CONSIDER THE CONTEXT OF THE ARTICLE. Give your response in the following format: \\\n",
    "# 1. A very brief summary of what the article is talking about. \\n 2.The specific location you chose based on the context of the article. \\\n",
    "# If you do not know, PLEASE GIVE THE BEST GUESS AS POSSIBLE. \\n\\n\n",
    "# Headline: \\n\\n {headline} \\n\\n [/INST]\"\"\",\n",
    "# )\n",
    "\n",
    "# prompt = PromptTemplate(\n",
    "#     input_variables=[\"headline\"],\n",
    "#     template=\"\"\"<<SYS>> \\n You are an assistant tasked in geo-locating \\\n",
    "# this news article. \\n <</SYS>> \\n\\n [INST] Generate a SHORT response \\\n",
    "# of where you think this article is talking about. BE SPECIFIC AND CONCISE AS POSSIBLE. IT IS IMPERATIVE THAT YOU HIGHLIGHT THE MOST SPECIFIC LOCATION. Give your response in the following format: \\\n",
    "# 1.Y/N indicating whether the article is talking about a region of Boston \\n 2.The specific location within the city you got if you got Y in the first question. \\\n",
    "# If you do not know, PLEASE GIVE THE BEST GUESS AS POSSIBLE. PLEASE KEEP YOUR ANSWER SHORT. \\n\\n\n",
    "# Headline: \\n\\n {headline} \\n\\n Body: \\n\\n {body}  \\n\\n [/INST]\"\"\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6331ce41-f3c6-4e87-875a-f0e81ea1bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model_path = \"./models/llama_7B/llama-2-7b-chat.Q4_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1316a33-e977-4c8b-bf91-d484aa67cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=llama_model_path,\n",
    "    n_gpu_layers=1,\n",
    "    n_batch=1024,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=True,\n",
    ")\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def96c8-d0fe-4cb3-995c-4c8b3d735675",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60b07dd-1443-428c-826e-47c32317ae15",
   "metadata": {},
   "source": [
    "## NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c05ec2-d1cc-4076-98c5-800e09d7e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from span_marker import SpanMarkerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebe03a3-5365-4847-8c96-05b776461308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spacy model with the span_marker pipeline component\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])\n",
    "nlp.add_pipe(\"span_marker\", config={\"model\": \"tomaarsen/span-marker-roberta-large-ontonotes5\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba225e5-af5f-4b75-b8b8-8fbb9cf1d70f",
   "metadata": {},
   "source": [
    "## Google Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c8e671-1360-4e45-8cc9-336c02a5612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import requests\n",
    "import googlemaps\n",
    "from mapbox import Geocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9387da38-0427-43bb-8fe8-79549a8247a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmap_client_key = \"YOUR_KEY_HERE\"\n",
    "gmaps = googlemaps.Client(key=gmap_client_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9295ba46-b68d-4436-b6b4-4f05b26c4982",
   "metadata": {},
   "source": [
    "## Pipeline Entry Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc4cd74-013c-4103-bd58-b0e943f75edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_dir = \"./sample_data/se_naacp_db.articles_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d823391c-7e1b-47ab-96c2-2b1bf9e7681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(sample_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4838f89-a535-4835-bf40-e61e20c95d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3f455-9942-43db-a367-df20ff231974",
   "metadata": {},
   "source": [
    "The ML Model honestly just needs the `id`, `header`, and `body`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fa6ca7-c57d-4df6-ba1e-0bbdc82d017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([raw_df['_id'], raw_df['hl1'], raw_df['body']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d345510-0b3a-404e-97aa-65731caa8156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Testing Purposes Only\n",
    "# df = df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20da336c-fa9d-4c3d-8cb5-d4a022c50a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"llama_prediction\"] = None # Add the llama_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a496fd4c-f010-47c9-a4fc-fd5143c21750",
   "metadata": {},
   "source": [
    "Remove Duplicates (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25afed5-24ec-4bd3-8cd8-012db223357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df.duplicated(subset=['hl1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa071092-8a53-427a-b465-485392c1ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(duplicates.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41608ff2-c439-40bd-b57a-84bd3c9f6859",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['hl1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327540c2-cfc9-4f57-9232-5467c44f8a20",
   "metadata": {},
   "source": [
    "Clean the HTML in the body and header -> Regex Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf11d77-e2e1-4f3f-9bd9-2e0b911f35fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_clean_html = lambda x: BeautifulSoup(x, \"html.parser\").get_text()\n",
    "df['body'] = df['body'].progress_apply(func_clean_html)\n",
    "df['hl1'] = df['hl1'].progress_apply(func_clean_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a50086c-3125-4ce9-ab3c-b0fb1af47b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_clean_regex = lambda x: ' '.join([item for item in re.findall(r'[A-Za-z0-9!@#$%^&*().]+', x) if len(item) > 1])\n",
    "df['body'] = df['body'].progress_apply(func_clean_regex)\n",
    "df['hl1'] = df['hl1'].progress_apply(func_clean_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f9c4ad-27e7-4f25-a560-3208b7185519",
   "metadata": {},
   "source": [
    "### Explicit Article Mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa85fc8-af09-4bbe-8e40-071f39049e64",
   "metadata": {},
   "source": [
    "Load the well-known locations, organizations, and neighborhoods dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9873218-4c33-49ae-9d95-3578847a98b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explicit_filtering(header):\n",
    "    known_locs_path = \"./geodata/known_locs.json\"\n",
    "    with open(known_locs_path, 'r') as file:\n",
    "        known_locs_dict = json.load(file)\n",
    "        \n",
    "    lowercase_header = header.lower()\n",
    "    for key in known_locs_dict.keys():\n",
    "        if (key in lowercase_header):\n",
    "            return [key, known_locs_dict[key]]   \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49ca938-d331-4232-a108-563b4552e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Explicit_Pass_1\"] = df[\"hl1\"].progress_apply(explicit_filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3395c5-7318-421c-bb7c-5c48d6f88945",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522219d3-ab62-490b-a124-49c1121a0a43",
   "metadata": {},
   "source": [
    "### NER Code First Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938dd64e-06ce-4363-99b7-8f74e5d4839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_NER = lambda x: [(entity, entity.label_) for entity in nlp(x).ents] if (x != None and x != \"\") else None\n",
    "def predict_NER_def(x):\n",
    "    try:\n",
    "        return predict_NER(x)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd84abb5-9923-473c-95f7-0ec09c90a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explicit_filtering_NER(col):\n",
    "    try:\n",
    "        if (col['Explicit_Pass_1'] != None): # We already found an explicit mention in the title\n",
    "            print(f\"Passed on {col['hl1']}\")\n",
    "            return None\n",
    "        else:\n",
    "            return predict_NER_def(col['body'])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e15698b-a749-4ddb-bf5e-7c70c4885f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NER_Pass_1'] = df.progress_apply(explicit_filtering_NER, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad18838-44e3-4f19-b1ad-53a9404f5054",
   "metadata": {},
   "source": [
    "Filter based on superb specific places such as 'FAC'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab392a42-ca93-4c45-8a09-e7413557be33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_loc_explicit(x):\n",
    "    if (x == None):\n",
    "        return None\n",
    "    res = []\n",
    "    for tup in x:\n",
    "        if (len(tup) >= 2): \n",
    "            if ((\"GPE\" in tup[1] and \"Boston\" not in tup[0] and \"Massachusetts\" not in tup[0])\n",
    "                or (\"ORG\" in tup[1]) \n",
    "                or (\"FAC\" in tup[1])\n",
    "                or (\"LOC\" in tup[1])\n",
    "            ):\n",
    "                res.append((tup[0], tup[1].strip()))\n",
    "    priority = {'FAC': 1, 'ORG': 2, 'LOC': 3, 'GPE': 4}\n",
    "    sorted_list = sorted(res, key=lambda x: priority[x[1]])\n",
    "    \n",
    "    return sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79622e1-f75b-49e6-835b-d99492d9eb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NER_Pass_1_Sorted'] = df['NER_Pass_1'].progress_apply(filter_loc_explicit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf8297-2431-4906-9698-ec34812caf03",
   "metadata": {},
   "source": [
    "Then we get the coordinates throught the first pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e499c-5efe-4edb-a005-f4d04c296dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLongLatsForFAC(x):\n",
    "    if (x == None or len(x) == 0):\n",
    "        return None  \n",
    "    \n",
    "    location = x[0][0] # (Location, Label)\n",
    "    if (x[0][1] == \"FAC\" or \"Boston\" in location): # Check if we have Boston + 'FAC' Label\n",
    "        response = gmaps.geocode(f\"{location}, Boston\")\n",
    "    elif(x[0][1] == \"FAC\"): # Check if we have 'FAC' Label\n",
    "        response = gmaps.geocode(f\"{location}, Massachusetts\")\n",
    "    else:\n",
    "        return None # Doesn't have 'FAC' Label\n",
    "        \n",
    "    if (len(response) == 0):\n",
    "        return None  \n",
    "    latitude = response[0]['geometry']['location']['lat']\n",
    "    longitude = response[0]['geometry']['location']['lng']\n",
    "    \n",
    "    return [longitude, latitude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db884a0-3eec-4135-befd-ad15822c62da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NER_Pass_1_Coordinates'] = df['NER_Pass_1_Sorted'].progress_apply(getLongLatsForFAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcfd69f-5c7e-4227-9dfa-d65ae1403c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6c6fb8-f722-4562-8802-66f343c2cddf",
   "metadata": {},
   "source": [
    "### Llama Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456c9a0d-6b6a-4752-b0cc-21fa660370a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_llama(col):\n",
    "    try:\n",
    "        if (col['Explicit_Pass_1'] != None or col['NER_Pass_1_Coordinates'] != None): # We already found an explicit mention in the previous passes\n",
    "            print(f\"Passed on {col['hl1']}\")\n",
    "            return None\n",
    "        else:\n",
    "            return chain.invoke({\"headline\": col['hl1'], \"body\": col['body']})\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1020aba-d244-46cc-857a-e5b52fe5680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['llama_prediction'] = df.progress_apply(predict_llama, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef47a65b-6089-4a99-aefa-0217ee0c12f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be341891-8981-4bb6-85f8-160f24fa252b",
   "metadata": {},
   "source": [
    "We then apply NER on the llama outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e1be57-5192-4cd4-a465-55b2b5a97d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NER_prediction'] = df['llama_prediction'].progress_apply(predict_NER_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6d8904-feba-48aa-97d8-39e553fbf8cb",
   "metadata": {},
   "source": [
    "Sort the NER Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4947843e-4771-4110-935f-923b806d183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_first_comma(x):\n",
    "    if (x[:1] == \",\"):\n",
    "        return x[2:]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def format_NER(x):\n",
    "    x = str(x)\n",
    "    res = []\n",
    "    if (x != None):\n",
    "        input = x.replace(\"(\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").split(\")\") \n",
    "        for word in input:\n",
    "            res.append(remove_first_comma(word).strip())\n",
    "    return res \n",
    "\n",
    "def filter_loc(x):\n",
    "    res = []\n",
    "    for tup in x:\n",
    "        cleaned_tup = tup.strip().split(\",\")\n",
    "        if (len(cleaned_tup) >= 2): \n",
    "            if ((\"GPE\" in cleaned_tup[1] and \"Boston\" not in cleaned_tup[0] and \"Massachusetts\" not in cleaned_tup[0])\n",
    "                or (\"ORG\" in cleaned_tup[1]) \n",
    "                or (\"FAC\" in cleaned_tup[1])\n",
    "                or (\"LOC\" in cleaned_tup[1])\n",
    "            ):\n",
    "                res.append((cleaned_tup[0], cleaned_tup[1].strip()))\n",
    "    priority = {'FAC': 1, 'ORG': 2, 'LOC': 3, 'GPE': 4}\n",
    "    sorted_list = sorted(res, key=lambda x: priority[x[1]])\n",
    "    \n",
    "    return sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b0e007-70db-4ba7-ad98-92335111105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NER_Sorted'] = df['NER_prediction'].progress_apply(format_NER)\n",
    "df['NER_Sorted'] = df['NER_Sorted'].progress_apply(filter_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc0d393-c567-4240-918e-94c1363f0a1e",
   "metadata": {},
   "source": [
    "## Geocoding -> Pass it off to Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af582b1-c9dc-4210-87ea-2f7dff9a1f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLongLats(x):\n",
    "    if (len(x) == 0):\n",
    "        return None  \n",
    "        \n",
    "    location = x[0][0] # (Location, Label)\n",
    "    if (x[0][1] == \"ORG\" or x[0][1] == \"FAC\" or \"Boston\" in location):\n",
    "        response = gmaps.geocode(f\"{location}, Boston\")\n",
    "    else:\n",
    "        response = gmaps.geocode(f\"{location}, Massachusetts\")\n",
    "    if (len(response) == 0):\n",
    "        return None  \n",
    "    latitude = response[0]['geometry']['location']['lat']\n",
    "    longitude = response[0]['geometry']['location']['lng']\n",
    "    return [longitude, latitude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2532e9-3a00-450b-8aa3-8f25e00ccffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NER_Sorted_Coordinates'] = df['NER_Sorted'].progress_apply(getLongLats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92b37e0-cc0d-465c-ba54-d62a8da6cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_census_api(longitude, latitude):\n",
    "    county = \"NO COUNTY\"\n",
    "    url = f\"https://geocoding.geo.census.gov/geocoder/geographies/coordinates?x={longitude}&y={latitude}&benchmark=Public_AR_Current&vintage=Census2020_Current&format=json\"\n",
    "    response = requests.get(url)\n",
    "    if (response.status_code == 200):\n",
    "        results = response.json()\n",
    "        # print(results['result']['geographies'])\n",
    "        census_tracts = results['result']['geographies'].get('Census Tracts', [])\n",
    "        # county = results['result']['geographies']['County Subdivisions'][0].get('COUNTY')\n",
    "        if (census_tracts):\n",
    "            return census_tracts[0].get('TRACT', 'No TRACT found'), county # Returning the TRACT of the first census tract found\n",
    "    return \"No TRACT found\", county  # Return this if API call failed or no tracts found\n",
    "\n",
    "def getTractList(col):\n",
    "    coordinates = []\n",
    "    tract_list = [] # Initialize an empty list to store TRACT information  \n",
    "    \n",
    "    if (col['Explicit_Pass_1'] != None): # If we got the locations from the first explicit pass\n",
    "        coordinates = col['Explicit_Pass_1'][1]\n",
    "    elif(col['NER_Pass_1_Coordinates'] != None): # If we got locations from the very first NER pass (specific locs only)\n",
    "        coordinates = col['NER_Pass_1_Coordinates']\n",
    "    elif (col['NER_Sorted_Coordinates'] != None): # Finally, if we got locations from llama + NER pass\n",
    "        coordinates = col['NER_Sorted_Coordinates']\n",
    "    else: # Must be a very hard/bad article :-(\n",
    "        return None \n",
    "        \n",
    "    longitude = coordinates[0]\n",
    "    latitude = coordinates[1]\n",
    "    TRACT, COUNTY = query_census_api(longitude,latitude)\n",
    "    tract_list.append(TRACT)\n",
    "    \n",
    "    return tract_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d98107d-7769-4412-ba44-0ab5e0883dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tracts'] = df.progress_apply(getTractList, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e5516-28a5-42ac-aef0-77c8911a506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"Tracts\"]) # Clean Those that doesn't have a Tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23501b4-f7a8-4034-bfc3-1edb2e0fd374",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18433712-41de-47f7-b7f0-aa320ed109fe",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759fb02d-cec9-4d51-bd47-a237f4878d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad39889b-e104-48fb-af30-4f916e51cec3",
   "metadata": {},
   "source": [
    "## OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ac341f-0c3e-43b5-92f2-d586e345bf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry up to 10 times with exponential backoff, starting at 1 second and maxing out at 20 seconds delay\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(10))\n",
    "def get_embedding(text: str, model=\"text-embedding-3-small\"):\n",
    "    #print(text)\n",
    "    try:\n",
    "        embedding = client.embeddings.create(input=text, model=model).data[0].embedding\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retrieve ADA Embedding: {e}. Replacing with replacement value!\")\n",
    "        return [-1.0]\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf84a1d-936f-48b4-b71a-41cc616ab9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key='YOUR_KEY_HERE',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34426fc-6558-49cc-8e14-99eaae0aaa54",
   "metadata": {},
   "source": [
    "## Taxonomy Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f199d99d-56ab-4572-98ad-83f3f0719d85",
   "metadata": {},
   "source": [
    "Content Taxanomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aa43db-4e38-4d7b-9706-01f99c5f3929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embedding for taxonomy\n",
    "taxonomy_df = pd.read_csv('./taxonomy_list/Content_Taxonomy.csv', skiprows=5, usecols=range(8))\n",
    "taxonomy_df.columns = taxonomy_df.iloc[0]\n",
    "taxonomy_df = taxonomy_df.tail(-1)\n",
    "\n",
    "tier_1_list = []\n",
    "tier_2_list = []\n",
    "tier_3_list = []\n",
    "tier_4_list = []\n",
    "for index, row in taxonomy_df.iterrows():\n",
    "    if not pd.isnull(row['Tier 4']) and row['Tier 4'] != ' ':\n",
    "        tier_1_label = row['Tier 1']\n",
    "        tier_2_label = row['Tier 2']\n",
    "        tier_3_label = row['Tier 3']\n",
    "        tier_4_label = row['Tier 4']\n",
    "        tier_4_list.append(f'{tier_1_label} - {tier_2_label} - {tier_3_label} - {tier_4_label}')\n",
    "    elif not pd.isnull(row['Tier 3']) and row['Tier 3'] != ' ':\n",
    "        tier_1_label = row['Tier 1']\n",
    "        tier_2_label = row['Tier 2']\n",
    "        tier_3_label = row['Tier 3']\n",
    "        tier_3_list.append(f'{tier_1_label} - {tier_2_label} - {tier_3_label}')\n",
    "    elif not pd.isnull(row['Tier 2']) and row['Tier 2'] != ' ':\n",
    "        tier_1_label = row['Tier 1']\n",
    "        tier_2_label = row['Tier 2']\n",
    "        tier_2_list.append(f'{tier_1_label} - {tier_2_label}')\n",
    "    else:\n",
    "        tier_1_label = row['Tier 1']\n",
    "        tier_1_list.append(f'{tier_1_label}')\n",
    "\n",
    "tier_1_list = list(set(tier_1_list))\n",
    "tier_2_list = list(set(tier_2_list))\n",
    "tier_3_list = list(set(tier_3_list))\n",
    "tier_4_list = list(set(tier_4_list))\n",
    "\n",
    "tier_1_embedding = [get_embedding(topic) for topic in tier_1_list]\n",
    "tier_2_embedding = [get_embedding(topic) for topic in tier_2_list]\n",
    "tier_3_embedding = [get_embedding(topic) for topic in tier_3_list]\n",
    "tier_4_embedding = [get_embedding(topic) for topic in tier_4_list]\n",
    "\n",
    "all_topics_list = []\n",
    "[all_topics_list.append(topic) for topic in tier_1_list]\n",
    "[all_topics_list.append(topic) for topic in tier_2_list]\n",
    "[all_topics_list.append(topic) for topic in tier_3_list]\n",
    "[all_topics_list.append(topic) for topic in tier_4_list]\n",
    "\n",
    "all_topics_embedding = []\n",
    "[all_topics_embedding.append(embedding) for embedding in tier_1_embedding]\n",
    "[all_topics_embedding.append(embedding) for embedding in tier_2_embedding]\n",
    "[all_topics_embedding.append(embedding) for embedding in tier_3_embedding]\n",
    "[all_topics_embedding.append(embedding) for embedding in tier_4_embedding]\n",
    "print(len(all_topics_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8cb2da-6008-4603-bb04-dc72d38a412e",
   "metadata": {},
   "source": [
    "Selected Taxonomy List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec06bd-49ac-4fd6-b190-df47579cd57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding for the 230 topics selected by BERTopic \n",
    "selected_taxonomy_df = pd.read_csv('./topics/embedding_similarity_label.csv')\n",
    "selected_taxonomy_df = selected_taxonomy_df.dropna(subset=['closest_topic'])\n",
    "selected_topics_list = selected_taxonomy_df['closest_topic'].values.tolist()\n",
    "\n",
    "selected_topics_embedding = [get_embedding(topic) for topic in selected_topics_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa984ca5-c42a-4971-8643-87243aa4a5ad",
   "metadata": {},
   "source": [
    "Client Taxonomy List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6d2647-de7b-4739-a634-e1fc6001ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative taxonomy: client's list of topics\n",
    "client_taxonomy_df = pd.read_excel('./topics/Asad_Topics_List.xlsx', names=['label'])\n",
    "client_taxonomy_df['ada_embedding'] = client_taxonomy_df['label'].map(get_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013b6e7e-b1bd-4967-ba6b-c69394b58f32",
   "metadata": {},
   "source": [
    "## Obtaining Ada Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8629be5e-08f8-40fe-a404-b78aeb648987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(tokens, length=500):\n",
    "    \"\"\"\n",
    "    Function to get the first 500 elements from a list\n",
    "    \"\"\"\n",
    "    return tokens[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c78b2-90d1-46f2-97c9-a0154b943ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic_model_body'] = df['body'].apply(lambda x: re.sub(re.compile('<.*?>'), '', x))\n",
    "df['tokens'] = df['topic_model_body'].apply(lambda x: x.split())\n",
    "df['tokens'] = df['tokens'].apply(truncate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae80c8a-85c4-403e-ba17-8614fde464c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ada_embedding'] = df.tokens.apply(lambda x: get_embedding(','.join(map(str,x)), model='text-embedding-3-small'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81ef63-a3fd-41e7-a13b-8b8261635e6f",
   "metadata": {},
   "source": [
    "## Similarity Matching After Ada Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc29a04e-8ba2-4d7b-a6a7-bb054b4d2b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most similar taxonomy (out of all toipcs) to news body\n",
    "closest_topic_list_all = []\n",
    "for index, row in df.iterrows():\n",
    "    target_embedding = row['ada_embedding']\n",
    "    similarities = [cosine_similarity(np.array(target_embedding).reshape(1, -1), np.array(topic).reshape(1, -1))[0][0] for topic in all_topics_embedding]\n",
    "\n",
    "    # Find the index of the topic with the highest similarity\n",
    "    closest_topic_index = np.argmax(similarities)\n",
    "\n",
    "    # Retrieve the closest topic embedding\n",
    "    closest_topic = all_topics_list[closest_topic_index]\n",
    "    closest_topic_list_all.append(closest_topic)\n",
    "\n",
    "df['closest_topic_all'] = closest_topic_list_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d07692-3c94-4467-b03f-fecb75167561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most similar taxonomy (out of 230 selected topics) to news body\n",
    "closest_topic_list_selected = []\n",
    "for index, row in df.iterrows():\n",
    "    target_embedding = row['ada_embedding']\n",
    "    similarities = [cosine_similarity(np.array(target_embedding).reshape(1, -1), np.array(topic).reshape(1, -1))[0][0] for topic in selected_topics_embedding]\n",
    "\n",
    "    # Find the index of the topic with the highest similarity\n",
    "    closest_topic_index = np.argmax(similarities)\n",
    "\n",
    "    # Retrieve the closest topic embedding\n",
    "    closest_topic = selected_topics_list[closest_topic_index]\n",
    "    closest_topic_list_selected.append(closest_topic)\n",
    "\n",
    "df['closest_topic_selected'] = closest_topic_list_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246894ca-66d9-4785-acc9-ff03f584297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_topic_embedding_list = client_taxonomy_df['ada_embedding'].to_list()\n",
    "client_topic_list = client_taxonomy_df['label'].to_list()\n",
    "similarity_arr = []\n",
    "\n",
    "closest_topic_list_client = []\n",
    "for index, row in df.iterrows():\n",
    "    target_embedding = row['ada_embedding']\n",
    "    similarities = [cosine_similarity(np.array(target_embedding).reshape(1, -1), np.array(topic).reshape(1, -1))[0][0] for topic in client_topic_embedding_list]\n",
    "    \n",
    "    if max(similarities) > 0.25:    \n",
    "        closest_topic_index = np.argmax(similarities) # Find the index of the topic with the highest similarity\n",
    "        closest_topic = client_topic_list[closest_topic_index] # Retrieve the closest topic embedding\n",
    "        closest_topic_list_client.append(closest_topic)\n",
    "    else:\n",
    "        closest_topic_list_client.append('Other')\n",
    "    similarity_arr.append(max(similarities))\n",
    "    \n",
    "df['closest_topic_client'] = closest_topic_list_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd6e668-52dc-4922-b678-69a1e03c3658",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bc9a6d-448e-4cdc-87a3-0ce4130e8dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./outputs/gbh_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c27603-7f20-4999-9b40-cc709abf6a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2593d02-3f45-4d86-9aa1-6a0246ca69bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b092abb-fcc4-4802-9d4d-b3fb81728787",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(raw_df, df, on='_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754cec32-50bd-4517-a471-69f93d1f0468",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36e641e-54f9-42e6-95cf-f4446ddb86b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"./outputs/gbh_output_all_fields.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a7416e-f170-4e07-b3c7-f72a0fe78c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be64af-8156-47a4-b65d-c9dd8371ef92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d850a568-7803-4b55-ade0-0beab0e61e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d735eea5-0eda-4b7c-987a-4c41e1ba341f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604faa57-f7f1-4488-a248-6c43cff52243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef60b252-dc6c-4271-9917-9ba090e559c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
