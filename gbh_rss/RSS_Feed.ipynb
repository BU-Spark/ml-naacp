{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43b23aa9-f4a4-41fc-84e5-6fbcdf0defdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from os.path import exists\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47f252c3-cf7b-409a-bb58-7f8f94323e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Compression\n",
    "import json\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67d97957-b63f-4a9a-9b71-a63331dbd406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Data fetching\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e55708-39d7-4133-ad09-a8a1c369c756",
   "metadata": {},
   "source": [
    "Some Implementation notes on GCP to consider:\n",
    "* We are opting for batch prediction to reduce the number of cold starts on GKE (that is 0 -> X number)\n",
    "* Since we are opting for batch prediction, it may be true that NOT ALL scheduled RSS feed scraping will end up going through the prediction pipeline. I set it where if the length of the RSS_List exceeds a number of articles we have, then we flush it and send it to the ML pipeline. After that we destroy the RSS_List object and instaniate a new one.\n",
    "* This pipeline needs to be robust, there is a risk with batching. If one article fails, we cannot just throw the entire batch out!\n",
    "\n",
    "The general idea is that:\n",
    "<br>\n",
    "PyMongo -> fetches RSS_Configurations and Links\n",
    "<br>\n",
    "RSS_Configurations and Links -> Iterate over to parse\n",
    "<br>\n",
    "RSS_Configurations and Links -> If Quota is met, flush and push to the ML API and Database as well\n",
    "<br>\n",
    "\n",
    "Extras:\n",
    "<br>\n",
    "Can we run a EDA or Data insights for batched data? Match distributions like using a Kolmogorov-Smirnov test?\n",
    "<br>\n",
    "Catch data drift?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc800993-40bb-47c0-8dd5-a89a9d83dfce",
   "metadata": {},
   "source": [
    "# PyMongo -> RSS_List "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34c43a9-9064-4ed7-8b4a-23d71d91ea27",
   "metadata": {},
   "source": [
    "Below is a an ideal list after fetching the information from PyMongo, it could be thought of as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "282c4d8d-95d2-4750-96b7-3cb0cd790320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are examples of 3 links to use with appropriate configurations\n",
    "rss_objects = [\n",
    "    (\n",
    "        'https://rss.nytimes.com/services/xml/rss/nyt/World.xml',\n",
    "        {\n",
    "            \"scrap_tags\": [\"title\", \"description\", \"dc:creator\", \"pubDate\"]\n",
    "        }\n",
    "    ), \n",
    "    (\n",
    "        'https://rss.nytimes.com/services/xml/rss/nyt/YourMoney.xml',\n",
    "        {\n",
    "            \"scrap_tags\": [\"title\", \"description\", \"dc:creator\", \"pubDate\"]\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        'https://www.wgbh.org/tags/bunp.rss',\n",
    "        {\n",
    "            \"scrap_tags\": [\"title\", \"link\", \"description\", \"content:encoded\", \"category\", \"pubDate\"]\n",
    "        }\n",
    "    )    \n",
    "]\n",
    "\n",
    "# Just GBH\n",
    "rss_objects = [\n",
    "    (\n",
    "        'https://www.wgbh.org/tags/bunp.rss',\n",
    "        {\n",
    "            \"scrap_tags\": [\"title\", \"link\", \"description\", \"content:encoded\", \"category\", \"pubDate\"]\n",
    "        }\n",
    "    )    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7f1464-84a4-420c-9242-2b6aea5722dd",
   "metadata": {},
   "source": [
    "# RSS_Request_Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d5af2c-5227-4573-92e3-eb5879770000",
   "metadata": {},
   "source": [
    "A request object that has all the necessary functions to parse, fetch, and maybe load. \n",
    "<br>\n",
    "This will be a seperate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdeffc7-4d6c-43f1-8182-8bc64bd7233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rss_request_obj:\n",
    "    \"\"\"\n",
    "    An RSS Request Object.\n",
    "    rss_url: str - The url to scrap at. Must be a valid RSS URL\n",
    "    scrap_configuration: dict - Settings for the RSS Request Object\n",
    "    \"\"\"\n",
    "    def __init__(self, rss_url: str, scrap_configuration: dict):\n",
    "        self.rss_url = rss_url\n",
    "        self.scrap_configuration = scrap_configuration\n",
    "        self.data_bucket = []\n",
    "        self.article_count = 0\n",
    "\n",
    "        if (not self.validate_settings()):\n",
    "            raise Exception(f\"RSS Object Failed to Init: Errors found in settings configuration\")\n",
    "\n",
    "    def getDataBucket(self):\n",
    "        \"\"\"\n",
    "        Gets all the data parsed from the request object so far.\n",
    "        \"\"\"\n",
    "        return self.data_bucket\n",
    "\n",
    "    def getArticleCount(self):\n",
    "        \"\"\"\n",
    "        Returns the count of the articles of the RSS request object.\n",
    "        \"\"\"\n",
    "        return self.article_count\n",
    "\n",
    "    def validate_settings(self):\n",
    "        \"\"\"\n",
    "        Ensures that scrap configuration settings is appropriate.\n",
    "\n",
    "        I will add more later...\n",
    "\n",
    "        Returns: \n",
    "        Boolean indicated whether the configuration dict abides the data schema\n",
    "        \"\"\"\n",
    "        data_schema = [\"scrap_tags\"] # I hard coded this, but in the future, this will be dynamic, you can add to this\n",
    "        \n",
    "        for schema_part in data_schema:\n",
    "            if (schema_part not in self.scrap_configuration.keys()):\n",
    "                raise Exception(f\"RSS Object Failed to Init: Missing setting '{schema_part}' in dict\")\n",
    "        if (len(self.scrap_configuration[\"scrap_tags\"]) != len(set(self.scrap_configuration[\"scrap_tags\"]))):\n",
    "            raise Exception(f\"RSS Object Failed to Init: Duplicate tag found!\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def rss_request(self):\n",
    "        \"\"\"\n",
    "        RSS Fetch for the the links. Should return an xml tree of the scrapped items.\n",
    "\n",
    "        Exception:\n",
    "        returns void if request fails or xml tree could not be built\n",
    "\n",
    "        Returns:\n",
    "        BeautifulSoup object that holds the xml tree\n",
    "        \"\"\"\n",
    "        try:\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "            r = requests.get(self.rss_url, headers=headers)\n",
    "            soup = BeautifulSoup(r.content, features='lxml-xml')\n",
    "            return soup\n",
    "        except Exception as e:\n",
    "            print(f\"> Job Failed: Failed Fetching RSS Feed! RSS Request Object at {self.rss_url} threw an error!\")\n",
    "            print(f\"{e}\\n\")\n",
    "            return\n",
    "\n",
    "    def rss_parse(self):\n",
    "        \"\"\"\n",
    "        RSS Fetch for the the links. Should return an xml tree of the scrapped items.\n",
    "\n",
    "        Exception:\n",
    "        returns void if there is failure in fields\n",
    "\n",
    "        Returns:\n",
    "        BeautifulSoup object that holds the xml tree\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if (self.rss_url == None):\n",
    "                raise Exception(\"RSS Parsed Failed: rss_url is of NoneType!\")\n",
    "            xml_tree = self.rss_request()\n",
    "            items = xml_tree.select('channel > item')\n",
    "            if (len(items) == 0):\n",
    "                raise Exception(\"RSS Parsed Failed: <item> tag could not be found!\")\n",
    "            for item in items: # This is O(n^2), I don't like it, but I think its the best for now...\n",
    "                tags = self.scrap_configuration[\"scrap_tags\"]\n",
    "                entry = {}\n",
    "                # Duplication check, we assume that duplicate articles will have the same first tag content in scrap_tags.\n",
    "                if (self.checkDuplicates(tags[0], (item.find(tags[0])).text)):\n",
    "                    continue\n",
    "                    \n",
    "                for tag in tags:\n",
    "                    data_obj = (item.find(tag))\n",
    "                    if (data_obj == None):\n",
    "                        print(f\"{tag} was not found in RSS feed.\")\n",
    "                        continue\n",
    "                    entry[tag] = data_obj.text\n",
    "                self.data_bucket.append(entry)\n",
    "                self.article_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"> Job Failed: Failed Parsing RSS Feed! RSS Request Object at {self.rss_url} threw an error!\")\n",
    "            print(f\"{e}\\n\")\n",
    "            return\n",
    "\n",
    "    def checkDuplicates(self, key, value):\n",
    "        \"\"\"\n",
    "        Checks for duplicate items by taking a key (tag) and a value of that tag and comparing\n",
    "        it against \n",
    "        \n",
    "        Returns:\n",
    "        Boolean representing if the item already exists\n",
    "        \"\"\"\n",
    "        return any(d[key] == value for d in self.data_bucket)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1639829-ba32-4243-a309-02b0c4b0856e",
   "metadata": {},
   "source": [
    "# RSS_List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831215de-9822-4be3-b2a8-77b3d110c7d7",
   "metadata": {},
   "source": [
    "This is a wrapper for RSS_Request Objects.\n",
    "<br>\n",
    "This will be a seperate file.\n",
    "<br>\n",
    "To Do:\n",
    "Add Try catch blocks so we don't block the flow of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a0cf76-3c5d-4fe1-94a5-e1ea171f31e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSS_List:\n",
    "    def __init__(self):\n",
    "        self.RSS_Request_Bucket = []\n",
    "        self.total_article_counts = 0\n",
    "        self.unparsed = []\n",
    "        self.flushed = False\n",
    "\n",
    "    def __repr__(self):\n",
    "        display = \"\"\n",
    "        display += \"=================================================================\\n\"\n",
    "        display += (f\"Total Article Counts: {self.total_article_counts}\\n\")\n",
    "        display += (f\"# Request Objects in RSS List: {len(self.RSS_Request_Bucket)}\\n\")\n",
    "        display += (f\"Unparsed Indexes of RSS Request Objects: {self.unparsed}\\n\")\n",
    "        display += \"=================================================================\\n\"\n",
    "\n",
    "        for rss_obj in self.RSS_Request_Bucket:\n",
    "            if (len(rss_obj.getDataBucket()) != 0):\n",
    "                display += (f\"\\n {rss_obj.getDataBucket()}\\n\")\n",
    "                display += \"\\n=================================================================\\n\"\n",
    "        \n",
    "        return display\n",
    "\n",
    "    def getRSSBucketList(self):\n",
    "        \"\"\"\n",
    "        Gets the the list of RSS_Request_Objects\n",
    "        \"\"\"\n",
    "        return self.RSS_Request_Bucket\n",
    "\n",
    "    def getTotalArticleCount(self):\n",
    "        \"\"\"\n",
    "        Gets the total articles aggregated across all RSS_Request_Objects\n",
    "        \"\"\"\n",
    "        return self.total_article_counts\n",
    "\n",
    "    def addRequestObject(self, RSS_req_obj):\n",
    "        \"\"\"\n",
    "        Adds the Request Object without parsing it.\n",
    "        \"\"\"\n",
    "        self.RSS_Request_Bucket.append(RSS_req_obj)\n",
    "        self.unparsed.append(len(self.RSS_Request_Bucket) - 1)\n",
    "        return\n",
    "\n",
    "    def addRequestObjectAndParse(self, RSS_req_obj):\n",
    "        \"\"\"\n",
    "        Adds the Request Object parsing it.\n",
    "        \"\"\"\n",
    "        RSS_req_obj.rss_parse();\n",
    "        self.RSS_Request_Bucket.append(RSS_req_obj)\n",
    "        self.total_article_counts += RSS_req_obj.getArticleCount()\n",
    "        return   \n",
    "\n",
    "    def parseAllObjects(self):\n",
    "        \"\"\"\n",
    "        Parses all the unparsed RSS_Request_Objects. \n",
    "        \"\"\"\n",
    "        while (len(self.unparsed) != 0):\n",
    "            unparsed_obj_idx = self.unparsed.pop(0)\n",
    "            parsed_obj = self.RSS_Request_Bucket[unparsed_obj_idx]\n",
    "            parsed_obj.rss_parse()\n",
    "            self.total_article_counts += parsed_obj.getArticleCount()\n",
    "        return\n",
    "\n",
    "    def flush_list_to_data(self):\n",
    "        \"\"\"\n",
    "        This converts all the RSS_Request_Objects to the list of dictionaries found in the attribute 'RSS_Request_Objects.data_bucket'.\n",
    "        I may contemplate on encoding this with lossless compression because the corpus can be very big!!!!!\n",
    "        Note: Data -> JSON -> gzip\n",
    "        The conversion is useful to send to the ML API endpoint for batch parsing and prediction.\n",
    "        Note that running this function is irreversible! \n",
    "        It essentially overwrites the RSS_Request_Objects with 'RSS_Request_Objects.data_bucket' so use with caution.\n",
    "\n",
    "        So far there are no protections for failed overwrites (you will get a mixed datatype array), I will add these protections later...\n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b4bde-e48b-4ff1-801a-3c9ebdab43a2",
   "metadata": {},
   "source": [
    "# Main Driver Script\n",
    "\n",
    "To be used in main(): func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f97633-1a25-414e-850c-18f19b9649a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyMongoFetchRSS(query):\n",
    "    client = MongoClient(\"mongodb://mongo:UTwpvdTfzaWxGt29evbw@containers-us-west-177.railway.app:6703\")\n",
    "    db = client.se_naacp_gbh\n",
    "    collection = db.rss_links\n",
    "\n",
    "    cursor = collection.find(query)\n",
    "    return list(cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca60b1c-f73d-4fe4-b021-1377bebf4615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_RSS_scrap():\n",
    "    try:\n",
    "        RSS_FREE_LIST = RSS_List()\n",
    "        \n",
    "        # MongoDB fetch on RSS links and objects\n",
    "        rss_links = pyMongoFetchRSS({}) # In the future, we will pass {} for all... {'name': 'GreatBlueHill'}\n",
    "        print(rss_links)\n",
    "\n",
    "        for link in rss_links:\n",
    "            if ('GreatBlueHill'): # \n",
    "                rss_obj = rss_request_obj(link['link'], {'scrap_tags': link['scrap_tags']}) # Create the RSS Request Object\n",
    "                RSS_FREE_LIST.addRequestObjectAndParse(rss_obj)\n",
    "                print(RSS_FREE_LIST)\n",
    "            \n",
    "    except Exception as e:\n",
    "            print(f\"> Job Failed: exec_RSS_scrap ran into errors!\")\n",
    "            print(f\"{e}\\n\")\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f4272-47ba-4ff5-83c5-f14336c4d6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_RSS_scrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1ecd2d-99b9-49e7-a336-973cb87c02a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86594cdf-3949-4e35-b228-b7ca0e550079",
   "metadata": {},
   "source": [
    "# Testing the Implmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bab515-730b-45c1-8311-2b6162195c06",
   "metadata": {},
   "source": [
    "A little note: Our worse case runtime is $O(n^3)$! We iterate every link, then for every link we iterate for each item, then for each item, we scrap the associated tags. 3 for-loops used. Is there a better way???? Maybe Look for some memory tradeoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7089e376-8d4f-4a2b-b277-d8b33708d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSS_FREE_LIST = RSS_List()\n",
    "\n",
    "# We then iterate over the pymongo list of links and configurations\n",
    "for rss in rss_objects:\n",
    "    rss_obj = rss_request_obj(rss[0], rss[1]) # Create the RSS Request Object\n",
    "    RSS_FREE_LIST.addRequestObjectAndParse(rss_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff3da82-a4ed-4e0a-88b4-f59c0cd17d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(RSS_FREE_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a15a963-c88b-4e5e-839c-9e9535eefc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSS_FREE_LIST = RSS_List()\n",
    "\n",
    "# We then iterate over the pymongo list of links and configurations\n",
    "for rss in rss_objects:\n",
    "    rss_obj = rss_request_obj(rss[0], rss[1]) # Create the RSS Request Object\n",
    "    RSS_FREE_LIST.addRequestObject(rss_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4723bc-fd5f-4e5b-b660-5a83a16529dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(RSS_FREE_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d885ba45-b3c6-4370-95a3-a2d548a1f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then parse all the unparsed\n",
    "RSS_FREE_LIST.parseAllObjects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5e2173-6bdd-4977-ad8d-580b720c50db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(RSS_FREE_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92169886-bac4-407d-b795-3baeb09576f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a30902-76f4-4ced-a1d0-5aa33ffc79e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b10990-a2ba-45df-9254-fe071d05170f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1a9e73-857c-4254-a4fe-b66699f04aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a093cf-662a-484f-8ce4-8973e925ca84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e8d263-5b0a-430b-b1c9-8620c807dc4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cef5d9-3875-43a1-afa7-94aec869f5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb483c0a-1891-498b-ae6f-e1071ee53a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79528489-b717-4370-8f61-7d2a8d65594e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1427e37-ea9f-475d-8da2-d738c17f67b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb40ff4-e0b1-4e16-9cfb-2b988502dbd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee8e351-80d1-4448-8054-2d1c1a9feb48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ab3e97-01d8-43ae-bc49-ded76d808872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
