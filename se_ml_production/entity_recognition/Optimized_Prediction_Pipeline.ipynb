{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54971437-f2dd-4d65-a8ae-371aebb0bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import nltk\n",
    "import spacy\n",
    "import requests\n",
    "import googlemaps \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from art import *\n",
    "import zipfile\n",
    "\n",
    "import secret # API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffcb8a9-8670-4052-a4ec-424e7423055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer \n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d39fdd-0421-491d-b33d-873fbb18a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import joblib\n",
    "import openai\n",
    "import torch\n",
    "import pickle\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132bca1c-25d0-418a-becb-549b1b0d99f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from bson import ObjectId\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5eb5a2-9f7d-4a8a-8f5d-a054d30f27e6",
   "metadata": {},
   "source": [
    "### Census Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a14141-b3f3-4914-b1a1-999bc71c1451",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a class that takes in two csv's. One csv for census data and one for census data overall by neighborhoods.\n",
    "As of this creation of this class, we are using the Boston Census 2020 data. Please make sure that future \n",
    "versions follow the same path.\n",
    "\"\"\"\n",
    "class census_data():\n",
    "    def __init__(self, db, fetch_arr):\n",
    "        self.db = db\n",
    "        self.fetch_arr = fetch_arr\n",
    "        self.load_census_neigh_data()\n",
    "        self.load_old_census_data()\n",
    "        self.load_census_data()\n",
    "        \n",
    "    def load_old_census_data(self):\n",
    "        try:\n",
    "            if (self.db == None):\n",
    "                raise Exception(\"No database was given!\")\n",
    "                \n",
    "            if (self.fetch_arr[2]):\n",
    "                bucket = self.db.bucket(\"ml_naacp_model_data\")\n",
    "                blob = bucket.blob(\"Topic_Modeling_Pipeline_Data/census.json\")\n",
    "                blob.download_to_filename(\"./geodata_prod/census.json\")\n",
    "\n",
    "            old_census_tracts_db = json.load(open(\"./geodata_prod/census.json\"))\n",
    "\n",
    "            self.old_census_tracts = old_census_tracts_db\n",
    "        except Exception as err:\n",
    "            print(f\"Error loading Census Data class!\\nError: {err}\")\n",
    "            raise Exception(\"Fatal Error in Class Construction!\")\n",
    "        return\n",
    "    \n",
    "    def load_census_data(self, fetch=True):\n",
    "        try:\n",
    "            if (self.db == None):\n",
    "                raise Exception(\"No database was given!\")\n",
    "\n",
    "            if (self.fetch_arr[1]):\n",
    "                bucket = self.db.bucket(\"ml_naacp_model_data\")\n",
    "                blob = bucket.blob(\"Topic_Modeling_Pipeline_Data/census_2020.csv\")\n",
    "                blob.download_to_filename(\"./geodata_prod/census_2020.csv\")\n",
    "\n",
    "            census_tracts_df = pd.read_csv(\"./geodata_prod/census_2020.csv\")\n",
    "            \n",
    "            self.census_tracts = self.process_census_data(census_tracts_df)\n",
    "        except Exception as err:\n",
    "            print(f\"Error loading Census Data class!\\nError: {err}\")\n",
    "            raise Exception(\"Fatal Error in Class Construction!\")\n",
    "        return\n",
    "        \n",
    "    def load_census_neigh_data(self, fetch=True):\n",
    "        try:\n",
    "            if (self.db == None):\n",
    "                raise Exception(\"No database was given!\")\n",
    "                \n",
    "            if (self.fetch_arr[0]):\n",
    "                bucket = self.db.bucket(\"ml_naacp_model_data\")\n",
    "                blob = bucket.blob(\"Topic_Modeling_Pipeline_Data/census_2020_neigh.csv\")\n",
    "                blob.download_to_filename(\"./geodata_prod/census_2020_neigh.csv\")\n",
    "\n",
    "            census_neigh_data_df = pd.read_csv(\"./geodata_prod/census_2020_neigh.csv\")\n",
    "\n",
    "            self.census_neighbourhoods = self.process_census_neigh_data(census_neigh_data_df)\n",
    "        except Exception as err:\n",
    "            print(f\"Error loading Census Data class!\\nError: {err}\")\n",
    "            raise Exception(\"Fatal Error in Class Construction!\")\n",
    "        return\n",
    "\n",
    "    def process_census_data(self, df):\n",
    "        demographics = df.iloc[:,11:20]\n",
    "        geoid_tract = df['GEOCODE']\n",
    "        tract = df['TRACT']\n",
    "\n",
    "        concat_pd = pd.concat([tract, geoid_tract, demographics], axis=1)\n",
    "        concat_pd.drop(concat_pd.index[0], inplace=True)\n",
    "        concat_pd = concat_pd.rename(\n",
    "            columns={\n",
    "                'TRACT': 'tract', \n",
    "                'GEOCODE': 'geoid_tract',\n",
    "                'P0020001': 'total'        \n",
    "            }\n",
    "        )\n",
    "        return concat_pd\n",
    "\n",
    "    # A function for the future\n",
    "    def process_census_neigh_data(self, df):\n",
    "        df = df.iloc[:,:7]\n",
    "        df = df.rename(\n",
    "            columns={\n",
    "                'tract20_nbhd': 'Neighborhood',       \n",
    "            }\n",
    "        )\n",
    "        df.drop(df.index[0], inplace=True)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea03bfa-1b2e-4701-9a51-aeed551026d3",
   "metadata": {},
   "source": [
    "### Neighbourhood Mapping Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8232458e-675c-4895-874f-d63dc3959930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neighborhood_mapping():\n",
    "    def __init__(self, db, fetch_arr):\n",
    "        self.db = db\n",
    "        self.fetch_arr = fetch_arr\n",
    "        self.load_mappings()\n",
    "    \n",
    "    def load_mappings(self):\n",
    "        try:\n",
    "            if (self.db == None):\n",
    "                raise Exception(\"No database was given!\")\n",
    "                \n",
    "            if (self.fetch_arr[0]):\n",
    "                bucket = self.db.bucket(\"ml_naacp_model_data\")\n",
    "                blob = bucket.blob(\"Entity_Recognition_Pipeline_Data/tracts-neighbors.json\")\n",
    "                blob.download_to_filename(\"./geodata_prod/tracts-neighbors.json\")\n",
    "\n",
    "            if (self.fetch_arr[1]):\n",
    "                bucket = self.db.bucket(\"ml_naacp_model_data\")\n",
    "                blob = bucket.blob(\"Entity_Recognition_Pipeline_Data/blocks-neighbors.json\")\n",
    "                blob.download_to_filename(\"./geodata_prod/blocks-neighbors.json\")\n",
    "\n",
    "            block_map_db = json.load(open(\"./geo-data/blocks-neighbors.json\"))\n",
    "            tract_map_db = json.load(open(\"./geodata_prod/tracts-neighbors.json\"))\n",
    "            \n",
    "            self.tract_mapping = tract_map_db\n",
    "            self.block_mapping = block_map_db\n",
    "        except Exception as err:\n",
    "            print(f\"Error loading neighborhood mapping class!\\nError: {err}\")\n",
    "            raise Exception(\"Fatal Error in Class Construction!\")\n",
    "        return\n",
    "\n",
    "    def tract_to_neighborhood(self, tract):\n",
    "        # given a census tract return the boston neighborhood it is in \n",
    "        return self.tract_mapping[tract]\n",
    "\n",
    "    def block_to_neighborhood(self, block):\n",
    "        # given a census block return the boston neighborhood it is in \n",
    "        return self.block_mapping(block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1de31e-31d4-48b2-901a-8a85bbb719b6",
   "metadata": {},
   "source": [
    "### Geography Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b14dd18-8e65-4b2a-b1f3-f7c616125fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class geography():\n",
    "    def __init__(self, db, fetch_arr):\n",
    "        self.db = db\n",
    "        self.fetch_arr = fetch_arr\n",
    "        self.load_mass_town_entities()\n",
    "        self.load_state_entities()\n",
    "        self.load_saved_geocodes()\n",
    "        self.load_org_entities()\n",
    "\n",
    "    def load_saved_geocodes(self):\n",
    "        try:\n",
    "            if (self.db == None):\n",
    "                raise Exception(\"No database was given!\")\n",
    "                \n",
    "            if (self.fetch_arr[0]):\n",
    "                bucket = self.db.bucket(\"ml_naacp_model_data\")\n",
    "                blob = bucket.blob(\"Entity_Recognition_Pipeline_Data/saved-geocodes.json\")\n",
    "                blob.download_to_filename(\"./geodata_prod/saved-geocodes.json\")\n",
    "                \n",
    "            saved_geocodes_db = json.load(open(\"./geodata_prod/saved-geocodes.json\"))\n",
    "            \n",
    "            self.saved_geocodes = saved_geocodes_db\n",
    "        except Exception as err:\n",
    "            print(f\"Error loading geography class!\\nError: {err}\")\n",
    "            raise Exception(\"Fatal Error in Class Construction!\")\n",
    "        return\n",
    "        \n",
    "    def load_state_entities(self, fetch=True):\n",
    "        try:\n",
    "            if (self.db == None):\n",
    "                raise Exception(\"No database was given!\")\n",
    "\n",
    "            if (self.fetch_arr[1]):\n",
    "                bucket = self.db.bucket(\"ml_naacp_model_data\")\n",
    "                blob = bucket.blob(\"Entity_Recognition_Pipeline_Data/states.csv\")\n",
    "                blob.download_to_filename(\"./geodata_prod/states.csv\")\n",
    "            \n",
    "            states = pd.read_csv(\"./geodata_prod/states.csv\")\n",
    "            \n",
    "            states_set = set()\n",
    "            for idx in range(len(states)):\n",
    "                tup = (states['state'][idx], 'LOC')\n",
    "                states_set.add(tup)\n",
    "            self.state_entities = states_set\n",
    "            \n",
    "        except Exception as err:\n",
    "            print(f\"Error loading geography class!\\nError: {err}\")\n",
    "            raise Exception(\"Fatal Error in Class Construction!\")\n",
    "        return\n",
    "    \n",
    "    def load_mass_town_entities(self, fetch=True):\n",
    "        try:\n",
    "            if (self.db == None):\n",
    "                raise Exception(\"No database was given!\")\n",
    "                \n",
    "            if (self.fetch_arr[2]):\n",
    "                bucket = self.db.bucket(\"ml_naacp_model_data\")\n",
    "                blob = bucket.blob(\"Entity_Recognition_Pipeline_Data/mass-towns.csv\")\n",
    "                blob.download_to_filename(\"./geodata_prod/mass-towns.csv\")\n",
    "                \n",
    "            towns = pd.read_csv(\"./geodata_prod/mass-towns.csv\")\n",
    "\n",
    "            towns_set = set()\n",
    "            for idx in range(len(towns)):\n",
    "                tup = (towns['town'][idx], 'LOC')\n",
    "                towns_set.add(tup)\n",
    "            self.mass_town_entities = towns_set\n",
    "        except Exception as err:\n",
    "            print(f\"Error loading geography class!\\nError: {err}\")\n",
    "            raise Exception(\"Fatal Error in Class Construction!\")\n",
    "        return\n",
    "    \n",
    "    def load_org_entities(self):\n",
    "        self.org_entities = (('GBH News', 'ORG'), ('Boston Public Radio', 'ORG'), \n",
    "                             ('Supreme Court', 'ORG'), ('New York Times', 'ORG'), \n",
    "                             ('Washington Post', 'ORG'), ('CNN', 'ORG'), \n",
    "                             ('NPR', 'ORG'), ('Associated', 'ORG'), \n",
    "                             ('Press', 'ORG'), ('Senate', 'ORG'), \n",
    "                             ('Associated Press', 'ORG'), ('AP', 'ORG'), \n",
    "                             ('ABC News', 'ORG'),('CSS', 'ORG'), \n",
    "                             ('Philadelphia Inquirer', 'ORG'), ('House', 'ORG'),\n",
    "                             ('Congress', 'ORG'), ('Worcester', 'ORG'),\n",
    "                             ('FBI', 'ORG'), ('Homeland Security Department', 'ORG'),\n",
    "                             ('CDC', 'ORG'),('Fox News', 'ORG'),('The Washington Post', 'ORG'),\n",
    "                             ('States', 'LOC'), ('S.', 'LOC'), ('Massachusetts', 'ORG'),\n",
    "                             ('White House', 'ORG'), ('High School', 'ORG'),\n",
    "                             ('MIT', 'ORG'), ('Harvard University', 'ORG'),\n",
    "                             ('White House', 'LOC'),('Greater Boston', 'LOC'),\n",
    "                             ('New England', 'LOC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59775c7d-8e07-4797-890c-2bc970ff4ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bert_NER():\n",
    "    # loading bert NER model \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
    "    nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"max\")\n",
    "\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97d42f0-236f-47eb-86ea-20f4ea246122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bert_TOPIC():\n",
    "    #loading bert Topic model\n",
    "    topic_model = BERTopic.load(\n",
    "        \"./llm_models/bglobe_519_body_230_cereal\", \n",
    "        embedding_model=SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    )\n",
    "    return topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12672df1-28f7-4d20-837a-58c692442f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_locations_bert(article_text, nlp):\n",
    "    \"\"\"\n",
    "    get location names from article using NER - bert model \n",
    "    https://huggingface.co/dslim/bert-base-NER\n",
    "    input: article_text as a string, aggregate of h1, h2, lede, and body\n",
    "    returns: locations - set of tuples of (NAME, 'LOC') and organizations - set of tuples (NAME, 'ORG) mentioned in the article\n",
    "    \"\"\"\n",
    "    \n",
    "    ner_results = nlp(article_text)\n",
    "    locations = set([(X['word'],X['entity_group']) for X in ner_results if X['entity_group'] == 'LOC'])\n",
    "    orgs = set([(X['word'], X['entity_group']) for X in ner_results if X['entity_group'] == 'ORG'])\n",
    "\n",
    "    return locations, orgs\n",
    "\n",
    "def clean_entity_results(extracted_loc, extracted_orgs, drop_geos):\n",
    "    # cleaning extracted entities from bert \n",
    "    # removing state names, and mass town names since the demographics data is too broad\n",
    "    # return cleaned set of entities\n",
    "    entity_result = extracted_loc | extracted_orgs\n",
    "\n",
    "    for tup in extracted_loc | extracted_orgs:\n",
    "        if len(tup[0]) <= 1:\n",
    "            entity_result.remove(tup)\n",
    "        elif tup in drop_geos.state_entities:\n",
    "            entity_result.remove(tup)\n",
    "        elif tup in drop_geos.mass_town_entities:\n",
    "            entity_result.remove(tup)\n",
    "        elif tup in drop_geos.org_entities:\n",
    "            entity_result.remove(tup)\n",
    "    return entity_result\n",
    "\n",
    "def remove_existing_geocodes(entity_result, saved_geocodes):\n",
    "    # check if any locations or organizations were recognized\n",
    "    # check if the geocodes already exist in dictionary\n",
    "    existing_loc_geocode = {}\n",
    "    new_loc_geocode = set()\n",
    "    for ent in entity_result:\n",
    "        try:\n",
    "            existing_loc_geocode[ent[0]] = saved_geocodes[ent[0]]\n",
    "        except KeyError:\n",
    "            new_loc_geocode.add(ent)\n",
    "    return existing_loc_geocode, new_loc_geocode\n",
    "\n",
    "def get_snippet(sentences, num_sent, lede=True, remaining_text=False):\n",
    "    \"\"\"\n",
    "    get the snippet of text from the article_text, replace single quotes\n",
    "    input: article text, and num_sent - number of sentences to return, default lede is true will return first x sentences\n",
    "           reamaining_text then must be False \n",
    "    returns: first x (num_sent) sentences\n",
    "    \"\"\"\n",
    "    #clean_text = clean_article_text(text)\n",
    "    #clean_text = \". \".join(clean_text.split(\".\")) # adding a space after period so nltk can do a better job recognizing sentences\n",
    "    #lede = nltk.sent_tokenize(clean_text)[:num_sent] # returns a list\n",
    "    \n",
    "    if lede: # get the first num_sent \n",
    "        lede_text = sentences[:num_sent]\n",
    "        result_text = \" \".join(lede_text)\n",
    "    elif remaining_text: # get rest of article num_sent * 2 until the end\n",
    "        result_text = sentences[num_sent*2:]\n",
    "        result_text = \" \".join(result_text)\n",
    "    else: # get sentences num_sent to num_sent * 2\n",
    "       result_text = sentences[num_sent:num_sent*2]\n",
    "       result_text = \" \".join(result_text) \n",
    "    \n",
    "    singleq = result_text.replace('’', \"'\")\n",
    "\n",
    "    return singleq\n",
    "\n",
    "def get_sentences(text):\n",
    "    # return article text as a list of its sentences \n",
    "\n",
    "    clean_text = clean_article_text(text)\n",
    "    clean_text = \". \".join(clean_text.split(\".\")) # adding a space after period so nltk can do a better job recognizing sentences\n",
    "    sentences = nltk.sent_tokenize(clean_text)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def clean_article_text(text):\n",
    "    # get text, removing html tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    clean_text = soup.get_text()\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb03f53c-89e3-49ae-9cbf-2641ee4fc351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location_geocode(API_KEY, locations):\n",
    "    \"\"\"\n",
    "    getting coordinates from location names in articles \n",
    "    input: google maps platform API KEY, locations article \n",
    "    return: dictionary of location names (key) with coordinates (value as a dictionary with lat and lon as keys)\n",
    "    \"\"\"\n",
    "    gmaps = googlemaps.Client(key=API_KEY)\n",
    "    results = {}\n",
    "\n",
    "    # getting coordinates\n",
    "    for place in locations:\n",
    "        # we can constrain google geocode api search to massachusetts or us - census geocoder will not work for places outside of U.S \n",
    "        #geocode_result = gmaps.geocode(place[0] + \", Suffok County, MA, USA\") # place is a tuple, where first value is the location name \n",
    "        geocode_result = gmaps.geocode(place[0] + \", Suffolk County\",  components={\"administrative_area_level\": \"MA\", \n",
    "                                                                                   \"country\": \"US\"})\n",
    "        #print(geocode_result)\n",
    "        #print()\n",
    "        temp = {}\n",
    "        try:\n",
    "            geocode_components = geocode_result[0]['address_components']\n",
    "            for i, addr_comp in enumerate(geocode_components):\n",
    "                if 'administrative_area_level_2' in addr_comp['types']:\n",
    "                    if \"Suffolk County\" == addr_comp['short_name'] and i != 0:\n",
    "                        temp['lat'] = geocode_result[0]['geometry']['location']['lat']\n",
    "                        temp['lon'] = geocode_result[0]['geometry']['location']['lng']\n",
    "                        results[place[0]] = temp\n",
    "                        \n",
    "        except IndexError: # unable to get coordinates for location\n",
    "            print(\"Unable to locate \" + place[0])\n",
    "\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a2244b-0460-430f-8da8-4dd6ec639151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_census_geos(geocode_results):\n",
    "    \"\"\"\n",
    "    get census geographies - tract, block group, block by coordinates\n",
    "    input: google maps geocode_results as a dictionary\n",
    "    return: block, block_group, tract, county for each location\n",
    "    \"\"\"\n",
    "    census_geos = {}\n",
    "    for place in geocode_results:\n",
    "        # building the geocoding url\n",
    "        base_url = f'https://geocoding.geo.census.gov/geocoder/geographies/coordinates?'\n",
    "        survey_ver = f'&benchmark=4&vintage=4&layers=2020 Census Blocks&format=json'\n",
    "        lon = geocode_results[place]['lon']\n",
    "        lat = geocode_results[place]['lat']\n",
    "        census_geo_url = f'{base_url}x={lon}&y={lat}{survey_ver}'\n",
    "\n",
    "        # getting the census geographies \n",
    "        response = requests.get(census_geo_url)\n",
    "        response_json = response.json()\n",
    "\n",
    "        try:\n",
    "            block = response_json['result']['geographies']['2020 Census Blocks'][0]['BLOCK']\n",
    "            block_group = response_json['result']['geographies']['2020 Census Blocks'][0]['BLKGRP']\n",
    "            tract = response_json['result']['geographies']['2020 Census Blocks'][0]['TRACT']\n",
    "            county = response_json['result']['geographies']['2020 Census Blocks'][0]['COUNTY']\n",
    "            census_geos[place] = {'block': block,\n",
    "                                  'blkgrp': block_group,\n",
    "                                  'tract': tract,\n",
    "                                  'county': county}\n",
    "        except IndexError:\n",
    "            print(\"Unable to retrieve census geography for: \" + place)\n",
    "        except KeyError:\n",
    "            print(\"Location is outside of the United States: \" + place)\n",
    "    return census_geos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d211234-51b8-4d19-b7ec-446c90a98186",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# CENSUS DATA API \n",
    "# https://www.census.gov/content/dam/Census/library/publications/2020/acs/acs_api_handbook_2020_ch02.pdf\n",
    "# any user can query small quantities of data with minimal restrictions - up to 50 variables in a single query, up to 500 queries per IP address per day \n",
    "# more than 500 queries per IP address per day requires you to register for API key - www.census.gov/developers\n",
    "# https://www.census.gov/data/developers/data-sets/decennial-census.html \n",
    "\"\"\"\n",
    "def get_census_demographics(year, dsource, dname, tract, county, state):\n",
    "    # input: census year, data source, survey name, tract, county, state\n",
    "    # return: demographic data for tract mentioned\n",
    "    \n",
    "    # census variables: https://api.census.gov/data/2020/dec/pl/variables.html \n",
    "    cols = 'NAME,P2_001N,P2_002N,P2_003N,P2_004N,P2_005N,P2_006N,P2_007N,P2_008N,P2_009N,P2_010N'\n",
    "    base_url = f\"https://api.census.gov/data/{year}/{dsource}/{dname}\"\n",
    "\n",
    "    # to get tract demographics \n",
    "    census_url = f\"{base_url}?get={cols}&for=tract:{tract}&in=county:{county}&in=state:{state}\"\n",
    "\n",
    "    # to get block demographics \n",
    "    # census_url = f\"{base_url}?get={cols}&for=block:{block}&in=tract:{tract}&in=county:{county}&in=state:{state}\"\n",
    "\n",
    "    census_response = requests.get(census_url)\n",
    "    census_response_json = census_response.json()\n",
    "\n",
    "    return census_response_json\n",
    "\n",
    "def run_entity_recognition(text, nlp, drop_geos, saved_geocodes):\n",
    "    # running entity recogntion on text\n",
    "    # parse existing geocoded entities and new geocoded entities\n",
    "    try:\n",
    "        extracted_loc, extracted_orgs = get_locations_bert(text, nlp)\n",
    "        ent_result = clean_entity_results(extracted_loc, extracted_orgs, drop_geos)\n",
    "        existing_loc_geocode, new_loc_geocode = remove_existing_geocodes(ent_result, saved_geocodes)\n",
    "    except TypeError as e:\n",
    "        print(f\"No entities: {e}\")\n",
    "        existing_loc_geocode = {}\n",
    "        new_loc_geocode = set()\n",
    "\n",
    "    return existing_loc_geocode, new_loc_geocode\n",
    "\n",
    "def run_location_geocode(API_KEY, new_loc_geocode):\n",
    "    # get geocodes for NEW locations and saving them to json\n",
    "    # returns new location geocodes as dictionary \n",
    "    location_geocode = {}\n",
    "    if new_loc_geocode:\n",
    "        location_geocode = get_location_geocode(API_KEY, new_loc_geocode)\n",
    "    return location_geocode\n",
    "\n",
    "def check_snippets(API_KEY, new_entities, existing_entities):\n",
    "    location_geocode = run_location_geocode(API_KEY, new_entities)\n",
    "    existing_loc_geocode = existing_entities\n",
    "    combined_geocodes = location_geocode | existing_loc_geocode # if this is empty, then try the next snippet of text \n",
    "    return (not combined_geocodes), location_geocode, existing_loc_geocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c9a89f-91c9-4395-b0bc-fdccd648a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(year, dsource, dname, state, existing_loc_geocode, location_geocode, mappings):\n",
    "    #location_geocode = {'Boston': {'lat': 42.3600825, 'lon': -71.0588801}, 'Massachusetts': {'lat': 42.4072107, 'lon': -71.3824374}, 'Boston city': {'lat': 42.3600825, 'lon': -71.0588801}, 'Roxbury': {'lat': 42.3125672, 'lon': -71.0898796}, 'Fitchburg': {'lat': 42.5834228, 'lon': -71.8022955}, 'Medford': {'lat': 42.4184296, 'lon': -71.1061639}}\n",
    "    #location_geocode = {'Massachusetts': {'lat': 42.4072107, 'lon': -71.3824374}, 'Salem': {'lat': 42.5197473, 'lon': -70.8954626}, 'Salem City Hall': {'lat': 42.5218851, 'lon': -70.8956157}}\n",
    "    #location_geocode = {'Salem': {'lat': 42.5197473, 'lon': -70.8954626}, 'Massachusetts': {'lat': 42.4072107, 'lon': -71.3824374}, 'Salem City Hall': {'lat': 42.5218851, 'lon': -70.8956157}}\n",
    "\n",
    "    #print(location_geocode | existing_loc_geocode)\n",
    "    \n",
    "    census_geos = get_census_geos(location_geocode | existing_loc_geocode)\n",
    "\n",
    "    result = []\n",
    "    for place_name in census_geos:\n",
    "        place_info = {}\n",
    "        county = census_geos[place_name]['county']\n",
    "        tract = census_geos[place_name]['tract']\n",
    "        \n",
    "        try:\n",
    "            demographic_results = get_census_demographics(year, dsource, dname, tract, county, state)\n",
    "\n",
    "            # build result dictionary \n",
    "            place_info[place_name] = {'county_code': county} \n",
    "            place_info[place_name] = {'county_name': demographic_results[1][0]}\n",
    "            place_info[place_name]['tract'] = tract\n",
    "            geoid_tract = state + county + tract # this includes the state and county and tract number\n",
    "            place_info[place_name]['geoid_tract'] = geoid_tract\n",
    "\n",
    "            if mappings.tract_mapping.get(geoid_tract): # get corresponding boston neighborhood \n",
    "                place_info[place_name]['neighborhood'] = mappings.tract_mapping[state + county + tract]\n",
    "            \n",
    "            place_info[place_name]['demographics'] = {\n",
    "                'p2_001n': demographic_results[1][1], # total population \n",
    "                'p2_002n': demographic_results[1][2], # total hispanic or latino \n",
    "                'p2_003n': demographic_results[1][3], # total not hispanic or latino \n",
    "                'p2_004n': demographic_results[1][4], # total not hispanic or latino - pop of one race\n",
    "                'p2_005n': demographic_results[1][5], # total not hispanic or latino - pop of one race - white alone \n",
    "                'p2_006n': demographic_results[1][6], # total not hispanic or latino - pop of one race - black or african american alone\n",
    "                'p2_007n': demographic_results[1][7], # total not hispanic or latino - pop of one race - american indian and alaska native alone\n",
    "                'p2_008n': demographic_results[1][8], # total not hispanic or latino - pop of one race - asian alone \n",
    "                'p2_009n': demographic_results[1][9], # total not hispanic or latino - pop of one race - native hawaiian and other pacific islander alone\n",
    "                'p2_010n': demographic_results[1][10] # total not hispanic or latino - pop of one race - some other race alone \n",
    "            } \n",
    "            \n",
    "            result.append(place_info)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Unable to get census demographics for: \" + place_name)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e6536-c08f-4a09-ae04-f11863c48d98",
   "metadata": {},
   "source": [
    "### Bootstrap the pipeline | The functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5baf9fc-6dc4-4cfb-881d-7cb73beb7c66",
   "metadata": {},
   "source": [
    "Next we have to bootstrap the pipeline and make sure that BERT is up and running/loaded and the variables needed is all good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca0d93c-1f2d-40d4-9524-89c9e2e52227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import itertools\n",
    "import threading\n",
    "\n",
    "class Spinner:\n",
    "    def __init__(self, message, delay=0.1):\n",
    "        self.spinner = itertools.cycle(['-', '/', '|', '\\\\'])\n",
    "        self.delay = delay\n",
    "        self.message = message\n",
    "        self.thread = threading.Thread(target=self.spin)\n",
    "        self.stop_running = threading.Event()\n",
    "\n",
    "    def spin(self):\n",
    "        while not self.stop_running.is_set():\n",
    "            sys.stdout.write(next(self.spinner))  # write the next character\n",
    "            sys.stdout.flush()                    # flush stdout buffer (actual character display)\n",
    "            sys.stdout.write('\\b')                # erase the last written char\n",
    "            time.sleep(self.delay)\n",
    "\n",
    "    def start(self):\n",
    "        self.stop_running.clear()\n",
    "        sys.stdout.write(self.message + ' ')\n",
    "        self.thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        self.stop_running.set()\n",
    "        self.thread.join()                       # wait for spinner to stop\n",
    "        sys.stdout.write('✔️ OK\\n')              # write final message\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def err(self):\n",
    "        self.stop_running.set()\n",
    "        self.thread.join()                       # wait for spinner to stop\n",
    "        sys.stdout.write('❌ Error!\\n')              # write final message\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335fcb14-1afe-4f06-921c-af58737b51c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_llm_models():\n",
    "    try:\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(\"naacp-models\")\n",
    "        blob = bucket.blob(\"BERTopic_Models/bglobe_519_body_230_cereal.zip\")\n",
    "        blob.download_to_filename(\"./llm_models/bglobe_519_body_230_cereal.zip\")\n",
    "\n",
    "        destination_file_name = \"./llm_models/bglobe_519_body_230_cereal.zip\"\n",
    "        with zipfile.ZipFile(destination_file_name, 'r') as zip_ref: # Extract the ZIP\n",
    "            zip_ref.extractall(\"./llm_models\")\n",
    "        os.remove(destination_file_name) # Remove the ZIP for cleanup\n",
    "    except Exception as e:\n",
    "        print(f\"Model fetching failed! {e}\")\n",
    "        raise Exception(\"Fatal Error in Fetching LLM models. Exiting...\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93768787-9bb8-4f20-ac3e-cec1ccac475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_heirarch_data(db, fetch=True):\n",
    "    try:\n",
    "        if (db == None):\n",
    "            raise Exception(\"No database was given!\")\n",
    "        if (fetch):\n",
    "            bucket = db.bucket(\"ml_naacp_model_data\")\n",
    "            blob = bucket.blob(\"Topic_Modeling_Pipeline_Data/openai_label_from_taxonomy_structured_230.json\")\n",
    "            blob.download_to_filename(\"./geodata_prod/openai_label_from_taxonomy_structured_230.json\")\n",
    "\n",
    "        heirarch_db = json.load(open(\"./geodata_prod/openai_label_from_taxonomy_structured_230.json\"))\n",
    "        \n",
    "        if (heirarch_db == None):\n",
    "            raise Exception(f\"heirarch_db returned None!\")\n",
    "            \n",
    "        return heirarch_db\n",
    "    except Exception as err:\n",
    "        print(f\"Error loading hierarchal data!\\nError: {err}\")\n",
    "        raise Exception(\"Fatal Error in fetching hierarchal data!\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce8c18-0000-40b8-93ff-8284c76571d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_pipeline():\n",
    "    try:\n",
    "        file_manifest = [\n",
    "            \"tracts-neighbors.json\", # Neigh Mappings\n",
    "            \"blocks-neighbors.json\", # Neigh Mappings\n",
    "            \"saved-geocodes.json\", # Geography\n",
    "            \"states.csv\", # Geography\n",
    "            \"mass-towns.csv\", # Geography\n",
    "            \"openai_label_from_taxonomy_structured_230.json\", # Topic Modeling\n",
    "            \"census_2020_neigh.csv\", # Census\n",
    "            \"census_2020.csv\", # Census\n",
    "            \"census.json\" # Census\n",
    "        ]\n",
    "        dependency_resolver_arr = []\n",
    "        \n",
    "        tprint(\"BU Spark!\", font=\"sub-zero\")\n",
    "        print(\"Bootstrapping pipeline... (This should only run once!)\")\n",
    "        print()\n",
    "        spinner = Spinner(\"Setting up variables...\")\n",
    "        spinner.start()\n",
    "        year='2020'\n",
    "        dsource='dec' # which survey are we interested in ? decennial \n",
    "        dname='pl' # a dataset within a survey, pl - redistricting data \n",
    "        state='25' # state code \n",
    "        spinner.stop()\n",
    "        print()\n",
    "\n",
    "        spinner = Spinner(\"Connecting to MongoDB...\")\n",
    "        spinner.start()\n",
    "        client = MongoClient(secret.MONGO_URI_NAACP) # A mock connection to catch early errors\n",
    "        spinner.stop()\n",
    "        print()\n",
    "\n",
    "        spinner = spinner = Spinner(\"Connecting to Google Cloud Storage Bucket...\")\n",
    "        spinner.start()\n",
    "        db = storage.Client()\n",
    "        spinner.stop()\n",
    "        print()\n",
    "        \n",
    "        spinner = Spinner(\"Detecting & Making local directories for Models...\")\n",
    "        spinner.start()\n",
    "        spinner.stop()\n",
    "        llm_model_directory_path = \"./llm_models\"\n",
    "        if (not os.path.exists(llm_model_directory_path)):\n",
    "            print(f\"No {llm_model_directory_path}! Creating...\")\n",
    "            os.makedirs(llm_model_directory_path)\n",
    "            spinner = Spinner(\"Fetching models...\")\n",
    "            spinner.start()\n",
    "            fetch_llm_models()\n",
    "            spinner.stop()\n",
    "        else:\n",
    "            print(f\"Found! {llm_model_directory_path}!\")\n",
    "            spinner = Spinner(\"Validating model files...\")\n",
    "            spinner.start()\n",
    "            if (not os.path.isfile(\"./llm_models/BERTopic_CPU_M1\")):\n",
    "                spinner.stop()\n",
    "                spinner = Spinner(\"Model file not found! Pulling models...\")\n",
    "                spinner.start()\n",
    "                fetch_llm_models()\n",
    "                spinner.stop()\n",
    "            else:\n",
    "                spinner.stop()\n",
    "\n",
    "        print(\"Model files successfully validated.\\n\")\n",
    "\n",
    "        spinner = Spinner(\"Detecting & Making local directories for Geographic Data...\")\n",
    "        spinner.start()\n",
    "        spinner.stop()\n",
    "        geodata_directory_path = \"./geodata_prod\"\n",
    "        if (not os.path.exists(geodata_directory_path)):\n",
    "            print(f\"No {geodata_directory_path}! Creating...\")\n",
    "            os.makedirs(geodata_directory_path)\n",
    "            dependency_resolver_arr = len(file_manifest) * [True]\n",
    "        else:\n",
    "            print(f\"Found! {geodata_directory_path}!\")\n",
    "            spinner = Spinner(\"Validating geodata files...\")\n",
    "            spinner.start()\n",
    "            for file in file_manifest:\n",
    "                if (not os.path.isfile(f\"./geodata_prod/{file}\")):\n",
    "                    dependency_resolver_arr.append(True)\n",
    "                else:\n",
    "                    dependency_resolver_arr.append(False)\n",
    "            spinner.stop()\n",
    "        print(\"Geodata files successfully validated and updated dependecy array.\\n\")\n",
    "        \n",
    "        spinner = Spinner(\"Fetching OpenAI Hierarchal Mappings...\")\n",
    "        spinner.start()\n",
    "        heir_data = load_heirarch_data(\n",
    "            db,\n",
    "            fetch=dependency_resolver_arr[5]\n",
    "        )\n",
    "        spinner.stop()\n",
    "        print()\n",
    "        \n",
    "        spinner = Spinner(\"Instantiating classes...\")\n",
    "        spinner.start()\n",
    "        mappings = neighborhood_mapping(\n",
    "            db=db,\n",
    "            fetch_arr=dependency_resolver_arr[:2]\n",
    "        )\n",
    "        drop_geos = geography(\n",
    "            db=db,\n",
    "            fetch_arr=dependency_resolver_arr[2:5] \n",
    "        )\n",
    "        census_base = census_data(\n",
    "            db=db,\n",
    "            fetch_arr=dependency_resolver_arr[6:] \n",
    "        )\n",
    "        saved_geocodes = drop_geos.saved_geocodes \n",
    "        spinner.stop()\n",
    "        print()\n",
    "        \n",
    "        spinner = Spinner(\"Loading Models...\")\n",
    "        spinner.start()\n",
    "        nlp_ner = load_bert_NER()\n",
    "        nlp_topic = load_bert_TOPIC()\n",
    "        spinner.stop()\n",
    "        \n",
    "        print(\"\\nBootstrap complete!\\n\")\n",
    "        return year, dsource, dname, state, drop_geos, mappings, census_base, heir_data, saved_geocodes, nlp_ner, nlp_topic, db\n",
    "    except Exception as e:\n",
    "        spinner.err()\n",
    "        print(f\"Bootstrap Failed!!!\\nFatal Error:{e}\")\n",
    "        raise Exception(\"Fatal Error in Bootstrapping ML Pipeline. Exiting...\")\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88fca66-55ac-4767-a0b8-67a30cf211f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_bootstrap(year, dsource, dname, state, drop_geos, mappings, census_base, heir_data, saved_geocodes, nlp_ner, nlp_topic, db):\n",
    "    try:\n",
    "        spinner = Spinner(\"Validating Bootstrap variables...\")\n",
    "        spinner.start()\n",
    "\n",
    "        variable_manifest = {\n",
    "            \"year\":year, \n",
    "            \"dsource\": dsource, \n",
    "            \"dname\": dname, \n",
    "            \"state\": state,\n",
    "            \"drop_geos\": drop_geos,\n",
    "            \"mappings\": mappings,\n",
    "            \"census_base\": census_base,\n",
    "            \"heir_data\": heir_data,\n",
    "            \"saved_geocodes\": saved_geocodes, \n",
    "            \"nlp_ner\": nlp_ner, \n",
    "            \"nlp_topic\": nlp_topic,\n",
    "            \"db\": db\n",
    "        }\n",
    "        \n",
    "        for var in variable_manifest.keys():\n",
    "            if (variable_manifest[var] == None):\n",
    "                raise Exception(f\"{var} returned None!. Exiting...\")\n",
    "        spinner.stop()\n",
    "        print()\n",
    "        print(\"Validation Complete! Everything seems to be in order.\")\n",
    "    except Exception as e:\n",
    "        spinner.err()\n",
    "        print(f\"Bootstrap Validation Failed!!!\\nFatal Error:{e}\")\n",
    "        raise Exception(\"Fatal Error in Bootstrapping Validation. Exiting...\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0debc854-866b-4b89-a188-972f035c548e",
   "metadata": {},
   "source": [
    "# Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad34dff6-711e-45f8-9ace-66d63179bb76",
   "metadata": {},
   "source": [
    "Ideally, we should only be concerned with articles that yield a recognition of location. If not, we chuck it out!.\n",
    "\n",
    "I think we should append it to the end of the df to make a cleaner since this needs to be passed to topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8e2aa6-13f7-4b40-abd1-63bcdb6df165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bootstrap Run\n",
    "year, dsource, dname, state, drop_geos, mappings, census_base, heir_data, saved_geocodes, nlp_ner, nlp_topic, db = bootstrap_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f52d17c-34f7-4730-8406-62b74cd85aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_bootstrap(year, dsource, dname, state, drop_geos, mappings, census_base, heir_data, saved_geocodes, nlp_ner, nlp_topic, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aadd0f-ffa5-4352-9f63-85228b33fb78",
   "metadata": {},
   "source": [
    "### Some Useful MongoDB functions for connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf5f973-caf5-4658-900f-7e63e5e26601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_MongoDB_Prod():\n",
    "    try:\n",
    "        client = MongoClient(secret.MONGO_URI_NAACP)\n",
    "        db = client['se_naacp_db']\n",
    "        print(\"[INFO] Connected to MongoDB Production Database!\")\n",
    "        return db\n",
    "    except Exception as err:\n",
    "        raise Exception(\"[Fatal Error!] Failed to Connect to MongoDB. [No retry implemented]\")\n",
    "    return \n",
    "\n",
    "def bootstrap_MongoDB_Prod(db_prod, defined_collection_names):\n",
    "    \"\"\"\n",
    "    Adds the upload collection and other necessities that both the GraphQL and AI Pipeline share.\n",
    "    Sets up the databse.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spinner = Spinner(\"Checking and Bootstrapping Production DB...\\n\")\n",
    "        spinner.start()\n",
    "        if (db_prod == None):\n",
    "            raise Exception(\"No database was given!\")\n",
    "            \n",
    "        # Here we check for the upload collection and make it if it doesn't exist\n",
    "        collection_list = db_prod.list_collection_names()\n",
    "        for collection in defined_collection_names:\n",
    "            if collection not in collection_list:\n",
    "                db_prod.create_collection(collection)\n",
    "                print(f\"[INFO] Collection '{collection}' created.\\n\")\n",
    "        spinner.stop()\n",
    "    except Exception as err:\n",
    "        spinner.err()\n",
    "        print(f\"[Error!] Error in Bootstrapping MongoDB Prod DB\\nError: {err}\")\n",
    "        raise Exception(\"Fatal Error in MongoDB Boostrap\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bb859b-0c82-4757-ae19-79a1e85f2097",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_prod = connect_MongoDB_Prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c81a97-5ad3-404a-82e3-364fc929b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "defined_collection_names = [\"uploads\", \"discarded\"]\n",
    "\n",
    "bootstrap_MongoDB_Prod(db_prod, defined_collection_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fd54a2-b842-4534-ae96-425bf7a1900d",
   "metadata": {},
   "source": [
    "# The Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41faf447-b0ed-4db5-aad5-0eaf38bbf1b3",
   "metadata": {},
   "source": [
    "### Incoming CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e0339e-013e-44f8-8af4-09de19949dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Articles Nov 2020 - March 2023.csv\", low_memory=False)\n",
    "df = df.iloc[:, : 12]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a8b998-1ed3-41e2-a228-ad1072acd6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05de1ee9-585f-4e7e-b972-fa71a021d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbcf816-83f9-4526-a217-84612b8eb2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\n",
    "    'Has Path?',\n",
    "    'Title',\n",
    "    'Section Navigation',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed65fff-7fa5-4f27-aa27-e43067a79dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c32373-e012-4fad-a96e-e1b0d02f9184",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b3924b-c585-4b4e-a1ac-ac4761b1343a",
   "metadata": {},
   "source": [
    "### Second Incoming CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f33345-2890-4908-b339-879efbf219ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./GBH.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e56fa9-3fc2-461d-8835-e5009c75515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10c73ac-dbb2-4171-b25b-4b6aa9613fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:, :9]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f0a1bf-0cdb-4fd5-bb95-a02797b199bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c7c31-aac1-4827-ad9b-92618a2ce48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We predetermine the userID and upload ID\n",
    "# This should have already been configured per CSV upload\n",
    "# Generate a unique UUID\n",
    "unique_id = str(uuid.uuid4())\n",
    "userID = \"1\"\n",
    "\n",
    "print(unique_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166c056a-781a-4953-a836-52cfcfc44d0a",
   "metadata": {},
   "source": [
    "## Start of the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc2f27-cf90-4f8b-b373-1e2a3d6de4b2",
   "metadata": {},
   "source": [
    "First we create a user ID and upload the state..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1176e8f-3198-4b14-b22b-9b53e6189f4c",
   "metadata": {},
   "source": [
    "### Single Stream Cocurrent Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75e8eb-53fa-42cc-96ed-74cca703f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(chunk, df, data_schema, data_packaging_scheme, nlp_ner):\n",
    "    \"\"\"\n",
    "    Processes a chunk of indices from the given dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----\n",
    "    df: The pandas dataframe that entity recognition is being done on.\n",
    "    chunk: The chunk of indices to proceses in the given df.\n",
    "\n",
    "    Returns\n",
    "    ---- \n",
    "    A list of dictionary items that constitute data for an article.\n",
    "    \"\"\"\n",
    "    ignore_article_types = [\"National News\", \"International News\", \"Programs\", \"Digital Mural\", \"Jazz\", \"Celtic\"]\n",
    "\n",
    "    discarded_articles = []\n",
    "    dataset_df = data_schema\n",
    "    neighborhoods = set()\n",
    "    census_tracts = set()\n",
    "    try: \n",
    "        # for idx in tqdm(chunk, desc='Processing Entity Recognition'):\n",
    "        for idx in chunk:\n",
    "            # Maybe nested 'try:' are cursed\n",
    "            try:\n",
    "                #if df['Section'][idx] not in ignore_article_types and df['Type'][idx] == 'Article':\n",
    "                if df['Section'][idx] not in ignore_article_types:\n",
    "                    headline = str(df['Label'][idx])\n",
    "                    text = str(df['Body'][idx])\n",
    "                    \n",
    "                    sentences = get_sentences(text)\n",
    "            \n",
    "                    # # get lede first 5 sentences, can change the number of sentences\n",
    "                    text_5 = get_snippet(sentences, 5)\n",
    "                    text_10 = get_snippet(sentences, 5, False) # get sentences 5-10\n",
    "                    text_remain = get_snippet(sentences, 5, False, True)\n",
    "            \n",
    "                    # get entities, returns existing entities that have been seen before and new entities as sets \n",
    "                    check_order = [\n",
    "                        (run_entity_recognition(headline, nlp_ner, drop_geos, saved_geocodes), \"headline\"), \n",
    "                        (run_entity_recognition(text_5, nlp_ner, drop_geos, saved_geocodes), \"first 5 sentences\"), \n",
    "                        (run_entity_recognition(text_10, nlp_ner, drop_geos, saved_geocodes), \"next 5 sentences\"),\n",
    "                        (run_entity_recognition(text_remain, nlp_ner, drop_geos, saved_geocodes), \"remaining text\")\n",
    "                    ]\n",
    "            \n",
    "                    for (entities, method) in check_order:\n",
    "                        check_text, location_geocode, existing_loc_geocode = check_snippets(secret.API_KEY, entities[1], entities[0])\n",
    "                        if not check_text:\n",
    "                            discarded_articles.append(df['Tagging'][idx])\n",
    "                            break \n",
    "        \n",
    "                    # No Census tracts we want is detected\n",
    "                    if (len(existing_loc_geocode) == 0 and len(location_geocode) == 0):\n",
    "                        discarded_articles.append(df['Tagging'][idx])\n",
    "                        continue\n",
    "                    \n",
    "                    pipeline_output = run_pipeline(year, dsource, dname, state, existing_loc_geocode, location_geocode, mappings)\n",
    "                    \n",
    "                    if (pipeline_output):\n",
    "                        for output in pipeline_output:\n",
    "                            if ('neighborhood' in output[list(output.keys())[0]] and 'tract' in output[list(output.keys())[0]]):\n",
    "                                neighborhood = output[list(output.keys())[0]]['neighborhood']\n",
    "                                census_tract = output[list(output.keys())[0]]['tract']\n",
    "                                neighborhoods.add(neighborhood)\n",
    "                                census_tracts.add(census_tract)\n",
    "                            else:\n",
    "                                print(\"Skipped an entry!\")\n",
    "                                continue\n",
    "                    \n",
    "                    # If we have valid entity recognition | We have both some neighborhoods and census tracts\n",
    "                    if (len(neighborhoods) != 0 and len(census_tracts) != 0):\n",
    "                        data_packaging_scheme(\n",
    "                            dataset_df, # This is the data scheme we are using\n",
    "                            df['Tagging'][idx],\n",
    "                            list(neighborhoods),\n",
    "                            df['Section'][idx],\n",
    "                            list(census_tracts),\n",
    "                            df['Byline'][idx],\n",
    "                            df['Body'][idx],\n",
    "                            df['Tagging'][idx],\n",
    "                            df['Label'][idx],\n",
    "                            df['Headline'][idx],\n",
    "                            df['Publish Date'][idx],\n",
    "                            \"GBH\", # I hard coded this as we have one main client\n",
    "                            df['Paths'][idx],\n",
    "                            # No Open AI Labels yet\n",
    "                            method,\n",
    "                            existing_loc_geocode | location_geocode\n",
    "                        )\n",
    "                    neighborhoods.clear()\n",
    "                    census_tracts.clear()\n",
    "                else:\n",
    "                    discarded_articles.append(df['Tagging'][idx])\n",
    "            except Exception as e: # Loop inbounded error\n",
    "                print(f\"[Error] process_data() ran into an error! Continuing... \\n[Raw Error]: {e}\")\n",
    "        ## Convert to Pandas dataframe...\n",
    "        new_df = pd.DataFrame(dataset_df)\n",
    "        return new_df, discarded_articles\n",
    "    except Exception as e: \n",
    "        print(f\"[Fatal Error] process_data() ran into an Error! Data is not saved!\\nRaw Error:{e}\")\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce29345a-e3dd-4184-91c6-c089e3e2e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "# To run the pipeline, two things we need to have defined is the data_schmea and data packing func\n",
    "data_schema = {\n",
    "        \"id\": [],\n",
    "        \"neighborhoods\": [],\n",
    "        \"position_section\": [],\n",
    "        \"tracts\": [],\n",
    "        \"author\": [],\n",
    "        \"body\": [],\n",
    "        \"content_id\": [],\n",
    "        \"hl1\": [],\n",
    "        \"hl2\": [],\n",
    "        \"pub_date\": [],\n",
    "        \"pub_name\": [],\n",
    "        \"link\": [],\n",
    "        \"method\": [],\n",
    "        \"ent_geocodes\": []\n",
    "    }\n",
    "\n",
    "# Data packing scheme, function attributes must be len(data_schema) + 1\n",
    "# Ideally, the developer should provide a function to figure out how to pack the data based on their needs\n",
    "def package_data_to_dict(\n",
    "    data_schema, \n",
    "    id,\n",
    "    neighborhoods,\n",
    "    position_section,\n",
    "    tracts,\n",
    "    author,\n",
    "    body,\n",
    "    content_id,\n",
    "    hl1,\n",
    "    hl2,\n",
    "    pub_date,\n",
    "    pub_name,\n",
    "    link,\n",
    "    method,\n",
    "    ent_geocodes\n",
    "):\n",
    "    try:\n",
    "        args, _, _, values = inspect.getargvalues(inspect.currentframe())\n",
    "        args.pop(0) # we pop off data_schema\n",
    "        for arg in args:\n",
    "            data_schema[arg].append(values[arg])\n",
    "    except KeyError as ke:\n",
    "        print(\"Key not found in data schema!\")\n",
    "        print(f\"Raw Error: {ke}\")\n",
    "        \n",
    "    return data_schema\n",
    "\n",
    "new_df, discarded_articles = process_data(list(range(len(df))), df, data_schema, package_data_to_dict, nlp_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b076bd58-c9d9-4a85-8853-13a09cad6bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "discarded_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4581ac7-11d5-4d83-9566-2d6ab0b2812d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f57bb-f693-4d40-aa6f-a11a82ac8793",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777241a5-5697-4999-9177-2d52582de279",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_articles = new_df\n",
    "unseen_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c47de7-7d29-45e5-bb49-795b9d4b9990",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_labels = heir_data\n",
    "unseen_articles = unseen_articles.dropna(subset=['content_id'])\n",
    "\n",
    "topics, probs = nlp_topic.transform(unseen_articles['body']) # get bertopics for each article\n",
    "unseen_articles['bertopic_topic_label'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efcd923-c4aa-46d7-acf2-e245e76367cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add open ai label to bglobe dataframe in new column\n",
    "unseen_label_name = [openai_labels[unseen_articles['bertopic_topic_label'][i]]['openai'] \n",
    "              if int(unseen_articles['bertopic_topic_label'][i]) != -1 else \"\" for i in range(len(unseen_articles))]\n",
    "unseen_articles['openai_label'] = unseen_label_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad09fc-6db1-40bb-8c2d-b51fa78b2c69",
   "metadata": {},
   "source": [
    "This will be the raw data that is saved in the ML data base for data analysis by the data science team. Naturally, we will strip the data that will allow the back-end to consume it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac52d9ab-2761-43d2-8b2d-be61fa44dfc7",
   "metadata": {},
   "source": [
    "### Some User ID & Upload ID Stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d0bc84-279f-4a45-be97-2f0fc91f2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_articles[\"userID\"] = userID\n",
    "unseen_articles[\"uploadID\"] = unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e821cdfc-1be3-4c75-b9e8-f8168ce2c09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_articles.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6d572-9631-4f75-8dca-58b0aad3f1c4",
   "metadata": {},
   "source": [
    "### Format for GraphQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3cb5c8-3d3e-44f9-94d8-05116dd676f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "packaged_data_df = unseen_articles.drop(columns=[\n",
    "    'method',\n",
    "    'ent_geocodes',\n",
    "    'bertopic_topic_label'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eecda7f-0aa1-4fbc-b1ba-3625df51eff8",
   "metadata": {},
   "source": [
    "### Force a reconnection to Production Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a267cd2-c6a6-454d-b346-5fbcfe806604",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_prod = connect_MongoDB_Prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b37381-aa74-4e9f-8a24-bef762ed390c",
   "metadata": {},
   "source": [
    "## Conversion to Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3421a63a-6c58-4133-89a3-c96cdafb0b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_collection_name = \"articles_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51de2f97-e226-49f3-91e5-bc712a6860a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "packaged_data_df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478c0fe-0a20-4f35-a056-286726f2dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_dict = packaged_data_df.T.to_dict('dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be2077-768c-4f43-b6b4-64e8597f544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_list(s):\n",
    "    if(s != ''):\n",
    "        return [s]\n",
    "    else:\n",
    "        return []  # Return the string as a single-element list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf1cd8-7513-4376-9d0b-2eda1e426d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_payload = []\n",
    "\n",
    "for article_key in article_dict.keys():\n",
    "    article = article_dict[article_key]\n",
    "    if ('openai_label' not in article):\n",
    "        article[\"openai_label\"] = []\n",
    "    else:\n",
    "        article[\"openai_label\"] = string_to_list(article[\"openai_label\"])\n",
    "    article_payload.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9673e06e-ca66-4b17-8faa-22651f78f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(article_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a2ec33-1151-4f4e-91a5-4e0985628759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existence of collection\n",
    "collection_list = db_prod.list_collection_names()\n",
    "\n",
    "if articles_collection_name not in collection_list:\n",
    "    db_prod.create_collection(articles_collection_name)\n",
    "    print(f\"Collection '{articles_collection_name}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29093541-8254-472e-8946-88cac8bb54b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "insertion_result = db_prod[articles_collection_name].insert_many(article_payload)\n",
    "print(\"Articles Successfully inserted!\")\n",
    "# print(\"Documents have been successfully inserted with the following IDs:\")\n",
    "# print(insertion_result.inserted_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac0c645-3e17-4564-afa9-7c3d0f2dcfd4",
   "metadata": {},
   "source": [
    "## Conversion to Neighborhood "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777baac1-5c36-47d1-b7a9-272483f4e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_collection_name = \"neighborhood_data\"\n",
    "neigh_collection = db_prod[neigh_collection_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af889b7-1385-4ce9-8947-ef83e0838c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existence of collection\n",
    "collection_list = db_prod.list_collection_names()\n",
    "\n",
    "if neigh_collection_name not in collection_list:\n",
    "    db_prod.create_collection(neigh_collection_name)\n",
    "    print(f\"Collection '{neigh_collection_name}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906b2b9b-a187-406e-806a-f6532a958874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that the neighborhood list we get is well ordered\n",
    "# We also assume that the lengths between each list is the same\n",
    "neighborhood_list = packaged_data_df['neighborhoods'].to_numpy()\n",
    "tagging_list = packaged_data_df['content_id'].to_numpy()\n",
    "\n",
    "for n_idx in range(len(neighborhood_list)):\n",
    "    neigh_list = neighborhood_list[n_idx]\n",
    "    \n",
    "    for neigh in neigh_list:\n",
    "        # Here we update the tags/articles by neighborhood\n",
    "        neigh_collection.find_one_and_update(\n",
    "            {'value': neigh},\n",
    "            {'$addToSet': {'articles': tagging_list[n_idx]}},\n",
    "            upsert = True # Creates a new document of it if it doesn't exist\n",
    "        )       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc4cef2-0a76-4710-9b96-50d0dc2ab39e",
   "metadata": {},
   "source": [
    "## Conversion to Topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea51be-7451-471b-a565-2221ce1d4b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_collection_name = \"topics_data\"\n",
    "topic_collection = db_prod[topics_collection_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e13972-9234-47ec-b87c-b287582f5050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existence of collection\n",
    "collection_list = db_prod.list_collection_names()\n",
    "\n",
    "if topics_collection_name not in collection_list:\n",
    "    db_prod.create_collection(topics_collection_name)\n",
    "    print(f\"Collection '{topics_collection_name}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93439dfd-112f-465a-9db5-b9a12606899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that the topics list we get is well ordered\n",
    "# We also assume that the lengths between each list is the same\n",
    "topics_list = packaged_data_df['position_section'].to_numpy()\n",
    "tagging_list = packaged_data_df['content_id'].to_numpy()\n",
    "\n",
    "for n_idx in range(len(topics_list)):\n",
    "    topic = topics_list[n_idx]\n",
    "    \n",
    "    # Here we update the tags/articles by Topics\n",
    "    topic_collection.find_one_and_update(\n",
    "        {'value': topic},\n",
    "        {'$addToSet': {'articles': tagging_list[n_idx]}},\n",
    "        upsert = True # Creates a new document of it if it doesn't exist\n",
    "    )       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6c71b8-c7dc-451c-b3fc-a39a7a6b3941",
   "metadata": {},
   "source": [
    "## Conversion to Tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d23199c-2a48-4144-8abd-0efb0fc10a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addExistingTracts(tract_collection):\n",
    "    tracts_list = []\n",
    "\n",
    "    for tract_key in census_base.old_census_tracts['tracts_filter'].keys():\n",
    "        tracts_list.append(census_base.old_census_tracts['tracts_filter'][tract_key])\n",
    "\n",
    "    tract_collection.insert_many(tracts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ac8b4e-ce41-47f5-a3f3-725ba41fe574",
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_collection_name = \"tracts_data\"\n",
    "tract_collection = db_prod[tract_collection_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79c5bc3-70a6-44bd-a8e2-21bda1b82388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existence of collection\n",
    "collection_list = db_prod.list_collection_names()\n",
    "\n",
    "if tract_collection_name not in collection_list:\n",
    "    db_prod.create_collection(tract_collection_name)\n",
    "    print(f\"Collection '{tract_collection_name}' created.\")\n",
    "    # We add existing tracts from the old census tracts\n",
    "    addExistingTracts(tract_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad365fc-b070-4330-a1fe-fb18136825a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that the topics list we get is well ordered\n",
    "# We also assume that the lengths between each list is the same\n",
    "tracts_lists = packaged_data_df['tracts'].to_numpy()\n",
    "tagging_list = packaged_data_df['content_id'].to_numpy()\n",
    "\n",
    "for n_idx in range(len(tracts_lists)):\n",
    "    tract_list = tracts_lists[n_idx]\n",
    "\n",
    "    for tract in tract_list:\n",
    "        # Here we update the tags/articles by tracts\n",
    "        if tract_collection.find_one({'tract': tract}):\n",
    "            tract_collection.find_one_and_update(\n",
    "                {'tract': tract},\n",
    "                {'$addToSet': {'articles': tagging_list[n_idx]}},\n",
    "            )\n",
    "        else: # We didn't find one and we have to label it as unknown\n",
    "            unknown_tract = {\n",
    "                'tract': tract,\n",
    "                'neighborhood': \"unknown\",\n",
    "                'articles': []\n",
    "            }\n",
    "            tract_collection.insert_one(unknown_tract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48cc442-3ebe-49d3-bed4-867b28e7912a",
   "metadata": {},
   "source": [
    "### Add Discarded Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66183ed-b50a-40df-bed7-c97e2cf85c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "discarded_collection_name = \"discarded\"\n",
    "discarded_collection = db_prod[discarded_collection_name]\n",
    "\n",
    "for discarded_article in discarded_articles:\n",
    "    if (discarded_collection.find_one({'uploadID': unique_id})):\n",
    "        discarded_collection.find_one_and_update(\n",
    "            {'uploadID': unique_id},\n",
    "            {'$addToSet': {'content_ids': discarded_article}},\n",
    "        )\n",
    "    else: # We didn't find one and we have to label it as unknown\n",
    "        new_discard = {\n",
    "            'uploadID': unique_id,\n",
    "            'content_ids': []\n",
    "        }\n",
    "        discarded_collection.insert_one(new_discard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f3667-3f9b-42f6-a560-6bf9ee9fe16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_prod = connect_MongoDB_Prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b6b35e-adb7-4a83-ac24-70181c5f1bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_collection_name = \"articles_data\"\n",
    "articles_collection = db_prod[neigh_collection_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee264b6-aa13-4c45-abb8-cbdc60f0f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_duplicate_article(tag, articles_collection):\n",
    "\treturn articles_collection.find_one({\"content_id\": tag}) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeab0eed-f219-424e-be29-a887596a0cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_duplicate_article(\"00000175-e21a-dd9a-a37d-eadf3d080001\", articles_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dfc6e0-6f4c-4543-9980-1dbc08030953",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_collection_name = \"discarded\"\n",
    "discarded_collection = db_prod[neigh_collection_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2160833-03ad-4555-8e78-fed9d6f22611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_duplicate_discarded(tag, discarded_collection):\n",
    "    return discarded_collection.count_documents({'content_ids': {'$in': [tag]}}) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b0cd7-207a-496d-adfc-ef6260eca767",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_duplicate_discarded(\"00000175-e21a-dd9a-a37d-eadf3d080001\", discarded_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40797c56-53e5-458c-a4bb-d2a71b0e84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bdef48-cc25-464a-9817-2eaa1f7ad755",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"./Articles Nov 2020 - March 2023.csv\", low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470a7da-4c3e-4da4-a309-b266e7727071",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdc0f72-e120-4a25-856a-9966ad0b3c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[3000:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d900ad70-ecee-450b-b9a4-697b6e30968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_pd = pd.concat(\n",
    "    [\n",
    "        test['Type'],\n",
    "        test['Label'],\n",
    "        test['Headline'],\n",
    "        test['Byline'],\n",
    "        test['Section'],\n",
    "        test['Tagging'],\n",
    "        test['Paths'],\n",
    "        test['Publish Date'],\n",
    "        test['Body'],\n",
    "    ],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f5f1ef-4a57-49f8-a7b4-1d986bd920c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aede82-24f8-4532-ae1e-e85008e6ac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_pd.to_csv(\"./test_for_gcloud_presentation_1000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b884ce-8e2f-42ce-9631-064bf9debff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b30656-4668-4b8e-9195-97e4b66d933f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b593a5-e776-4261-be50-e28252726752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4387a122-d139-445b-bdb6-686deaeccbf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
