{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54971437-f2dd-4d65-a8ae-371aebb0bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import nltk\n",
    "import spacy\n",
    "import requests\n",
    "import googlemaps \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from art import *\n",
    "import zipfile\n",
    "\n",
    "import secret # API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bffcb8a9-8670-4052-a4ec-424e7423055f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zacharyg/miniconda3/envs/python_se/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer \n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d39fdd-0421-491d-b33d-873fbb18a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import joblib\n",
    "import openai\n",
    "import torch\n",
    "import pickle\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "132bca1c-25d0-418a-becb-549b1b0d99f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from bson import ObjectId\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5eb5a2-9f7d-4a8a-8f5d-a054d30f27e6",
   "metadata": {},
   "source": [
    "### Census Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07a14141-b3f3-4914-b1a1-999bc71c1451",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a class that takes in two csv's. One csv for census data and one for census data overall by neighborhoods.\n",
    "As of this creation of this class, we are using the Boston Census 2020 data. Please make sure that future \n",
    "versions follow the same path.\n",
    "\"\"\"\n",
    "class census_data():\n",
    "    def __init__(self, db, fetch_arr):\n",
    "        self.db = db\n",
    "        self.fetch_arr = fetch_arr\n",
    "        self.load_census_neigh_data()\n",
    "        self.load_old_census_data()\n",
    "        self.load_census_data()\n",
    "        \n",
    "    def load_old_census_data(self):\n",
    "        try:\n",
    "            if (self.db == None):\n",
    "                raise Exception(\"No database was given!\")\n",
    "                \n",
    "            if (self.fetch_arr[2]):\n",
    "                bucket = self.db.bucket(\"ml_naacp_model_data\")\n",
    "                blob = bucket.blob(\"Topic_Modeling_Pipeline_Data/census.json\")\n",
    "                blob.download_to_filename(\"./geodata_prod/census.json\")\n",
    "\n",
    "            old_census_tracts_db = json.load(open(\"./geodata_prod/census.json\"))\n",
    "\n",
    "            self.old_census_tracts = old_census_tracts_db\n",
    "        except Exception as err:\n",
    "            print(f\"Error loading Census Data class!\\nError: {err}\")\n",
    "            raise Exception(\"Fatal Error in Class Construction!\")\n",
    "        return\n",
    "    \n",
    "    def load_census_data(self, fetch=True):\n",
    "        try:\n",
    "            if (self.db == None):\n",
    "                raise Exception(\"No database was given!\")\n",
    "\n",
    "            if (self.fetch_arr[1]):\n",
    "                bucket = self.db.bucket(\"ml_naacp_model_data\")\n",
    "                blob = bucket.blob(\"Topic_Modeling_Pipeline_Data/census_2020.csv\")\n",
    "                blob.download_to_filename(\"./geodata_prod/census_2020.csv\")\n",
    "\n",
    "            census_tracts_df = pd.read_csv(\"./geodata_prod/census_2020.csv\")\n",
    "            \n",
    "            self.census_tracts = self.process_census_data(census_tracts_df)\n",
    "        except Exception as err:\n",
    "            print(f\"Error loading Census Data class!\\nError: {err}\")\n",
    "            raise Exception(\"Fatal Error in Class Construction!\")\n",
    "        return\n",
    "        \n",
    "    def load_census_neigh_data(self, fetch=True):\n",
    "        try:\n",
    "            if (self.db == None):\n",
    "                raise Exception(\"No database was given!\")\n",
    "                \n",
    "            if (self.fetch_arr[0]):\n",
    "                bucket = self.db.bucket(\"ml_naacp_model_data\")\n",
    "                blob = bucket.blob(\"Topic_Modeling_Pipeline_Data/census_2020_neigh.csv\")\n",
    "                blob.download_to_filename(\"./geodata_prod/census_2020_neigh.csv\")\n",
    "\n",
    "            census_neigh_data_df = pd.read_csv(\"./geodata_prod/census_2020_neigh.csv\")\n",
    "\n",
    "            self.census_neighbourhoods = self.process_census_neigh_data(census_neigh_data_df)\n",
    "        except Exception as err:\n",
    "            print(f\"Error loading Census Data class!\\nError: {err}\")\n",
    "            raise Exception(\"Fatal Error in Class Construction!\")\n",
    "        return\n",
    "\n",
    "    def process_census_data(self, df):\n",
    "        demographics = df.iloc[:,11:20]\n",
    "        geoid_tract = df['GEOCODE']\n",
    "        tract = df['TRACT']\n",
    "\n",
    "        concat_pd = pd.concat([tract, geoid_tract, demographics], axis=1)\n",
    "        concat_pd.drop(concat_pd.index[0], inplace=True)\n",
    "        concat_pd = concat_pd.rename(\n",
    "            columns={\n",
    "                'TRACT': 'tract', \n",
    "                'GEOCODE': 'geoid_tract',\n",
    "                'P0020001': 'total'        \n",
    "            }\n",
    "        )\n",
    "        return concat_pd\n",
    "\n",
    "    # A function for the future\n",
    "    def process_census_neigh_data(self, df):\n",
    "        df = df.iloc[:,:7]\n",
    "        df = df.rename(\n",
    "            columns={\n",
    "                'tract20_nbhd': 'Neighborhood',       \n",
    "            }\n",
    "        )\n",
    "        df.drop(df.index[0], inplace=True)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea03bfa-1b2e-4701-9a51-aeed551026d3",
   "metadata": {},
   "source": [
    "### Neighbourhood Mapping Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8232458e-675c-4895-874f-d63dc3959930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neighborhood_mapping():\n",
    "    def __init__(self, db, fetch_arr):\n",
    "        self.db = db\n",
    "        self.fetch_arr = fetch_arr\n",
    "        self.load_mappings()\n",
    "    \n",
    "    def load_mappings(self):\n",
    "        try:\n",
    "            if (self.db == None):\n",
    "                raise Exception(\"No database was given!\")\n",
    "                \n",
    "            if (self.fetch_arr[0]):\n",
    "                bucket = self.db.bucket(\"ml_naacp_model_data\")\n",
    "                blob = bucket.blob(\"Entity_Recognition_Pipeline_Data/tracts-neighbors.json\")\n",
    "                blob.download_to_filename(\"./geodata_prod/tracts-neighbors.json\")\n",
    "\n",
    "            if (self.fetch_arr[1]):\n",
    "                bucket = self.db.bucket(\"ml_naacp_model_data\")\n",
    "                blob = bucket.blob(\"Entity_Recognition_Pipeline_Data/blocks-neighbors.json\")\n",
    "                blob.download_to_filename(\"./geodata_prod/blocks-neighbors.json\")\n",
    "\n",
    "            block_map_db = json.load(open(\"./geo-data/blocks-neighbors.json\"))\n",
    "            tract_map_db = json.load(open(\"./geodata_prod/tracts-neighbors.json\"))\n",
    "            \n",
    "            self.tract_mapping = tract_map_db\n",
    "            self.block_mapping = block_map_db\n",
    "        except Exception as err:\n",
    "            print(f\"Error loading neighborhood mapping class!\\nError: {err}\")\n",
    "            raise Exception(\"Fatal Error in Class Construction!\")\n",
    "        return\n",
    "\n",
    "    def tract_to_neighborhood(self, tract):\n",
    "        # given a census tract return the boston neighborhood it is in \n",
    "        return self.tract_mapping[tract]\n",
    "\n",
    "    def block_to_neighborhood(self, block):\n",
    "        # given a census block return the boston neighborhood it is in \n",
    "        return self.block_mapping(block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1de31e-31d4-48b2-901a-8a85bbb719b6",
   "metadata": {},
   "source": [
    "### Geography Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b14dd18-8e65-4b2a-b1f3-f7c616125fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class geography():\n",
    "    def __init__(self, db, fetch_arr):\n",
    "        self.db = db\n",
    "        self.fetch_arr = fetch_arr\n",
    "        self.load_mass_town_entities()\n",
    "        self.load_state_entities()\n",
    "        self.load_saved_geocodes()\n",
    "        self.load_org_entities()\n",
    "\n",
    "    def load_saved_geocodes(self):\n",
    "        try:\n",
    "            if (self.db == None):\n",
    "                raise Exception(\"No database was given!\")\n",
    "                \n",
    "            if (self.fetch_arr[0]):\n",
    "                bucket = self.db.bucket(\"ml_naacp_model_data\")\n",
    "                blob = bucket.blob(\"Entity_Recognition_Pipeline_Data/saved-geocodes.json\")\n",
    "                blob.download_to_filename(\"./geodata_prod/saved-geocodes.json\")\n",
    "                \n",
    "            saved_geocodes_db = json.load(open(\"./geodata_prod/saved-geocodes.json\"))\n",
    "            \n",
    "            self.saved_geocodes = saved_geocodes_db\n",
    "        except Exception as err:\n",
    "            print(f\"Error loading geography class!\\nError: {err}\")\n",
    "            raise Exception(\"Fatal Error in Class Construction!\")\n",
    "        return\n",
    "        \n",
    "    def load_state_entities(self, fetch=True):\n",
    "        try:\n",
    "            if (self.db == None):\n",
    "                raise Exception(\"No database was given!\")\n",
    "\n",
    "            if (self.fetch_arr[1]):\n",
    "                bucket = self.db.bucket(\"ml_naacp_model_data\")\n",
    "                blob = bucket.blob(\"Entity_Recognition_Pipeline_Data/states.csv\")\n",
    "                blob.download_to_filename(\"./geodata_prod/states.csv\")\n",
    "            \n",
    "            states = pd.read_csv(\"./geodata_prod/states.csv\")\n",
    "            \n",
    "            states_set = set()\n",
    "            for idx in range(len(states)):\n",
    "                tup = (states['state'][idx], 'LOC')\n",
    "                states_set.add(tup)\n",
    "            self.state_entities = states_set\n",
    "            \n",
    "        except Exception as err:\n",
    "            print(f\"Error loading geography class!\\nError: {err}\")\n",
    "            raise Exception(\"Fatal Error in Class Construction!\")\n",
    "        return\n",
    "    \n",
    "    def load_mass_town_entities(self, fetch=True):\n",
    "        try:\n",
    "            if (self.db == None):\n",
    "                raise Exception(\"No database was given!\")\n",
    "                \n",
    "            if (self.fetch_arr[2]):\n",
    "                bucket = self.db.bucket(\"ml_naacp_model_data\")\n",
    "                blob = bucket.blob(\"Entity_Recognition_Pipeline_Data/mass-towns.csv\")\n",
    "                blob.download_to_filename(\"./geodata_prod/mass-towns.csv\")\n",
    "                \n",
    "            towns = pd.read_csv(\"./geodata_prod/mass-towns.csv\")\n",
    "\n",
    "            towns_set = set()\n",
    "            for idx in range(len(towns)):\n",
    "                tup = (towns['town'][idx], 'LOC')\n",
    "                towns_set.add(tup)\n",
    "            self.mass_town_entities = towns_set\n",
    "        except Exception as err:\n",
    "            print(f\"Error loading geography class!\\nError: {err}\")\n",
    "            raise Exception(\"Fatal Error in Class Construction!\")\n",
    "        return\n",
    "    \n",
    "    def load_org_entities(self):\n",
    "        self.org_entities = (('GBH News', 'ORG'), ('Boston Public Radio', 'ORG'), \n",
    "                             ('Supreme Court', 'ORG'), ('New York Times', 'ORG'), \n",
    "                             ('Washington Post', 'ORG'), ('CNN', 'ORG'), \n",
    "                             ('NPR', 'ORG'), ('Associated', 'ORG'), \n",
    "                             ('Press', 'ORG'), ('Senate', 'ORG'), \n",
    "                             ('Associated Press', 'ORG'), ('AP', 'ORG'), \n",
    "                             ('ABC News', 'ORG'),('CSS', 'ORG'), \n",
    "                             ('Philadelphia Inquirer', 'ORG'), ('House', 'ORG'),\n",
    "                             ('Congress', 'ORG'), ('Worcester', 'ORG'),\n",
    "                             ('FBI', 'ORG'), ('Homeland Security Department', 'ORG'),\n",
    "                             ('CDC', 'ORG'),('Fox News', 'ORG'),('The Washington Post', 'ORG'),\n",
    "                             ('States', 'LOC'), ('S.', 'LOC'), ('Massachusetts', 'ORG'),\n",
    "                             ('White House', 'ORG'), ('High School', 'ORG'),\n",
    "                             ('MIT', 'ORG'), ('Harvard University', 'ORG'),\n",
    "                             ('White House', 'LOC'),('Greater Boston', 'LOC'),\n",
    "                             ('New England', 'LOC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59775c7d-8e07-4797-890c-2bc970ff4ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bert_NER():\n",
    "    # loading bert NER model \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
    "    nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"max\")\n",
    "\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e97d42f0-236f-47eb-86ea-20f4ea246122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bert_TOPIC():\n",
    "    #loading bert Topic model\n",
    "    topic_model = BERTopic.load(\n",
    "        \"./llm_models/bglobe_519_body_230_cereal\", \n",
    "        embedding_model=SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    )\n",
    "    return topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12672df1-28f7-4d20-837a-58c692442f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_locations_bert(article_text, nlp):\n",
    "    \"\"\"\n",
    "    get location names from article using NER - bert model \n",
    "    https://huggingface.co/dslim/bert-base-NER\n",
    "    input: article_text as a string, aggregate of h1, h2, lede, and body\n",
    "    returns: locations - set of tuples of (NAME, 'LOC') and organizations - set of tuples (NAME, 'ORG) mentioned in the article\n",
    "    \"\"\"\n",
    "    \n",
    "    ner_results = nlp(article_text)\n",
    "    locations = set([(X['word'],X['entity_group']) for X in ner_results if X['entity_group'] == 'LOC'])\n",
    "    orgs = set([(X['word'], X['entity_group']) for X in ner_results if X['entity_group'] == 'ORG'])\n",
    "\n",
    "    return locations, orgs\n",
    "\n",
    "def clean_entity_results(extracted_loc, extracted_orgs, drop_geos):\n",
    "    # cleaning extracted entities from bert \n",
    "    # removing state names, and mass town names since the demographics data is too broad\n",
    "    # return cleaned set of entities\n",
    "    entity_result = extracted_loc | extracted_orgs\n",
    "\n",
    "    for tup in extracted_loc | extracted_orgs:\n",
    "        if len(tup[0]) <= 1:\n",
    "            entity_result.remove(tup)\n",
    "        elif tup in drop_geos.state_entities:\n",
    "            entity_result.remove(tup)\n",
    "        elif tup in drop_geos.mass_town_entities:\n",
    "            entity_result.remove(tup)\n",
    "        elif tup in drop_geos.org_entities:\n",
    "            entity_result.remove(tup)\n",
    "    return entity_result\n",
    "\n",
    "def remove_existing_geocodes(entity_result, saved_geocodes):\n",
    "    # check if any locations or organizations were recognized\n",
    "    # check if the geocodes already exist in dictionary\n",
    "    existing_loc_geocode = {}\n",
    "    new_loc_geocode = set()\n",
    "    for ent in entity_result:\n",
    "        try:\n",
    "            existing_loc_geocode[ent[0]] = saved_geocodes[ent[0]]\n",
    "        except KeyError:\n",
    "            new_loc_geocode.add(ent)\n",
    "    return existing_loc_geocode, new_loc_geocode\n",
    "\n",
    "def get_snippet(sentences, num_sent, lede=True, remaining_text=False):\n",
    "    \"\"\"\n",
    "    get the snippet of text from the article_text, replace single quotes\n",
    "    input: article text, and num_sent - number of sentences to return, default lede is true will return first x sentences\n",
    "           reamaining_text then must be False \n",
    "    returns: first x (num_sent) sentences\n",
    "    \"\"\"\n",
    "    #clean_text = clean_article_text(text)\n",
    "    #clean_text = \". \".join(clean_text.split(\".\")) # adding a space after period so nltk can do a better job recognizing sentences\n",
    "    #lede = nltk.sent_tokenize(clean_text)[:num_sent] # returns a list\n",
    "    \n",
    "    if lede: # get the first num_sent \n",
    "        lede_text = sentences[:num_sent]\n",
    "        result_text = \" \".join(lede_text)\n",
    "    elif remaining_text: # get rest of article num_sent * 2 until the end\n",
    "        result_text = sentences[num_sent*2:]\n",
    "        result_text = \" \".join(result_text)\n",
    "    else: # get sentences num_sent to num_sent * 2\n",
    "       result_text = sentences[num_sent:num_sent*2]\n",
    "       result_text = \" \".join(result_text) \n",
    "    \n",
    "    singleq = result_text.replace('’', \"'\")\n",
    "\n",
    "    return singleq\n",
    "\n",
    "def get_sentences(text):\n",
    "    # return article text as a list of its sentences \n",
    "\n",
    "    clean_text = clean_article_text(text)\n",
    "    clean_text = \". \".join(clean_text.split(\".\")) # adding a space after period so nltk can do a better job recognizing sentences\n",
    "    sentences = nltk.sent_tokenize(clean_text)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def clean_article_text(text):\n",
    "    # get text, removing html tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    clean_text = soup.get_text()\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb03f53c-89e3-49ae-9cbf-2641ee4fc351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location_geocode(API_KEY, locations):\n",
    "    \"\"\"\n",
    "    getting coordinates from location names in articles \n",
    "    input: google maps platform API KEY, locations article \n",
    "    return: dictionary of location names (key) with coordinates (value as a dictionary with lat and lon as keys)\n",
    "    \"\"\"\n",
    "    gmaps = googlemaps.Client(key=API_KEY)\n",
    "    results = {}\n",
    "\n",
    "    # getting coordinates\n",
    "    for place in locations:\n",
    "        # we can constrain google geocode api search to massachusetts or us - census geocoder will not work for places outside of U.S \n",
    "        #geocode_result = gmaps.geocode(place[0] + \", Suffok County, MA, USA\") # place is a tuple, where first value is the location name \n",
    "        geocode_result = gmaps.geocode(place[0] + \", Suffolk County\",  components={\"administrative_area_level\": \"MA\", \n",
    "                                                                                   \"country\": \"US\"})\n",
    "        #print(geocode_result)\n",
    "        #print()\n",
    "        temp = {}\n",
    "        try:\n",
    "            geocode_components = geocode_result[0]['address_components']\n",
    "            for i, addr_comp in enumerate(geocode_components):\n",
    "                if 'administrative_area_level_2' in addr_comp['types']:\n",
    "                    if \"Suffolk County\" == addr_comp['short_name'] and i != 0:\n",
    "                        temp['lat'] = geocode_result[0]['geometry']['location']['lat']\n",
    "                        temp['lon'] = geocode_result[0]['geometry']['location']['lng']\n",
    "                        results[place[0]] = temp\n",
    "                        \n",
    "        except IndexError: # unable to get coordinates for location\n",
    "            print(\"Unable to locate \" + place[0])\n",
    "\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18a2244b-0460-430f-8da8-4dd6ec639151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_census_geos(geocode_results):\n",
    "    \"\"\"\n",
    "    get census geographies - tract, block group, block by coordinates\n",
    "    input: google maps geocode_results as a dictionary\n",
    "    return: block, block_group, tract, county for each location\n",
    "    \"\"\"\n",
    "    census_geos = {}\n",
    "    for place in geocode_results:\n",
    "        # building the geocoding url\n",
    "        base_url = f'https://geocoding.geo.census.gov/geocoder/geographies/coordinates?'\n",
    "        survey_ver = f'&benchmark=4&vintage=4&layers=2020 Census Blocks&format=json'\n",
    "        lon = geocode_results[place]['lon']\n",
    "        lat = geocode_results[place]['lat']\n",
    "        census_geo_url = f'{base_url}x={lon}&y={lat}{survey_ver}'\n",
    "\n",
    "        # getting the census geographies \n",
    "        response = requests.get(census_geo_url)\n",
    "        response_json = response.json()\n",
    "\n",
    "        try:\n",
    "            block = response_json['result']['geographies']['2020 Census Blocks'][0]['BLOCK']\n",
    "            block_group = response_json['result']['geographies']['2020 Census Blocks'][0]['BLKGRP']\n",
    "            tract = response_json['result']['geographies']['2020 Census Blocks'][0]['TRACT']\n",
    "            county = response_json['result']['geographies']['2020 Census Blocks'][0]['COUNTY']\n",
    "            census_geos[place] = {'block': block,\n",
    "                                  'blkgrp': block_group,\n",
    "                                  'tract': tract,\n",
    "                                  'county': county}\n",
    "        except IndexError:\n",
    "            print(\"Unable to retrieve census geography for: \" + place)\n",
    "        except KeyError:\n",
    "            print(\"Location is outside of the United States: \" + place)\n",
    "    return census_geos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d211234-51b8-4d19-b7ec-446c90a98186",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# CENSUS DATA API \n",
    "# https://www.census.gov/content/dam/Census/library/publications/2020/acs/acs_api_handbook_2020_ch02.pdf\n",
    "# any user can query small quantities of data with minimal restrictions - up to 50 variables in a single query, up to 500 queries per IP address per day \n",
    "# more than 500 queries per IP address per day requires you to register for API key - www.census.gov/developers\n",
    "# https://www.census.gov/data/developers/data-sets/decennial-census.html \n",
    "\"\"\"\n",
    "def get_census_demographics(year, dsource, dname, tract, county, state):\n",
    "    # input: census year, data source, survey name, tract, county, state\n",
    "    # return: demographic data for tract mentioned\n",
    "    \n",
    "    # census variables: https://api.census.gov/data/2020/dec/pl/variables.html \n",
    "    cols = 'NAME,P2_001N,P2_002N,P2_003N,P2_004N,P2_005N,P2_006N,P2_007N,P2_008N,P2_009N,P2_010N'\n",
    "    base_url = f\"https://api.census.gov/data/{year}/{dsource}/{dname}\"\n",
    "\n",
    "    # to get tract demographics \n",
    "    census_url = f\"{base_url}?get={cols}&for=tract:{tract}&in=county:{county}&in=state:{state}\"\n",
    "\n",
    "    # to get block demographics \n",
    "    # census_url = f\"{base_url}?get={cols}&for=block:{block}&in=tract:{tract}&in=county:{county}&in=state:{state}\"\n",
    "\n",
    "    census_response = requests.get(census_url)\n",
    "    census_response_json = census_response.json()\n",
    "\n",
    "    return census_response_json\n",
    "\n",
    "def run_entity_recognition(text, nlp, drop_geos, saved_geocodes):\n",
    "    # running entity recogntion on text\n",
    "    # parse existing geocoded entities and new geocoded entities\n",
    "    try:\n",
    "        extracted_loc, extracted_orgs = get_locations_bert(text, nlp)\n",
    "        ent_result = clean_entity_results(extracted_loc, extracted_orgs, drop_geos)\n",
    "        existing_loc_geocode, new_loc_geocode = remove_existing_geocodes(ent_result, saved_geocodes)\n",
    "    except TypeError as e:\n",
    "        print(f\"No entities: {e}\")\n",
    "        existing_loc_geocode = {}\n",
    "        new_loc_geocode = set()\n",
    "\n",
    "    return existing_loc_geocode, new_loc_geocode\n",
    "\n",
    "def run_location_geocode(API_KEY, new_loc_geocode):\n",
    "    # get geocodes for NEW locations and saving them to json\n",
    "    # returns new location geocodes as dictionary \n",
    "    location_geocode = {}\n",
    "    if new_loc_geocode:\n",
    "        location_geocode = get_location_geocode(API_KEY, new_loc_geocode)\n",
    "    return location_geocode\n",
    "\n",
    "def check_snippets(API_KEY, new_entities, existing_entities):\n",
    "    location_geocode = run_location_geocode(API_KEY, new_entities)\n",
    "    existing_loc_geocode = existing_entities\n",
    "    combined_geocodes = location_geocode | existing_loc_geocode # if this is empty, then try the next snippet of text \n",
    "    return (not combined_geocodes), location_geocode, existing_loc_geocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47c9a89f-91c9-4395-b0bc-fdccd648a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(year, dsource, dname, state, existing_loc_geocode, location_geocode, mappings):\n",
    "    #location_geocode = {'Boston': {'lat': 42.3600825, 'lon': -71.0588801}, 'Massachusetts': {'lat': 42.4072107, 'lon': -71.3824374}, 'Boston city': {'lat': 42.3600825, 'lon': -71.0588801}, 'Roxbury': {'lat': 42.3125672, 'lon': -71.0898796}, 'Fitchburg': {'lat': 42.5834228, 'lon': -71.8022955}, 'Medford': {'lat': 42.4184296, 'lon': -71.1061639}}\n",
    "    #location_geocode = {'Massachusetts': {'lat': 42.4072107, 'lon': -71.3824374}, 'Salem': {'lat': 42.5197473, 'lon': -70.8954626}, 'Salem City Hall': {'lat': 42.5218851, 'lon': -70.8956157}}\n",
    "    #location_geocode = {'Salem': {'lat': 42.5197473, 'lon': -70.8954626}, 'Massachusetts': {'lat': 42.4072107, 'lon': -71.3824374}, 'Salem City Hall': {'lat': 42.5218851, 'lon': -70.8956157}}\n",
    "\n",
    "    #print(location_geocode | existing_loc_geocode)\n",
    "    \n",
    "    census_geos = get_census_geos(location_geocode | existing_loc_geocode)\n",
    "\n",
    "    result = []\n",
    "    for place_name in census_geos:\n",
    "        place_info = {}\n",
    "        county = census_geos[place_name]['county']\n",
    "        tract = census_geos[place_name]['tract']\n",
    "        \n",
    "        try:\n",
    "            demographic_results = get_census_demographics(year, dsource, dname, tract, county, state)\n",
    "\n",
    "            # build result dictionary \n",
    "            place_info[place_name] = {'county_code': county} \n",
    "            place_info[place_name] = {'county_name': demographic_results[1][0]}\n",
    "            place_info[place_name]['tract'] = tract\n",
    "            geoid_tract = state + county + tract # this includes the state and county and tract number\n",
    "            place_info[place_name]['geoid_tract'] = geoid_tract\n",
    "\n",
    "            if mappings.tract_mapping.get(geoid_tract): # get corresponding boston neighborhood \n",
    "                place_info[place_name]['neighborhood'] = mappings.tract_mapping[state + county + tract]\n",
    "            \n",
    "            place_info[place_name]['demographics'] = {\n",
    "                'p2_001n': demographic_results[1][1], # total population \n",
    "                'p2_002n': demographic_results[1][2], # total hispanic or latino \n",
    "                'p2_003n': demographic_results[1][3], # total not hispanic or latino \n",
    "                'p2_004n': demographic_results[1][4], # total not hispanic or latino - pop of one race\n",
    "                'p2_005n': demographic_results[1][5], # total not hispanic or latino - pop of one race - white alone \n",
    "                'p2_006n': demographic_results[1][6], # total not hispanic or latino - pop of one race - black or african american alone\n",
    "                'p2_007n': demographic_results[1][7], # total not hispanic or latino - pop of one race - american indian and alaska native alone\n",
    "                'p2_008n': demographic_results[1][8], # total not hispanic or latino - pop of one race - asian alone \n",
    "                'p2_009n': demographic_results[1][9], # total not hispanic or latino - pop of one race - native hawaiian and other pacific islander alone\n",
    "                'p2_010n': demographic_results[1][10] # total not hispanic or latino - pop of one race - some other race alone \n",
    "            } \n",
    "            \n",
    "            result.append(place_info)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Unable to get census demographics for: \" + place_name)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e6536-c08f-4a09-ae04-f11863c48d98",
   "metadata": {},
   "source": [
    "### Bootstrap the pipeline | The functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5baf9fc-6dc4-4cfb-881d-7cb73beb7c66",
   "metadata": {},
   "source": [
    "Next we have to bootstrap the pipeline and make sure that BERT is up and running/loaded and the variables needed is all good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ca0d93c-1f2d-40d4-9524-89c9e2e52227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import itertools\n",
    "import threading\n",
    "\n",
    "class Spinner:\n",
    "    def __init__(self, message, delay=0.1):\n",
    "        self.spinner = itertools.cycle(['-', '/', '|', '\\\\'])\n",
    "        self.delay = delay\n",
    "        self.message = message\n",
    "        self.thread = threading.Thread(target=self.spin)\n",
    "        self.stop_running = threading.Event()\n",
    "\n",
    "    def spin(self):\n",
    "        while not self.stop_running.is_set():\n",
    "            sys.stdout.write(next(self.spinner))  # write the next character\n",
    "            sys.stdout.flush()                    # flush stdout buffer (actual character display)\n",
    "            sys.stdout.write('\\b')                # erase the last written char\n",
    "            time.sleep(self.delay)\n",
    "\n",
    "    def start(self):\n",
    "        self.stop_running.clear()\n",
    "        sys.stdout.write(self.message + ' ')\n",
    "        self.thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        self.stop_running.set()\n",
    "        self.thread.join()                       # wait for spinner to stop\n",
    "        sys.stdout.write('✔️ OK\\n')              # write final message\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def err(self):\n",
    "        self.stop_running.set()\n",
    "        self.thread.join()                       # wait for spinner to stop\n",
    "        sys.stdout.write('❌ Error!\\n')              # write final message\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "335fcb14-1afe-4f06-921c-af58737b51c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_llm_models():\n",
    "    try:\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(\"naacp-models\")\n",
    "        blob = bucket.blob(\"BERTopic_Models/bglobe_519_body_230_cereal.zip\")\n",
    "        blob.download_to_filename(\"./llm_models/bglobe_519_body_230_cereal.zip\")\n",
    "\n",
    "        destination_file_name = \"./llm_models/bglobe_519_body_230_cereal.zip\"\n",
    "        with zipfile.ZipFile(destination_file_name, 'r') as zip_ref: # Extract the ZIP\n",
    "            zip_ref.extractall(\"./llm_models\")\n",
    "        os.remove(destination_file_name) # Remove the ZIP for cleanup\n",
    "    except Exception as e:\n",
    "        print(f\"Model fetching failed! {e}\")\n",
    "        raise Exception(\"Fatal Error in Fetching LLM models. Exiting...\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93768787-9bb8-4f20-ac3e-cec1ccac475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_heirarch_data(db, fetch=True):\n",
    "    try:\n",
    "        if (db == None):\n",
    "            raise Exception(\"No database was given!\")\n",
    "        if (fetch):\n",
    "            bucket = db.bucket(\"ml_naacp_model_data\")\n",
    "            blob = bucket.blob(\"Topic_Modeling_Pipeline_Data/openai_label_from_taxonomy_structured_230.json\")\n",
    "            blob.download_to_filename(\"./geodata_prod/openai_label_from_taxonomy_structured_230.json\")\n",
    "\n",
    "        heirarch_db = json.load(open(\"./geodata_prod/openai_label_from_taxonomy_structured_230.json\"))\n",
    "        \n",
    "        if (heirarch_db == None):\n",
    "            raise Exception(f\"heirarch_db returned None!\")\n",
    "            \n",
    "        return heirarch_db\n",
    "    except Exception as err:\n",
    "        print(f\"Error loading hierarchal data!\\nError: {err}\")\n",
    "        raise Exception(\"Fatal Error in fetching hierarchal data!\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52ce8c18-0000-40b8-93ff-8284c76571d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_pipeline():\n",
    "    try:\n",
    "        file_manifest = [\n",
    "            \"tracts-neighbors.json\", # Neigh Mappings\n",
    "            \"blocks-neighbors.json\", # Neigh Mappings\n",
    "            \"saved-geocodes.json\", # Geography\n",
    "            \"states.csv\", # Geography\n",
    "            \"mass-towns.csv\", # Geography\n",
    "            \"openai_label_from_taxonomy_structured_230.json\", # Topic Modeling\n",
    "            \"census_2020_neigh.csv\", # Census\n",
    "            \"census_2020.csv\", # Census\n",
    "            \"census.json\" # Census\n",
    "        ]\n",
    "        dependency_resolver_arr = []\n",
    "        \n",
    "        tprint(\"BU Spark!\", font=\"sub-zero\")\n",
    "        print(\"Bootstrapping pipeline... (This should only run once!)\")\n",
    "        print()\n",
    "        spinner = Spinner(\"Setting up variables...\")\n",
    "        spinner.start()\n",
    "        year='2020'\n",
    "        dsource='dec' # which survey are we interested in ? decennial \n",
    "        dname='pl' # a dataset within a survey, pl - redistricting data \n",
    "        state='25' # state code \n",
    "        spinner.stop()\n",
    "        print()\n",
    "\n",
    "        spinner = Spinner(\"Connecting to MongoDB...\")\n",
    "        spinner.start()\n",
    "        client = MongoClient(secret.MONGO_URI_NAACP) # A mock connection to catch early errors\n",
    "        spinner.stop()\n",
    "        print()\n",
    "\n",
    "        spinner = spinner = Spinner(\"Connecting to Google Cloud Storage Bucket...\")\n",
    "        spinner.start()\n",
    "        db = storage.Client()\n",
    "        spinner.stop()\n",
    "        print()\n",
    "        \n",
    "        spinner = Spinner(\"Detecting & Making local directories for Models...\")\n",
    "        spinner.start()\n",
    "        spinner.stop()\n",
    "        llm_model_directory_path = \"./llm_models\"\n",
    "        if (not os.path.exists(llm_model_directory_path)):\n",
    "            print(f\"No {llm_model_directory_path}! Creating...\")\n",
    "            os.makedirs(llm_model_directory_path)\n",
    "            spinner = Spinner(\"Fetching models...\")\n",
    "            spinner.start()\n",
    "            fetch_llm_models()\n",
    "            spinner.stop()\n",
    "        else:\n",
    "            print(f\"Found! {llm_model_directory_path}!\")\n",
    "            spinner = Spinner(\"Validating model files...\")\n",
    "            spinner.start()\n",
    "            if (not os.path.isfile(\"./llm_models/BERTopic_CPU_M1\")):\n",
    "                spinner.stop()\n",
    "                spinner = Spinner(\"Model file not found! Pulling models...\")\n",
    "                spinner.start()\n",
    "                fetch_llm_models()\n",
    "                spinner.stop()\n",
    "            else:\n",
    "                spinner.stop()\n",
    "\n",
    "        print(\"Model files successfully validated.\\n\")\n",
    "\n",
    "        spinner = Spinner(\"Detecting & Making local directories for Geographic Data...\")\n",
    "        spinner.start()\n",
    "        spinner.stop()\n",
    "        geodata_directory_path = \"./geodata_prod\"\n",
    "        if (not os.path.exists(geodata_directory_path)):\n",
    "            print(f\"No {geodata_directory_path}! Creating...\")\n",
    "            os.makedirs(geodata_directory_path)\n",
    "            dependency_resolver_arr = len(file_manifest) * [True]\n",
    "        else:\n",
    "            print(f\"Found! {geodata_directory_path}!\")\n",
    "            spinner = Spinner(\"Validating geodata files...\")\n",
    "            spinner.start()\n",
    "            for file in file_manifest:\n",
    "                if (not os.path.isfile(f\"./geodata_prod/{file}\")):\n",
    "                    dependency_resolver_arr.append(True)\n",
    "                else:\n",
    "                    dependency_resolver_arr.append(False)\n",
    "            spinner.stop()\n",
    "        print(\"Geodata files successfully validated and updated dependecy array.\\n\")\n",
    "        \n",
    "        spinner = Spinner(\"Fetching OpenAI Hierarchal Mappings...\")\n",
    "        spinner.start()\n",
    "        heir_data = load_heirarch_data(\n",
    "            db,\n",
    "            fetch=dependency_resolver_arr[5]\n",
    "        )\n",
    "        spinner.stop()\n",
    "        print()\n",
    "        \n",
    "        spinner = Spinner(\"Instantiating classes...\")\n",
    "        spinner.start()\n",
    "        mappings = neighborhood_mapping(\n",
    "            db=db,\n",
    "            fetch_arr=dependency_resolver_arr[:2]\n",
    "        )\n",
    "        drop_geos = geography(\n",
    "            db=db,\n",
    "            fetch_arr=dependency_resolver_arr[2:5] \n",
    "        )\n",
    "        census_base = census_data(\n",
    "            db=db,\n",
    "            fetch_arr=dependency_resolver_arr[6:] \n",
    "        )\n",
    "        saved_geocodes = drop_geos.saved_geocodes \n",
    "        spinner.stop()\n",
    "        print()\n",
    "        \n",
    "        spinner = Spinner(\"Loading Models...\")\n",
    "        spinner.start()\n",
    "        nlp_ner = load_bert_NER()\n",
    "        nlp_topic = load_bert_TOPIC()\n",
    "        spinner.stop()\n",
    "        \n",
    "        print(\"\\nBootstrap complete!\\n\")\n",
    "        return year, dsource, dname, state, drop_geos, mappings, census_base, heir_data, saved_geocodes, nlp_ner, nlp_topic, db\n",
    "    except Exception as e:\n",
    "        spinner.err()\n",
    "        print(f\"Bootstrap Failed!!!\\nFatal Error:{e}\")\n",
    "        raise Exception(\"Fatal Error in Bootstrapping ML Pipeline. Exiting...\")\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a88fca66-55ac-4767-a0b8-67a30cf211f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_bootstrap(year, dsource, dname, state, drop_geos, mappings, census_base, heir_data, saved_geocodes, nlp_ner, nlp_topic, db):\n",
    "    try:\n",
    "        spinner = Spinner(\"Validating Bootstrap variables...\")\n",
    "        spinner.start()\n",
    "\n",
    "        variable_manifest = {\n",
    "            \"year\":year, \n",
    "            \"dsource\": dsource, \n",
    "            \"dname\": dname, \n",
    "            \"state\": state,\n",
    "            \"drop_geos\": drop_geos,\n",
    "            \"mappings\": mappings,\n",
    "            \"census_base\": census_base,\n",
    "            \"heir_data\": heir_data,\n",
    "            \"saved_geocodes\": saved_geocodes, \n",
    "            \"nlp_ner\": nlp_ner, \n",
    "            \"nlp_topic\": nlp_topic,\n",
    "            \"db\": db\n",
    "        }\n",
    "        \n",
    "        for var in variable_manifest.keys():\n",
    "            if (variable_manifest[var] == None):\n",
    "                raise Exception(f\"{var} returned None!. Exiting...\")\n",
    "        spinner.stop()\n",
    "        print()\n",
    "        print(\"Validation Complete! Everything seems to be in order.\")\n",
    "    except Exception as e:\n",
    "        spinner.err()\n",
    "        print(f\"Bootstrap Validation Failed!!!\\nFatal Error:{e}\")\n",
    "        raise Exception(\"Fatal Error in Bootstrapping Validation. Exiting...\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0debc854-866b-4b89-a188-972f035c548e",
   "metadata": {},
   "source": [
    "# Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad34dff6-711e-45f8-9ace-66d63179bb76",
   "metadata": {},
   "source": [
    "Ideally, we should only be concerned with articles that yield a recognition of location. If not, we chuck it out!.\n",
    "\n",
    "I think we should append it to the end of the df to make a cleaner since this needs to be passed to topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f8e2aa6-13f7-4b40-abd1-63bcdb6df165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ______     __  __        ______     ______   ______     ______     __  __    \n",
      "/\\  == \\   /\\ \\/\\ \\      /\\  ___\\   /\\  == \\ /\\  __ \\   /\\  == \\   /\\ \\/ /    \n",
      "\\ \\  __<   \\ \\ \\_\\ \\     \\ \\___  \\  \\ \\  _-/ \\ \\  __ \\  \\ \\  __<   \\ \\  _\"-.  \n",
      " \\ \\_____\\  \\ \\_____\\     \\/\\_____\\  \\ \\_\\    \\ \\_\\ \\_\\  \\ \\_\\ \\_\\  \\ \\_\\ \\_\\ \n",
      "  \\/_____/   \\/_____/      \\/_____/   \\/_/     \\/_/\\/_/   \\/_/ /_/   \\/_/\\/_/ \n",
      "                                                                              \n",
      "\n",
      "Bootstrapping pipeline... (This should only run once!)\n",
      "\n",
      "Setting up variables... ✔️ OK\n",
      "\n",
      "Connecting to MongoDB... ✔️ OK\n",
      "\n",
      "Connecting to Google Cloud Storage Bucket... ✔️ OK\n",
      "\n",
      "Detecting & Making local directories for Models... ✔️ OK\n",
      "Found! ./llm_models!\n",
      "Validating model files... ✔️ OK\n",
      "Model file not found! Pulling models... ✔️ OK\n",
      "Model files successfully validated.\n",
      "\n",
      "Detecting & Making local directories for Geographic Data... ✔️ OK\n",
      "Found! ./geodata_prod!\n",
      "Validating geodata files... ✔️ OK\n",
      "Geodata files successfully validated and updated dependecy array.\n",
      "\n",
      "Fetching OpenAI Hierarchal Mappings... ✔️ OK\n",
      "\n",
      "Instantiating classes... ✔️ OK\n",
      "\n",
      "Loading Models... |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-large-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b✔️ OK\n",
      "\n",
      "Bootstrap complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Bootstrap Run\n",
    "year, dsource, dname, state, drop_geos, mappings, census_base, heir_data, saved_geocodes, nlp_ner, nlp_topic, db = bootstrap_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f52d17c-34f7-4730-8406-62b74cd85aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating Bootstrap variables... ✔️ OK\n",
      "\n",
      "Validation Complete! Everything seems to be in order.\n"
     ]
    }
   ],
   "source": [
    "validate_bootstrap(year, dsource, dname, state, drop_geos, mappings, census_base, heir_data, saved_geocodes, nlp_ner, nlp_topic, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aadd0f-ffa5-4352-9f63-85228b33fb78",
   "metadata": {},
   "source": [
    "### Some Useful MongoDB functions for connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bf5f973-caf5-4658-900f-7e63e5e26601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_MongoDB_Prod():\n",
    "    try:\n",
    "        client = MongoClient(secret.MONGO_URI_NAACP)\n",
    "        db = client['se_naacp_db']\n",
    "        print(\"[INFO] Connected to MongoDB Production Database!\")\n",
    "        return db\n",
    "    except Exception as err:\n",
    "        raise Exception(\"[Fatal Error!] Failed to Connect to MongoDB. [No retry implemented]\")\n",
    "    return \n",
    "\n",
    "def bootstrap_MongoDB_Prod(db_prod, defined_collection_names):\n",
    "    \"\"\"\n",
    "    Adds the upload collection and other necessities that both the GraphQL and AI Pipeline share.\n",
    "    Sets up the databse.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spinner = Spinner(\"Checking and Bootstrapping Production DB...\\n\")\n",
    "        spinner.start()\n",
    "        if (db_prod == None):\n",
    "            raise Exception(\"No database was given!\")\n",
    "            \n",
    "        # Here we check for the upload collection and make it if it doesn't exist\n",
    "        collection_list = db_prod.list_collection_names()\n",
    "        for collection in defined_collection_names:\n",
    "            if collection not in collection_list:\n",
    "                db_prod.create_collection(collection)\n",
    "                print(f\"[INFO] Collection '{collection}' created.\\n\")\n",
    "        spinner.stop()\n",
    "    except Exception as err:\n",
    "        spinner.err()\n",
    "        print(f\"[Error!] Error in Bootstrapping MongoDB Prod DB\\nError: {err}\")\n",
    "        raise Exception(\"Fatal Error in MongoDB Boostrap\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5bb859b-0c82-4757-ae19-79a1e85f2097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected to MongoDB Production Database!\n"
     ]
    }
   ],
   "source": [
    "db_prod = connect_MongoDB_Prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54c81a97-5ad3-404a-82e3-364fc929b700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking and Bootstrapping Production DB...\n",
      " ✔️ OK\n"
     ]
    }
   ],
   "source": [
    "defined_collection_names = [\"uploads\", \"discarded\"]\n",
    "\n",
    "bootstrap_MongoDB_Prod(db_prod, defined_collection_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fd54a2-b842-4534-ae96-425bf7a1900d",
   "metadata": {},
   "source": [
    "# The Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41faf447-b0ed-4db5-aad5-0eaf38bbf1b3",
   "metadata": {},
   "source": [
    "### Incoming CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8e0339e-013e-44f8-8af4-09de19949dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12795, 12)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./Articles Nov 2020 - March 2023.csv\", low_memory=False)\n",
    "df = df.iloc[:, : 12]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9a8b998-1ed3-41e2-a228-ad1072acd6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Label</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Byline</th>\n",
       "      <th>Section Navigation</th>\n",
       "      <th>Section</th>\n",
       "      <th>Tagging</th>\n",
       "      <th>Title</th>\n",
       "      <th>Paths</th>\n",
       "      <th>Publish Date</th>\n",
       "      <th>Has Path?</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Article</td>\n",
       "      <td>A Celtic Playlist For Easter</td>\n",
       "      <td>A Celtic Playlist For Easter</td>\n",
       "      <td>Brian O'Donovan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Celtic</td>\n",
       "      <td>0000016a-3bcb-d661-af7b-7bff0d200001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/music/celtic/2019/04/20/a-celtic-playlist-for...</td>\n",
       "      <td>Mon Mar 29 15:22:35 EDT 2021</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>The above is a continuous stream.  &lt;br/&gt;&lt;br/&gt;I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Article</td>\n",
       "      <td>Songs Of War And Remembrance: Memorial Day</td>\n",
       "      <td>Songs Of War And Remembrance: Memorial Day</td>\n",
       "      <td>Brian O'Donovan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Celtic</td>\n",
       "      <td>0000016a-f0d5-dbfd-a56f-f4dfc34c0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/music/celtic/2019/05/25/songs-of-war-and-reme...</td>\n",
       "      <td>Sat May 29 01:00:45 EDT 2021</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Click above for the audio of a special segment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Article</td>\n",
       "      <td>Words And Music: Father's Day</td>\n",
       "      <td>Words And Music: Father's Day</td>\n",
       "      <td>Brian O'Donovan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Celtic</td>\n",
       "      <td>0000016b-539d-d757-adef-f7bd4a5f0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/music/celtic/2019/06/13/words-and-music-fathe...</td>\n",
       "      <td>Fri Jun 18 01:00:28 EDT 2021</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>In honor of Father&amp;#39;s Day, this segment of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Article</td>\n",
       "      <td>Celebrating The Birthday Of Robert Burns — Jan...</td>\n",
       "      <td>Celebrating The Birthday Of Robert Burns — Jan...</td>\n",
       "      <td>Brian O'Donovan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Celtic</td>\n",
       "      <td>0000016f-c48a-d14c-a57f-e59b02310001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/music/celtic/2020/01/21/celebrating-the-birth...</td>\n",
       "      <td>Sat Jan 23 08:47:27 EST 2021</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Robert Burns is known as &amp;quot;Scotland&amp;#39;s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Article</td>\n",
       "      <td>Ten Celtic Love Songs For St. Valentine's Day</td>\n",
       "      <td>Ten Celtic Love Songs For St. Valentine's Day</td>\n",
       "      <td>Brian O'Donovan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Celtic</td>\n",
       "      <td>00000170-273a-d4d2-a378-677e83f60001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/celtic/ValentinesDay (Permalink)</td>\n",
       "      <td>Wed Feb 10 12:09:13 EST 2021</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Folk music generally, and Celtic music, in par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12790</th>\n",
       "      <td>Article</td>\n",
       "      <td>5 key takeaways from the Trump indictment news</td>\n",
       "      <td>5 key takeaways from the Trump indictment news</td>\n",
       "      <td>Emily Olson, Emma Bowman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>National News</td>\n",
       "      <td>00000187-3753-da17-afc7-f77b1ea40001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/national-news/2023/03/31/5-key-takeaways-from...</td>\n",
       "      <td>Fri Mar 31 06:26:00 EDT 2023</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Former president Donald Trump has been indicte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12791</th>\n",
       "      <td>Article</td>\n",
       "      <td>Harvard professor says government should pause...</td>\n",
       "      <td>Harvard professor says government should pause...</td>\n",
       "      <td>Alexi Cohan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Science and Technology</td>\n",
       "      <td>00000187-37f6-d65f-a1f7-bffee1410001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/science-and-technology/2023/03/31/harvard-pro...</td>\n",
       "      <td>Fri Mar 31 11:06:14 EDT 2023</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Artificial intelligence has advanced rapidly i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12792</th>\n",
       "      <td>Article</td>\n",
       "      <td>Why Trump isn't the first president to face ar...</td>\n",
       "      <td>Why Trump isn't the first president to face ar...</td>\n",
       "      <td>Dustin Jones, Kaitlyn Radde</td>\n",
       "      <td>NaN</td>\n",
       "      <td>National News</td>\n",
       "      <td>00000187-37f7-da17-afc7-f7ffec2a0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/national-news/2023/03/31/why-trump-isnt-the-f...</td>\n",
       "      <td>Fri Mar 31 09:05:00 EDT 2023</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Former President Donald Trump was indicted Thu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12793</th>\n",
       "      <td>Article</td>\n",
       "      <td>These cockroaches tweaked their mating rituals...</td>\n",
       "      <td>These cockroaches tweaked their mating rituals...</td>\n",
       "      <td>Ari Daniel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>News</td>\n",
       "      <td>00000187-382e-da17-afc7-f96fcf4c0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/news/2023/03/31/these-cockroaches-tweaked-the...</td>\n",
       "      <td>Fri Mar 31 10:02:00 EDT 2023</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Human attempts to kill cockroaches with sugary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12794</th>\n",
       "      <td>Article</td>\n",
       "      <td>U.S. Capitol rioter the 'QAnon Shaman' is rele...</td>\n",
       "      <td>U.S. Capitol rioter the 'QAnon Shaman' is rele...</td>\n",
       "      <td>Juliana Kim</td>\n",
       "      <td>NaN</td>\n",
       "      <td>National News</td>\n",
       "      <td>00000187-3865-da17-afc7-f96fb8210001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/national-news/2023/03/31/u-s-capitol-rioter-t...</td>\n",
       "      <td>Fri Mar 31 11:19:00 EDT 2023</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Jacob Chansley, who received one of the longes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12795 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Type                                              Label  \\\n",
       "0      Article                      A Celtic Playlist For Easter    \n",
       "1      Article        Songs Of War And Remembrance: Memorial Day    \n",
       "2      Article                      Words And Music: Father's Day   \n",
       "3      Article  Celebrating The Birthday Of Robert Burns — Jan...   \n",
       "4      Article      Ten Celtic Love Songs For St. Valentine's Day   \n",
       "...        ...                                                ...   \n",
       "12790  Article     5 key takeaways from the Trump indictment news   \n",
       "12791  Article  Harvard professor says government should pause...   \n",
       "12792  Article  Why Trump isn't the first president to face ar...   \n",
       "12793  Article  These cockroaches tweaked their mating rituals...   \n",
       "12794  Article  U.S. Capitol rioter the 'QAnon Shaman' is rele...   \n",
       "\n",
       "                                                Headline  \\\n",
       "0                          A Celtic Playlist For Easter    \n",
       "1            Songs Of War And Remembrance: Memorial Day    \n",
       "2                          Words And Music: Father's Day   \n",
       "3      Celebrating The Birthday Of Robert Burns — Jan...   \n",
       "4          Ten Celtic Love Songs For St. Valentine's Day   \n",
       "...                                                  ...   \n",
       "12790     5 key takeaways from the Trump indictment news   \n",
       "12791  Harvard professor says government should pause...   \n",
       "12792  Why Trump isn't the first president to face ar...   \n",
       "12793  These cockroaches tweaked their mating rituals...   \n",
       "12794  U.S. Capitol rioter the 'QAnon Shaman' is rele...   \n",
       "\n",
       "                            Byline Section Navigation                 Section  \\\n",
       "0                  Brian O'Donovan                NaN                  Celtic   \n",
       "1                  Brian O'Donovan                NaN                  Celtic   \n",
       "2                  Brian O'Donovan                NaN                  Celtic   \n",
       "3                  Brian O'Donovan                NaN                  Celtic   \n",
       "4                  Brian O'Donovan                NaN                  Celtic   \n",
       "...                            ...                ...                     ...   \n",
       "12790     Emily Olson, Emma Bowman                NaN           National News   \n",
       "12791                  Alexi Cohan                NaN  Science and Technology   \n",
       "12792  Dustin Jones, Kaitlyn Radde                NaN           National News   \n",
       "12793                   Ari Daniel                NaN                    News   \n",
       "12794                  Juliana Kim                NaN           National News   \n",
       "\n",
       "                                    Tagging Title  \\\n",
       "0      0000016a-3bcb-d661-af7b-7bff0d200001   NaN   \n",
       "1      0000016a-f0d5-dbfd-a56f-f4dfc34c0001   NaN   \n",
       "2      0000016b-539d-d757-adef-f7bd4a5f0001   NaN   \n",
       "3      0000016f-c48a-d14c-a57f-e59b02310001   NaN   \n",
       "4      00000170-273a-d4d2-a378-677e83f60001   NaN   \n",
       "...                                     ...   ...   \n",
       "12790  00000187-3753-da17-afc7-f77b1ea40001   NaN   \n",
       "12791  00000187-37f6-d65f-a1f7-bffee1410001   NaN   \n",
       "12792  00000187-37f7-da17-afc7-f7ffec2a0001   NaN   \n",
       "12793  00000187-382e-da17-afc7-f96fcf4c0001   NaN   \n",
       "12794  00000187-3865-da17-afc7-f96fb8210001   NaN   \n",
       "\n",
       "                                                   Paths  \\\n",
       "0      /music/celtic/2019/04/20/a-celtic-playlist-for...   \n",
       "1      /music/celtic/2019/05/25/songs-of-war-and-reme...   \n",
       "2      /music/celtic/2019/06/13/words-and-music-fathe...   \n",
       "3      /music/celtic/2020/01/21/celebrating-the-birth...   \n",
       "4                      /celtic/ValentinesDay (Permalink)   \n",
       "...                                                  ...   \n",
       "12790  /national-news/2023/03/31/5-key-takeaways-from...   \n",
       "12791  /science-and-technology/2023/03/31/harvard-pro...   \n",
       "12792  /national-news/2023/03/31/why-trump-isnt-the-f...   \n",
       "12793  /news/2023/03/31/these-cockroaches-tweaked-the...   \n",
       "12794  /national-news/2023/03/31/u-s-capitol-rioter-t...   \n",
       "\n",
       "                       Publish Date Has Path?  \\\n",
       "0      Mon Mar 29 15:22:35 EDT 2021      TRUE   \n",
       "1      Sat May 29 01:00:45 EDT 2021      TRUE   \n",
       "2      Fri Jun 18 01:00:28 EDT 2021      TRUE   \n",
       "3      Sat Jan 23 08:47:27 EST 2021      TRUE   \n",
       "4      Wed Feb 10 12:09:13 EST 2021      TRUE   \n",
       "...                             ...       ...   \n",
       "12790  Fri Mar 31 06:26:00 EDT 2023      TRUE   \n",
       "12791  Fri Mar 31 11:06:14 EDT 2023      TRUE   \n",
       "12792  Fri Mar 31 09:05:00 EDT 2023      TRUE   \n",
       "12793  Fri Mar 31 10:02:00 EDT 2023      TRUE   \n",
       "12794  Fri Mar 31 11:19:00 EDT 2023      TRUE   \n",
       "\n",
       "                                                    Body  \n",
       "0      The above is a continuous stream.  <br/><br/>I...  \n",
       "1      Click above for the audio of a special segment...  \n",
       "2      In honor of Father&#39;s Day, this segment of ...  \n",
       "3      Robert Burns is known as &quot;Scotland&#39;s ...  \n",
       "4      Folk music generally, and Celtic music, in par...  \n",
       "...                                                  ...  \n",
       "12790  Former president Donald Trump has been indicte...  \n",
       "12791  Artificial intelligence has advanced rapidly i...  \n",
       "12792  Former President Donald Trump was indicted Thu...  \n",
       "12793  Human attempts to kill cockroaches with sugary...  \n",
       "12794  Jacob Chansley, who received one of the longes...  \n",
       "\n",
       "[12795 rows x 12 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05de1ee9-585f-4e7e-b972-fa71a021d336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Type', 'Label', 'Headline', 'Byline', 'Section Navigation', 'Section',\n",
       "       'Tagging', 'Title', 'Paths', 'Publish Date', 'Has Path?', 'Body'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abbcf816-83f9-4526-a217-84612b8eb2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\n",
    "    'Has Path?',\n",
    "    'Title',\n",
    "    'Section Navigation',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aed65fff-7fa5-4f27-aa27-e43067a79dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Type',\n",
       " 'Label',\n",
       " 'Headline',\n",
       " 'Byline',\n",
       " 'Section',\n",
       " 'Tagging',\n",
       " 'Paths',\n",
       " 'Publish Date',\n",
       " 'Body']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58c32373-e012-4fad-a96e-e1b0d02f9184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Label</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Byline</th>\n",
       "      <th>Section</th>\n",
       "      <th>Tagging</th>\n",
       "      <th>Paths</th>\n",
       "      <th>Publish Date</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Article</td>\n",
       "      <td>A Celtic Playlist For Easter</td>\n",
       "      <td>A Celtic Playlist For Easter</td>\n",
       "      <td>Brian O'Donovan</td>\n",
       "      <td>Celtic</td>\n",
       "      <td>0000016a-3bcb-d661-af7b-7bff0d200001</td>\n",
       "      <td>/music/celtic/2019/04/20/a-celtic-playlist-for...</td>\n",
       "      <td>Mon Mar 29 15:22:35 EDT 2021</td>\n",
       "      <td>The above is a continuous stream.  &lt;br/&gt;&lt;br/&gt;I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Article</td>\n",
       "      <td>Songs Of War And Remembrance: Memorial Day</td>\n",
       "      <td>Songs Of War And Remembrance: Memorial Day</td>\n",
       "      <td>Brian O'Donovan</td>\n",
       "      <td>Celtic</td>\n",
       "      <td>0000016a-f0d5-dbfd-a56f-f4dfc34c0001</td>\n",
       "      <td>/music/celtic/2019/05/25/songs-of-war-and-reme...</td>\n",
       "      <td>Sat May 29 01:00:45 EDT 2021</td>\n",
       "      <td>Click above for the audio of a special segment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Article</td>\n",
       "      <td>Words And Music: Father's Day</td>\n",
       "      <td>Words And Music: Father's Day</td>\n",
       "      <td>Brian O'Donovan</td>\n",
       "      <td>Celtic</td>\n",
       "      <td>0000016b-539d-d757-adef-f7bd4a5f0001</td>\n",
       "      <td>/music/celtic/2019/06/13/words-and-music-fathe...</td>\n",
       "      <td>Fri Jun 18 01:00:28 EDT 2021</td>\n",
       "      <td>In honor of Father&amp;#39;s Day, this segment of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Article</td>\n",
       "      <td>Celebrating The Birthday Of Robert Burns — Jan...</td>\n",
       "      <td>Celebrating The Birthday Of Robert Burns — Jan...</td>\n",
       "      <td>Brian O'Donovan</td>\n",
       "      <td>Celtic</td>\n",
       "      <td>0000016f-c48a-d14c-a57f-e59b02310001</td>\n",
       "      <td>/music/celtic/2020/01/21/celebrating-the-birth...</td>\n",
       "      <td>Sat Jan 23 08:47:27 EST 2021</td>\n",
       "      <td>Robert Burns is known as &amp;quot;Scotland&amp;#39;s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Article</td>\n",
       "      <td>Ten Celtic Love Songs For St. Valentine's Day</td>\n",
       "      <td>Ten Celtic Love Songs For St. Valentine's Day</td>\n",
       "      <td>Brian O'Donovan</td>\n",
       "      <td>Celtic</td>\n",
       "      <td>00000170-273a-d4d2-a378-677e83f60001</td>\n",
       "      <td>/celtic/ValentinesDay (Permalink)</td>\n",
       "      <td>Wed Feb 10 12:09:13 EST 2021</td>\n",
       "      <td>Folk music generally, and Celtic music, in par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12790</th>\n",
       "      <td>Article</td>\n",
       "      <td>5 key takeaways from the Trump indictment news</td>\n",
       "      <td>5 key takeaways from the Trump indictment news</td>\n",
       "      <td>Emily Olson, Emma Bowman</td>\n",
       "      <td>National News</td>\n",
       "      <td>00000187-3753-da17-afc7-f77b1ea40001</td>\n",
       "      <td>/national-news/2023/03/31/5-key-takeaways-from...</td>\n",
       "      <td>Fri Mar 31 06:26:00 EDT 2023</td>\n",
       "      <td>Former president Donald Trump has been indicte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12791</th>\n",
       "      <td>Article</td>\n",
       "      <td>Harvard professor says government should pause...</td>\n",
       "      <td>Harvard professor says government should pause...</td>\n",
       "      <td>Alexi Cohan</td>\n",
       "      <td>Science and Technology</td>\n",
       "      <td>00000187-37f6-d65f-a1f7-bffee1410001</td>\n",
       "      <td>/science-and-technology/2023/03/31/harvard-pro...</td>\n",
       "      <td>Fri Mar 31 11:06:14 EDT 2023</td>\n",
       "      <td>Artificial intelligence has advanced rapidly i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12792</th>\n",
       "      <td>Article</td>\n",
       "      <td>Why Trump isn't the first president to face ar...</td>\n",
       "      <td>Why Trump isn't the first president to face ar...</td>\n",
       "      <td>Dustin Jones, Kaitlyn Radde</td>\n",
       "      <td>National News</td>\n",
       "      <td>00000187-37f7-da17-afc7-f7ffec2a0001</td>\n",
       "      <td>/national-news/2023/03/31/why-trump-isnt-the-f...</td>\n",
       "      <td>Fri Mar 31 09:05:00 EDT 2023</td>\n",
       "      <td>Former President Donald Trump was indicted Thu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12793</th>\n",
       "      <td>Article</td>\n",
       "      <td>These cockroaches tweaked their mating rituals...</td>\n",
       "      <td>These cockroaches tweaked their mating rituals...</td>\n",
       "      <td>Ari Daniel</td>\n",
       "      <td>News</td>\n",
       "      <td>00000187-382e-da17-afc7-f96fcf4c0001</td>\n",
       "      <td>/news/2023/03/31/these-cockroaches-tweaked-the...</td>\n",
       "      <td>Fri Mar 31 10:02:00 EDT 2023</td>\n",
       "      <td>Human attempts to kill cockroaches with sugary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12794</th>\n",
       "      <td>Article</td>\n",
       "      <td>U.S. Capitol rioter the 'QAnon Shaman' is rele...</td>\n",
       "      <td>U.S. Capitol rioter the 'QAnon Shaman' is rele...</td>\n",
       "      <td>Juliana Kim</td>\n",
       "      <td>National News</td>\n",
       "      <td>00000187-3865-da17-afc7-f96fb8210001</td>\n",
       "      <td>/national-news/2023/03/31/u-s-capitol-rioter-t...</td>\n",
       "      <td>Fri Mar 31 11:19:00 EDT 2023</td>\n",
       "      <td>Jacob Chansley, who received one of the longes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12795 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Type                                              Label  \\\n",
       "0      Article                      A Celtic Playlist For Easter    \n",
       "1      Article        Songs Of War And Remembrance: Memorial Day    \n",
       "2      Article                      Words And Music: Father's Day   \n",
       "3      Article  Celebrating The Birthday Of Robert Burns — Jan...   \n",
       "4      Article      Ten Celtic Love Songs For St. Valentine's Day   \n",
       "...        ...                                                ...   \n",
       "12790  Article     5 key takeaways from the Trump indictment news   \n",
       "12791  Article  Harvard professor says government should pause...   \n",
       "12792  Article  Why Trump isn't the first president to face ar...   \n",
       "12793  Article  These cockroaches tweaked their mating rituals...   \n",
       "12794  Article  U.S. Capitol rioter the 'QAnon Shaman' is rele...   \n",
       "\n",
       "                                                Headline  \\\n",
       "0                          A Celtic Playlist For Easter    \n",
       "1            Songs Of War And Remembrance: Memorial Day    \n",
       "2                          Words And Music: Father's Day   \n",
       "3      Celebrating The Birthday Of Robert Burns — Jan...   \n",
       "4          Ten Celtic Love Songs For St. Valentine's Day   \n",
       "...                                                  ...   \n",
       "12790     5 key takeaways from the Trump indictment news   \n",
       "12791  Harvard professor says government should pause...   \n",
       "12792  Why Trump isn't the first president to face ar...   \n",
       "12793  These cockroaches tweaked their mating rituals...   \n",
       "12794  U.S. Capitol rioter the 'QAnon Shaman' is rele...   \n",
       "\n",
       "                            Byline                 Section  \\\n",
       "0                  Brian O'Donovan                  Celtic   \n",
       "1                  Brian O'Donovan                  Celtic   \n",
       "2                  Brian O'Donovan                  Celtic   \n",
       "3                  Brian O'Donovan                  Celtic   \n",
       "4                  Brian O'Donovan                  Celtic   \n",
       "...                            ...                     ...   \n",
       "12790     Emily Olson, Emma Bowman           National News   \n",
       "12791                  Alexi Cohan  Science and Technology   \n",
       "12792  Dustin Jones, Kaitlyn Radde           National News   \n",
       "12793                   Ari Daniel                    News   \n",
       "12794                  Juliana Kim           National News   \n",
       "\n",
       "                                    Tagging  \\\n",
       "0      0000016a-3bcb-d661-af7b-7bff0d200001   \n",
       "1      0000016a-f0d5-dbfd-a56f-f4dfc34c0001   \n",
       "2      0000016b-539d-d757-adef-f7bd4a5f0001   \n",
       "3      0000016f-c48a-d14c-a57f-e59b02310001   \n",
       "4      00000170-273a-d4d2-a378-677e83f60001   \n",
       "...                                     ...   \n",
       "12790  00000187-3753-da17-afc7-f77b1ea40001   \n",
       "12791  00000187-37f6-d65f-a1f7-bffee1410001   \n",
       "12792  00000187-37f7-da17-afc7-f7ffec2a0001   \n",
       "12793  00000187-382e-da17-afc7-f96fcf4c0001   \n",
       "12794  00000187-3865-da17-afc7-f96fb8210001   \n",
       "\n",
       "                                                   Paths  \\\n",
       "0      /music/celtic/2019/04/20/a-celtic-playlist-for...   \n",
       "1      /music/celtic/2019/05/25/songs-of-war-and-reme...   \n",
       "2      /music/celtic/2019/06/13/words-and-music-fathe...   \n",
       "3      /music/celtic/2020/01/21/celebrating-the-birth...   \n",
       "4                      /celtic/ValentinesDay (Permalink)   \n",
       "...                                                  ...   \n",
       "12790  /national-news/2023/03/31/5-key-takeaways-from...   \n",
       "12791  /science-and-technology/2023/03/31/harvard-pro...   \n",
       "12792  /national-news/2023/03/31/why-trump-isnt-the-f...   \n",
       "12793  /news/2023/03/31/these-cockroaches-tweaked-the...   \n",
       "12794  /national-news/2023/03/31/u-s-capitol-rioter-t...   \n",
       "\n",
       "                       Publish Date  \\\n",
       "0      Mon Mar 29 15:22:35 EDT 2021   \n",
       "1      Sat May 29 01:00:45 EDT 2021   \n",
       "2      Fri Jun 18 01:00:28 EDT 2021   \n",
       "3      Sat Jan 23 08:47:27 EST 2021   \n",
       "4      Wed Feb 10 12:09:13 EST 2021   \n",
       "...                             ...   \n",
       "12790  Fri Mar 31 06:26:00 EDT 2023   \n",
       "12791  Fri Mar 31 11:06:14 EDT 2023   \n",
       "12792  Fri Mar 31 09:05:00 EDT 2023   \n",
       "12793  Fri Mar 31 10:02:00 EDT 2023   \n",
       "12794  Fri Mar 31 11:19:00 EDT 2023   \n",
       "\n",
       "                                                    Body  \n",
       "0      The above is a continuous stream.  <br/><br/>I...  \n",
       "1      Click above for the audio of a special segment...  \n",
       "2      In honor of Father&#39;s Day, this segment of ...  \n",
       "3      Robert Burns is known as &quot;Scotland&#39;s ...  \n",
       "4      Folk music generally, and Celtic music, in par...  \n",
       "...                                                  ...  \n",
       "12790  Former president Donald Trump has been indicte...  \n",
       "12791  Artificial intelligence has advanced rapidly i...  \n",
       "12792  Former President Donald Trump was indicted Thu...  \n",
       "12793  Human attempts to kill cockroaches with sugary...  \n",
       "12794  Jacob Chansley, who received one of the longes...  \n",
       "\n",
       "[12795 rows x 9 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "555c7c31-aac1-4827-ad9b-92618a2ce48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6710adf8-ec86-49ef-85a4-75d5a1060e29\n"
     ]
    }
   ],
   "source": [
    "# We predetermine the userID and upload ID\n",
    "# This should have already been configured per CSV upload\n",
    "# Generate a unique UUID\n",
    "unique_id = str(uuid.uuid4())\n",
    "userID = \"1\"\n",
    "\n",
    "print(unique_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166c056a-781a-4953-a836-52cfcfc44d0a",
   "metadata": {},
   "source": [
    "## Start of the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc2f27-cf90-4f8b-b373-1e2a3d6de4b2",
   "metadata": {},
   "source": [
    "First we create a user ID and upload the state..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1176e8f-3198-4b14-b22b-9b53e6189f4c",
   "metadata": {},
   "source": [
    "### Single Stream Cocurrent Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff75e8eb-53fa-42cc-96ed-74cca703f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(chunk, df, data_schema, data_packaging_scheme, nlp_ner):\n",
    "    \"\"\"\n",
    "    Processes a chunk of indices from the given dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----\n",
    "    df: The pandas dataframe that entity recognition is being done on.\n",
    "    chunk: The chunk of indices to proceses in the given df.\n",
    "\n",
    "    Returns\n",
    "    ---- \n",
    "    A list of dictionary items that constitute data for an article.\n",
    "    \"\"\"\n",
    "    ignore_article_types = [\"National News\", \"International News\", \"Programs\", \"Digital Mural\", \"Jazz\", \"Celtic\"]\n",
    "\n",
    "    discarded_articles = []\n",
    "    dataset_df = data_schema\n",
    "    neighborhoods = set()\n",
    "    census_tracts = set()\n",
    "    try: \n",
    "        # for idx in tqdm(chunk, desc='Processing Entity Recognition'):\n",
    "        for idx in chunk:\n",
    "            print(idx)\n",
    "            # Maybe nested 'try:' are cursed\n",
    "            try:\n",
    "                if df['Section'][idx] not in ignore_article_types and df['Type'][idx] == 'Article':\n",
    "                    headline = str(df['Label'][idx])\n",
    "                    text = str(df['Body'][idx])\n",
    "                    \n",
    "                    sentences = get_sentences(text)\n",
    "            \n",
    "                    # # get lede first 5 sentences, can change the number of sentences\n",
    "                    text_5 = get_snippet(sentences, 5)\n",
    "                    text_10 = get_snippet(sentences, 5, False) # get sentences 5-10\n",
    "                    text_remain = get_snippet(sentences, 5, False, True)\n",
    "            \n",
    "                    # get entities, returns existing entities that have been seen before and new entities as sets \n",
    "                    check_order = [\n",
    "                        (run_entity_recognition(headline, nlp_ner, drop_geos, saved_geocodes), \"headline\"), \n",
    "                        (run_entity_recognition(text_5, nlp_ner, drop_geos, saved_geocodes), \"first 5 sentences\"), \n",
    "                        (run_entity_recognition(text_10, nlp_ner, drop_geos, saved_geocodes), \"next 5 sentences\"),\n",
    "                        (run_entity_recognition(text_remain, nlp_ner, drop_geos, saved_geocodes), \"remaining text\")\n",
    "                    ]\n",
    "            \n",
    "                    for (entities, method) in check_order:\n",
    "                        check_text, location_geocode, existing_loc_geocode = check_snippets(secret.API_KEY, entities[1], entities[0])\n",
    "                        if not check_text:\n",
    "                            discarded_articles.append(df['Tagging'][idx])\n",
    "                            break \n",
    "        \n",
    "                    # No Census tracts we want is detected\n",
    "                    if (len(existing_loc_geocode) == 0 and len(location_geocode) == 0):\n",
    "                        discarded_articles.append(df['Tagging'][idx])\n",
    "                        continue\n",
    "                    \n",
    "                    pipeline_output = run_pipeline(year, dsource, dname, state, existing_loc_geocode, location_geocode, mappings)\n",
    "        \n",
    "                    if (pipeline_output):\n",
    "                        for output in pipeline_output:\n",
    "                            if ('neighborhood' in output[list(output.keys())[0]] and 'tract' in output[list(output.keys())[0]]):\n",
    "                                neighborhood = output[list(output.keys())[0]]['neighborhood']\n",
    "                                census_tract = output[list(output.keys())[0]]['tract']\n",
    "                                neighborhoods.add(neighborhood)\n",
    "                                census_tracts.add(census_tract)\n",
    "                            else:\n",
    "                                print(\"Skipped an entry!\")\n",
    "                                continue\n",
    "                    \n",
    "                    # If we have valid entity recognition | We have both some neighborhoods and census tracts\n",
    "                    if (len(neighborhoods) != 0 and len(census_tracts) != 0):\n",
    "                        data_packaging_scheme(\n",
    "                            dataset_df, # This is the data scheme we are using\n",
    "                            df['Tagging'][idx],\n",
    "                            list(neighborhoods),\n",
    "                            df['Section'][idx],\n",
    "                            list(census_tracts),\n",
    "                            df['Byline'][idx],\n",
    "                            df['Body'][idx],\n",
    "                            df['Tagging'][idx],\n",
    "                            df['Label'][idx],\n",
    "                            df['Headline'][idx],\n",
    "                            df['Publish Date'][idx],\n",
    "                            \"GBH\", # I hard coded this as we have one main client\n",
    "                            df['Paths'][idx],\n",
    "                            # No Open AI Labels yet\n",
    "                            method,\n",
    "                            existing_loc_geocode | location_geocode\n",
    "                        )\n",
    "                    neighborhoods.clear()\n",
    "                    census_tracts.clear()\n",
    "                else:\n",
    "                    discarded_articles.append(df['Tagging'][idx])\n",
    "            except Exception as e: # Loop inbounded error\n",
    "                print(f\"[Error] process_data() ran into an error! Continuing... \\n[Raw Error]: {e}\")\n",
    "        ## Convert to Pandas dataframe...\n",
    "        new_df = pd.DataFrame(dataset_df)\n",
    "        return new_df, discarded_articles\n",
    "    except Exception as e: \n",
    "        print(f\"[Fatal Error] process_data() ran into an Error! Data is not saved!\\nRaw Error:{e}\")\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce29345a-e3dd-4184-91c6-c089e3e2e4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 51\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mke\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_schema\n\u001b[0;32m---> 51\u001b[0m new_df, discarded_articles \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1700\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1800\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_schema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage_data_to_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp_ner\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 40\u001b[0m, in \u001b[0;36mprocess_data\u001b[0;34m(chunk, df, data_schema, data_packaging_scheme, nlp_ner)\u001b[0m\n\u001b[1;32m     35\u001b[0m text_remain \u001b[38;5;241m=\u001b[39m get_snippet(sentences, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# get entities, returns existing entities that have been seen before and new entities as sets \u001b[39;00m\n\u001b[1;32m     38\u001b[0m check_order \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     39\u001b[0m     (run_entity_recognition(headline, nlp_ner, drop_geos, saved_geocodes), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheadline\u001b[39m\u001b[38;5;124m\"\u001b[39m), \n\u001b[0;32m---> 40\u001b[0m     (\u001b[43mrun_entity_recognition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp_ner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_geos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaved_geocodes\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst 5 sentences\u001b[39m\u001b[38;5;124m\"\u001b[39m), \n\u001b[1;32m     41\u001b[0m     (run_entity_recognition(text_10, nlp_ner, drop_geos, saved_geocodes), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext 5 sentences\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     42\u001b[0m     (run_entity_recognition(text_remain, nlp_ner, drop_geos, saved_geocodes), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremaining text\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m ]\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (entities, method) \u001b[38;5;129;01min\u001b[39;00m check_order:\n\u001b[1;32m     46\u001b[0m     check_text, location_geocode, existing_loc_geocode \u001b[38;5;241m=\u001b[39m check_snippets(secret\u001b[38;5;241m.\u001b[39mAPI_KEY, entities[\u001b[38;5;241m1\u001b[39m], entities[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[13], line 31\u001b[0m, in \u001b[0;36mrun_entity_recognition\u001b[0;34m(text, nlp, drop_geos, saved_geocodes)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_entity_recognition\u001b[39m(text, nlp, drop_geos, saved_geocodes):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# running entity recogntion on text\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# parse existing geocoded entities and new geocoded entities\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m         extracted_loc, extracted_orgs \u001b[38;5;241m=\u001b[39m \u001b[43mget_locations_bert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m         ent_result \u001b[38;5;241m=\u001b[39m clean_entity_results(extracted_loc, extracted_orgs, drop_geos)\n\u001b[1;32m     33\u001b[0m         existing_loc_geocode, new_loc_geocode \u001b[38;5;241m=\u001b[39m remove_existing_geocodes(ent_result, saved_geocodes)\n",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m, in \u001b[0;36mget_locations_bert\u001b[0;34m(article_text, nlp)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_locations_bert\u001b[39m(article_text, nlp):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    get location names from article using NER - bert model \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    https://huggingface.co/dslim/bert-base-NER\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    input: article_text as a string, aggregate of h1, h2, lede, and body\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    returns: locations - set of tuples of (NAME, 'LOC') and organizations - set of tuples (NAME, 'ORG) mentioned in the article\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     ner_results \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     locations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([(X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m],X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity_group\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m ner_results \u001b[38;5;28;01mif\u001b[39;00m X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity_group\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLOC\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     11\u001b[0m     orgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([(X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m], X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity_group\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m ner_results \u001b[38;5;28;01mif\u001b[39;00m X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity_group\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mORG\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:249\u001b[0m, in \u001b[0;36mTokenClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset_mapping:\n\u001b[1;32m    247\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m offset_mapping\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/transformers/pipelines/base.py:1132\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[0;32m-> 1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:266\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 266\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:286\u001b[0m, in \u001b[0;36mTokenClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m     logits \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m: logits,\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecial_tokens_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: special_tokens_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[1;32m    296\u001b[0m }\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1758\u001b[0m, in \u001b[0;36mBertForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1758\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1770\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1772\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1015\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1016\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1017\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[0;32m-> 1022\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1035\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    603\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    605\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 612\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    536\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    537\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 539\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/transformers/pytorch_utils.py:240\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    551\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 552\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:464\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 464\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    466\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python_se/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "# To run the pipeline, two things we need to have defined is the data_schmea and data packing func\n",
    "data_schema = {\n",
    "        \"id\": [],\n",
    "        \"neighborhoods\": [],\n",
    "        \"position_section\": [],\n",
    "        \"tracts\": [],\n",
    "        \"author\": [],\n",
    "        \"body\": [],\n",
    "        \"content_id\": [],\n",
    "        \"hl1\": [],\n",
    "        \"hl2\": [],\n",
    "        \"pub_date\": [],\n",
    "        \"pub_name\": [],\n",
    "        \"link\": [],\n",
    "        \"method\": [],\n",
    "        \"ent_geocodes\": []\n",
    "    }\n",
    "\n",
    "# Data packing scheme, function attributes must be len(data_schema) + 1\n",
    "# Ideally, the developer should provide a function to figure out how to pack the data based on their needs\n",
    "def package_data_to_dict(\n",
    "    data_schema, \n",
    "    id,\n",
    "    neighborhoods,\n",
    "    position_section,\n",
    "    tracts,\n",
    "    author,\n",
    "    body,\n",
    "    content_id,\n",
    "    hl1,\n",
    "    hl2,\n",
    "    pub_date,\n",
    "    pub_name,\n",
    "    link,\n",
    "    method,\n",
    "    ent_geocodes\n",
    "):\n",
    "    try:\n",
    "        args, _, _, values = inspect.getargvalues(inspect.currentframe())\n",
    "        args.pop(0) # we pop off data_schema\n",
    "        for arg in args:\n",
    "            data_schema[arg].append(values[arg])\n",
    "    except KeyError as ke:\n",
    "        print(\"Key not found in data schema!\")\n",
    "        print(f\"Raw Error: {ke}\")\n",
    "        \n",
    "    return data_schema\n",
    "\n",
    "new_df, discarded_articles = process_data(list(range(1700, 1800)), df, data_schema, package_data_to_dict, nlp_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b076bd58-c9d9-4a85-8853-13a09cad6bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "discarded_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4581ac7-11d5-4d83-9566-2d6ab0b2812d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f57bb-f693-4d40-aa6f-a11a82ac8793",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777241a5-5697-4999-9177-2d52582de279",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_articles = new_df\n",
    "unseen_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c47de7-7d29-45e5-bb49-795b9d4b9990",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_labels = heir_data\n",
    "unseen_articles = unseen_articles.dropna(subset=['content_id'])\n",
    "\n",
    "topics, probs = nlp_topic.transform(unseen_articles['body']) # get bertopics for each article\n",
    "unseen_articles['bertopic_topic_label'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efcd923-c4aa-46d7-acf2-e245e76367cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add open ai label to bglobe dataframe in new column\n",
    "unseen_label_name = [openai_labels[unseen_articles['bertopic_topic_label'][i]]['openai'] \n",
    "              if int(unseen_articles['bertopic_topic_label'][i]) != -1 else \"\" for i in range(len(unseen_articles))]\n",
    "unseen_articles['openai_label'] = unseen_label_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad09fc-6db1-40bb-8c2d-b51fa78b2c69",
   "metadata": {},
   "source": [
    "This will be the raw data that is saved in the ML data base for data analysis by the data science team. Naturally, we will strip the data that will allow the back-end to consume it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac52d9ab-2761-43d2-8b2d-be61fa44dfc7",
   "metadata": {},
   "source": [
    "### Some User ID & Upload ID Stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d0bc84-279f-4a45-be97-2f0fc91f2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_articles[\"userID\"] = userID\n",
    "unseen_articles[\"uploadID\"] = unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e821cdfc-1be3-4c75-b9e8-f8168ce2c09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_articles.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6d572-9631-4f75-8dca-58b0aad3f1c4",
   "metadata": {},
   "source": [
    "### Format for GraphQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3cb5c8-3d3e-44f9-94d8-05116dd676f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "packaged_data_df = unseen_articles.drop(columns=[\n",
    "    'method',\n",
    "    'ent_geocodes',\n",
    "    'bertopic_topic_label'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eecda7f-0aa1-4fbc-b1ba-3625df51eff8",
   "metadata": {},
   "source": [
    "### Force a reconnection to Production Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a267cd2-c6a6-454d-b346-5fbcfe806604",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_prod = connect_MongoDB_Prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b37381-aa74-4e9f-8a24-bef762ed390c",
   "metadata": {},
   "source": [
    "## Conversion to Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3421a63a-6c58-4133-89a3-c96cdafb0b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_collection_name = \"articles_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51de2f97-e226-49f3-91e5-bc712a6860a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "packaged_data_df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478c0fe-0a20-4f35-a056-286726f2dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_dict = packaged_data_df.T.to_dict('dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be2077-768c-4f43-b6b4-64e8597f544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_list(s):\n",
    "    if(s != ''):\n",
    "        return [s]\n",
    "    else:\n",
    "        return []  # Return the string as a single-element list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf1cd8-7513-4376-9d0b-2eda1e426d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_payload = []\n",
    "\n",
    "for article_key in article_dict.keys():\n",
    "    article = article_dict[article_key]\n",
    "    if ('openai_label' not in article):\n",
    "        article[\"openai_label\"] = []\n",
    "    else:\n",
    "        article[\"openai_label\"] = string_to_list(article[\"openai_label\"])\n",
    "    article_payload.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9673e06e-ca66-4b17-8faa-22651f78f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(article_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a2ec33-1151-4f4e-91a5-4e0985628759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existence of collection\n",
    "collection_list = db_prod.list_collection_names()\n",
    "\n",
    "if articles_collection_name not in collection_list:\n",
    "    db_prod.create_collection(articles_collection_name)\n",
    "    print(f\"Collection '{articles_collection_name}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29093541-8254-472e-8946-88cac8bb54b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "insertion_result = db_prod[articles_collection_name].insert_many(article_payload)\n",
    "print(\"Articles Successfully inserted!\")\n",
    "# print(\"Documents have been successfully inserted with the following IDs:\")\n",
    "# print(insertion_result.inserted_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac0c645-3e17-4564-afa9-7c3d0f2dcfd4",
   "metadata": {},
   "source": [
    "## Conversion to Neighborhood "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777baac1-5c36-47d1-b7a9-272483f4e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_collection_name = \"neighborhood_data\"\n",
    "neigh_collection = db_prod[neigh_collection_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af889b7-1385-4ce9-8947-ef83e0838c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existence of collection\n",
    "collection_list = db_prod.list_collection_names()\n",
    "\n",
    "if neigh_collection_name not in collection_list:\n",
    "    db_prod.create_collection(neigh_collection_name)\n",
    "    print(f\"Collection '{neigh_collection_name}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906b2b9b-a187-406e-806a-f6532a958874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that the neighborhood list we get is well ordered\n",
    "# We also assume that the lengths between each list is the same\n",
    "neighborhood_list = packaged_data_df['neighborhoods'].to_numpy()\n",
    "tagging_list = packaged_data_df['content_id'].to_numpy()\n",
    "\n",
    "for n_idx in range(len(neighborhood_list)):\n",
    "    neigh_list = neighborhood_list[n_idx]\n",
    "    \n",
    "    for neigh in neigh_list:\n",
    "        # Here we update the tags/articles by neighborhood\n",
    "        neigh_collection.find_one_and_update(\n",
    "            {'value': neigh},\n",
    "            {'$addToSet': {'articles': tagging_list[n_idx]}},\n",
    "            upsert = True # Creates a new document of it if it doesn't exist\n",
    "        )       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc4cef2-0a76-4710-9b96-50d0dc2ab39e",
   "metadata": {},
   "source": [
    "## Conversion to Topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea51be-7451-471b-a565-2221ce1d4b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_collection_name = \"topics_data\"\n",
    "topic_collection = db_prod[topics_collection_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e13972-9234-47ec-b87c-b287582f5050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existence of collection\n",
    "collection_list = db_prod.list_collection_names()\n",
    "\n",
    "if topics_collection_name not in collection_list:\n",
    "    db_prod.create_collection(topics_collection_name)\n",
    "    print(f\"Collection '{topics_collection_name}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93439dfd-112f-465a-9db5-b9a12606899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that the topics list we get is well ordered\n",
    "# We also assume that the lengths between each list is the same\n",
    "topics_list = packaged_data_df['position_section'].to_numpy()\n",
    "tagging_list = packaged_data_df['content_id'].to_numpy()\n",
    "\n",
    "for n_idx in range(len(topics_list)):\n",
    "    topic = topics_list[n_idx]\n",
    "    \n",
    "    # Here we update the tags/articles by Topics\n",
    "    topic_collection.find_one_and_update(\n",
    "        {'value': topic},\n",
    "        {'$addToSet': {'articles': tagging_list[n_idx]}},\n",
    "        upsert = True # Creates a new document of it if it doesn't exist\n",
    "    )       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6c71b8-c7dc-451c-b3fc-a39a7a6b3941",
   "metadata": {},
   "source": [
    "## Conversion to Tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d23199c-2a48-4144-8abd-0efb0fc10a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addExistingTracts(tract_collection):\n",
    "    tracts_list = []\n",
    "\n",
    "    for tract_key in census_base.old_census_tracts['tracts_filter'].keys():\n",
    "        tracts_list.append(census_base.old_census_tracts['tracts_filter'][tract_key])\n",
    "\n",
    "    tract_collection.insert_many(tracts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ac8b4e-ce41-47f5-a3f3-725ba41fe574",
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_collection_name = \"tracts_data\"\n",
    "tract_collection = db_prod[tract_collection_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79c5bc3-70a6-44bd-a8e2-21bda1b82388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existence of collection\n",
    "collection_list = db_prod.list_collection_names()\n",
    "\n",
    "if tract_collection_name not in collection_list:\n",
    "    db_prod.create_collection(tract_collection_name)\n",
    "    print(f\"Collection '{tract_collection_name}' created.\")\n",
    "    # We add existing tracts from the old census tracts\n",
    "    addExistingTracts(tract_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad365fc-b070-4330-a1fe-fb18136825a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that the topics list we get is well ordered\n",
    "# We also assume that the lengths between each list is the same\n",
    "tracts_lists = packaged_data_df['tracts'].to_numpy()\n",
    "tagging_list = packaged_data_df['content_id'].to_numpy()\n",
    "\n",
    "for n_idx in range(len(tracts_lists)):\n",
    "    tract_list = tracts_lists[n_idx]\n",
    "\n",
    "    for tract in tract_list:\n",
    "        # Here we update the tags/articles by tracts\n",
    "        if tract_collection.find_one({'tract': tract}):\n",
    "            tract_collection.find_one_and_update(\n",
    "                {'tract': tract},\n",
    "                {'$addToSet': {'articles': tagging_list[n_idx]}},\n",
    "            )\n",
    "        else: # We didn't find one and we have to label it as unknown\n",
    "            unknown_tract = {\n",
    "                'tract': tract,\n",
    "                'neighborhood': \"unknown\",\n",
    "                'articles': []\n",
    "            }\n",
    "            tract_collection.insert_one(unknown_tract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48cc442-3ebe-49d3-bed4-867b28e7912a",
   "metadata": {},
   "source": [
    "### Add Discarded Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66183ed-b50a-40df-bed7-c97e2cf85c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "discarded_collection_name = \"discarded\"\n",
    "discarded_collection = db_prod[discarded_collection_name]\n",
    "\n",
    "for discarded_article in discarded_articles:\n",
    "    if (discarded_collection.find_one({'uploadID': unique_id})):\n",
    "        discarded_collection.find_one_and_update(\n",
    "            {'uploadID': unique_id},\n",
    "            {'$addToSet': {'content_ids': discarded_article}},\n",
    "        )\n",
    "    else: # We didn't find one and we have to label it as unknown\n",
    "        new_discard = {\n",
    "            'uploadID': unique_id,\n",
    "            'content_ids': []\n",
    "        }\n",
    "        discarded_collection.insert_one(new_discard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f3667-3f9b-42f6-a560-6bf9ee9fe16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_prod = connect_MongoDB_Prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b6b35e-adb7-4a83-ac24-70181c5f1bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_collection_name = \"articles_data\"\n",
    "articles_collection = db_prod[neigh_collection_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee264b6-aa13-4c45-abb8-cbdc60f0f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_duplicate_article(tag, articles_collection):\n",
    "\treturn articles_collection.find_one({\"content_id\": tag}) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeab0eed-f219-424e-be29-a887596a0cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_duplicate_article(\"00000175-e21a-dd9a-a37d-eadf3d080001\", articles_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dfc6e0-6f4c-4543-9980-1dbc08030953",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_collection_name = \"discarded\"\n",
    "discarded_collection = db_prod[neigh_collection_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2160833-03ad-4555-8e78-fed9d6f22611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_duplicate_discarded(tag, discarded_collection):\n",
    "    return discarded_collection.count_documents({'content_ids': {'$in': [tag]}}) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b0cd7-207a-496d-adfc-ef6260eca767",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_duplicate_discarded(\"00000175-e21a-dd9a-a37d-eadf3d080001\", discarded_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40797c56-53e5-458c-a4bb-d2a71b0e84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21bdef48-cc25-464a-9817-2eaa1f7ad755",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"./Articles Nov 2020 - March 2023.csv\", low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e470a7da-4c3e-4da4-a309-b266e7727071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12795"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccdc0f72-e120-4a25-856a-9966ad0b3c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[3000:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d900ad70-ecee-450b-b9a4-697b6e30968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_pd = pd.concat(\n",
    "    [\n",
    "        test['Type'],\n",
    "        test['Label'],\n",
    "        test['Headline'],\n",
    "        test['Byline'],\n",
    "        test['Section'],\n",
    "        test['Tagging'],\n",
    "        test['Paths'],\n",
    "        test['Publish Date'],\n",
    "        test['Body'],\n",
    "    ],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04f5f1ef-4a57-49f8-a7b4-1d986bd920c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Label</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Byline</th>\n",
       "      <th>Section</th>\n",
       "      <th>Tagging</th>\n",
       "      <th>Paths</th>\n",
       "      <th>Publish Date</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>Article</td>\n",
       "      <td>Save The Bees: Massachusetts Issues New Rules ...</td>\n",
       "      <td>Save The Bees: Massachusetts Issues New Rules ...</td>\n",
       "      <td>Hannah Uebele</td>\n",
       "      <td>Local News</td>\n",
       "      <td>00000178-65d2-dbea-a578-ffda90a40001</td>\n",
       "      <td>/local-news/2021/03/25/save-the-bees-massachus...</td>\n",
       "      <td>Thu Mar 25 06:00:08 EDT 2021</td>\n",
       "      <td>Spring has officially sprung in Massachusetts,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3001</th>\n",
       "      <td>Article</td>\n",
       "      <td>Greater Boston Full Show: 03/24/21</td>\n",
       "      <td>Greater Boston Full Show: 03/24/21</td>\n",
       "      <td>Greater Boston Staff</td>\n",
       "      <td>National News</td>\n",
       "      <td>00000178-6630-d89e-a7ff-ef3aaf8d0001</td>\n",
       "      <td>/national-news/2021/03/31/greater-boston-full-...</td>\n",
       "      <td>Wed Mar 31 21:23:34 EDT 2021</td>\n",
       "      <td>&lt;b&gt;&lt;a href=\"https://www.wgbh.org/news/local-ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3002</th>\n",
       "      <td>Article</td>\n",
       "      <td>Mass. Ed. Commissioner Jeffrey Riley On Lettin...</td>\n",
       "      <td>Mass. Ed. Commissioner Jeffrey Riley On Lettin...</td>\n",
       "      <td>Greater Boston Staff</td>\n",
       "      <td>Local News</td>\n",
       "      <td>00000178-6633-dbea-a578-fffbc6c40001</td>\n",
       "      <td>/local-news/2021/03/24/mass-ed-commissioner-je...</td>\n",
       "      <td>Wed Mar 24 21:58:41 EDT 2021</td>\n",
       "      <td>Massachusetts education officials announced We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3003</th>\n",
       "      <td>Article</td>\n",
       "      <td>Michelle Singletary On How To Maximize Your St...</td>\n",
       "      <td>Michelle Singletary On How To Maximize Your St...</td>\n",
       "      <td>Greater Boston Staff</td>\n",
       "      <td>National News</td>\n",
       "      <td>00000178-6633-dbea-a578-fffbd5650001</td>\n",
       "      <td>/national-news/2021/03/24/michelle-singletary-...</td>\n",
       "      <td>Wed Mar 24 21:59:42 EDT 2021</td>\n",
       "      <td>Washington Post personal finance columnist Mic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3004</th>\n",
       "      <td>Article</td>\n",
       "      <td>Colorado Democrats Consider Ban On Assault Wea...</td>\n",
       "      <td>Colorado Democrats Consider Ban On Assault Wea...</td>\n",
       "      <td>Bente Birkeland</td>\n",
       "      <td>National News</td>\n",
       "      <td>00000178-6640-dbea-a578-ffc891d10001</td>\n",
       "      <td>/national-news/2021/03/25/colorado-democrats-c...</td>\n",
       "      <td>Wed Mar 24 17:41:00 EDT 2021</td>\n",
       "      <td>Colorado could be the next state to consider a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>Article</td>\n",
       "      <td>United Holds Drawings For Free Flights For Tho...</td>\n",
       "      <td>United Holds Drawings For Free Flights For Tho...</td>\n",
       "      <td>Joe Hernandez</td>\n",
       "      <td>National News</td>\n",
       "      <td>00000179-a478-d1dc-a97f-bd7b3a7c0001</td>\n",
       "      <td>/national-news/2021/05/25/united-holds-drawing...</td>\n",
       "      <td>Tue May 25 12:23:00 EDT 2021</td>\n",
       "      <td>Krispy Kreme is giving away a complimentary gl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>Article</td>\n",
       "      <td>Baker Seeks To Preserve Outdoor Dining, Remote...</td>\n",
       "      <td>Baker Seeks To Preserve Outdoor Dining, Remote...</td>\n",
       "      <td>Chris Lisinski | State House News</td>\n",
       "      <td>Local News</td>\n",
       "      <td>00000179-a48c-dcfc-a1fd-b6dc8d1d0001</td>\n",
       "      <td>/local-news/2021/05/25/baker-seeks-to-preserve...</td>\n",
       "      <td>Tue May 25 13:29:29 EDT 2021</td>\n",
       "      <td>[Story Developing] Gov. Charlie Baker on Tuesd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>Article</td>\n",
       "      <td>Mass. Changes Rules On Police In Schools As Bo...</td>\n",
       "      <td>Mass. Changes Rules On Police In Schools As Bo...</td>\n",
       "      <td>Meg Woolhouse</td>\n",
       "      <td>Education</td>\n",
       "      <td>00000179-a4a3-dcfc-a1fd-b6f38db00001</td>\n",
       "      <td>/education/2021/05/26/mass-changes-rules-on-po...</td>\n",
       "      <td>Wed May 26 17:14:06 EDT 2021</td>\n",
       "      <td>Boston sits at a crossroads when it comes to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>Article</td>\n",
       "      <td>Boston Public Radio Full Show: 5/25/21</td>\n",
       "      <td>Boston Public Radio Full Show: 5/25/21</td>\n",
       "      <td>Mackenzie Farkus</td>\n",
       "      <td>National News</td>\n",
       "      <td>00000179-a4c4-dcfc-a1fd-b6d407970001</td>\n",
       "      <td>/national-news/2021/05/25/boston-public-radio-...</td>\n",
       "      <td>Tue May 25 22:49:51 EDT 2021</td>\n",
       "      <td>Today on &lt;i&gt;Boston Public Radio&lt;/i&gt;:&lt;br/&gt;&lt;br/&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>Article</td>\n",
       "      <td>Half Of All U.S. Adults Are Now Fully Vaccinat...</td>\n",
       "      <td>Half Of All U.S. Adults Are Now Fully Vaccinat...</td>\n",
       "      <td>Bill Chappell</td>\n",
       "      <td>National News</td>\n",
       "      <td>00000179-a4e6-dcfc-a1fd-b6f60a780001</td>\n",
       "      <td>/national-news/2021/05/25/half-of-all-u-s-adul...</td>\n",
       "      <td>Tue May 25 14:53:00 EDT 2021</td>\n",
       "      <td>&lt;b&gt;Updated May 25, 2021 at 2:42 PM ET&lt;/b&gt;&lt;br&gt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Type                                              Label  \\\n",
       "3000  Article  Save The Bees: Massachusetts Issues New Rules ...   \n",
       "3001  Article                 Greater Boston Full Show: 03/24/21   \n",
       "3002  Article  Mass. Ed. Commissioner Jeffrey Riley On Lettin...   \n",
       "3003  Article  Michelle Singletary On How To Maximize Your St...   \n",
       "3004  Article  Colorado Democrats Consider Ban On Assault Wea...   \n",
       "...       ...                                                ...   \n",
       "3995  Article  United Holds Drawings For Free Flights For Tho...   \n",
       "3996  Article  Baker Seeks To Preserve Outdoor Dining, Remote...   \n",
       "3997  Article  Mass. Changes Rules On Police In Schools As Bo...   \n",
       "3998  Article             Boston Public Radio Full Show: 5/25/21   \n",
       "3999  Article  Half Of All U.S. Adults Are Now Fully Vaccinat...   \n",
       "\n",
       "                                               Headline  \\\n",
       "3000  Save The Bees: Massachusetts Issues New Rules ...   \n",
       "3001                 Greater Boston Full Show: 03/24/21   \n",
       "3002  Mass. Ed. Commissioner Jeffrey Riley On Lettin...   \n",
       "3003  Michelle Singletary On How To Maximize Your St...   \n",
       "3004  Colorado Democrats Consider Ban On Assault Wea...   \n",
       "...                                                 ...   \n",
       "3995  United Holds Drawings For Free Flights For Tho...   \n",
       "3996  Baker Seeks To Preserve Outdoor Dining, Remote...   \n",
       "3997  Mass. Changes Rules On Police In Schools As Bo...   \n",
       "3998             Boston Public Radio Full Show: 5/25/21   \n",
       "3999  Half Of All U.S. Adults Are Now Fully Vaccinat...   \n",
       "\n",
       "                                  Byline        Section  \\\n",
       "3000                       Hannah Uebele     Local News   \n",
       "3001                Greater Boston Staff  National News   \n",
       "3002                Greater Boston Staff     Local News   \n",
       "3003                Greater Boston Staff  National News   \n",
       "3004                     Bente Birkeland  National News   \n",
       "...                                  ...            ...   \n",
       "3995                       Joe Hernandez  National News   \n",
       "3996  Chris Lisinski | State House News      Local News   \n",
       "3997                       Meg Woolhouse      Education   \n",
       "3998                   Mackenzie Farkus   National News   \n",
       "3999                       Bill Chappell  National News   \n",
       "\n",
       "                                   Tagging  \\\n",
       "3000  00000178-65d2-dbea-a578-ffda90a40001   \n",
       "3001  00000178-6630-d89e-a7ff-ef3aaf8d0001   \n",
       "3002  00000178-6633-dbea-a578-fffbc6c40001   \n",
       "3003  00000178-6633-dbea-a578-fffbd5650001   \n",
       "3004  00000178-6640-dbea-a578-ffc891d10001   \n",
       "...                                    ...   \n",
       "3995  00000179-a478-d1dc-a97f-bd7b3a7c0001   \n",
       "3996  00000179-a48c-dcfc-a1fd-b6dc8d1d0001   \n",
       "3997  00000179-a4a3-dcfc-a1fd-b6f38db00001   \n",
       "3998  00000179-a4c4-dcfc-a1fd-b6d407970001   \n",
       "3999  00000179-a4e6-dcfc-a1fd-b6f60a780001   \n",
       "\n",
       "                                                  Paths  \\\n",
       "3000  /local-news/2021/03/25/save-the-bees-massachus...   \n",
       "3001  /national-news/2021/03/31/greater-boston-full-...   \n",
       "3002  /local-news/2021/03/24/mass-ed-commissioner-je...   \n",
       "3003  /national-news/2021/03/24/michelle-singletary-...   \n",
       "3004  /national-news/2021/03/25/colorado-democrats-c...   \n",
       "...                                                 ...   \n",
       "3995  /national-news/2021/05/25/united-holds-drawing...   \n",
       "3996  /local-news/2021/05/25/baker-seeks-to-preserve...   \n",
       "3997  /education/2021/05/26/mass-changes-rules-on-po...   \n",
       "3998  /national-news/2021/05/25/boston-public-radio-...   \n",
       "3999  /national-news/2021/05/25/half-of-all-u-s-adul...   \n",
       "\n",
       "                      Publish Date  \\\n",
       "3000  Thu Mar 25 06:00:08 EDT 2021   \n",
       "3001  Wed Mar 31 21:23:34 EDT 2021   \n",
       "3002  Wed Mar 24 21:58:41 EDT 2021   \n",
       "3003  Wed Mar 24 21:59:42 EDT 2021   \n",
       "3004  Wed Mar 24 17:41:00 EDT 2021   \n",
       "...                            ...   \n",
       "3995  Tue May 25 12:23:00 EDT 2021   \n",
       "3996  Tue May 25 13:29:29 EDT 2021   \n",
       "3997  Wed May 26 17:14:06 EDT 2021   \n",
       "3998  Tue May 25 22:49:51 EDT 2021   \n",
       "3999  Tue May 25 14:53:00 EDT 2021   \n",
       "\n",
       "                                                   Body  \n",
       "3000  Spring has officially sprung in Massachusetts,...  \n",
       "3001  <b><a href=\"https://www.wgbh.org/news/local-ne...  \n",
       "3002  Massachusetts education officials announced We...  \n",
       "3003  Washington Post personal finance columnist Mic...  \n",
       "3004  Colorado could be the next state to consider a...  \n",
       "...                                                 ...  \n",
       "3995  Krispy Kreme is giving away a complimentary gl...  \n",
       "3996  [Story Developing] Gov. Charlie Baker on Tuesd...  \n",
       "3997  Boston sits at a crossroads when it comes to t...  \n",
       "3998  Today on <i>Boston Public Radio</i>:<br/><br/>...  \n",
       "3999   <b>Updated May 25, 2021 at 2:42 PM ET</b><br>...  \n",
       "\n",
       "[1000 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22aede82-24f8-4532-ae1e-e85008e6ac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_pd.to_csv(\"./test_for_gcloud_presentation_1000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b884ce-8e2f-42ce-9631-064bf9debff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b30656-4668-4b8e-9195-97e4b66d933f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b593a5-e776-4261-be50-e28252726752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4387a122-d139-445b-bdb6-686deaeccbf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
