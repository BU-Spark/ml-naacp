{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66cdea73-38a8-4b7a-bfc1-c47bd88aca6e",
   "metadata": {},
   "source": [
    "# Here we run the entity recognition pipeline end-to-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eef5f91d-690d-4005-8e16-48d88f26989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json \n",
    "import nltk\n",
    "import spacy\n",
    "import requests\n",
    "import googlemaps \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import secret # API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca9e189-ed16-4a35-bbbb-411f0abeb62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zacharyg/miniconda3/envs/python_se/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f8e99-fde9-4a1c-9d66-203f49b99754",
   "metadata": {},
   "source": [
    "### Neighbourhood Mapping Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef77bb00-c9ff-4cd6-8fb1-d0d7b02d9009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neighborhood_mapping():\n",
    "    def __init__(self):\n",
    "        self.load_mappings()\n",
    "    \n",
    "    def load_mappings(self):\n",
    "        # load census tract to boston neighborhood mapping \n",
    "        # load census block to boston neighborhood mapping \n",
    "        self.tract_mapping = json.load(open(\"./geo-data/tracts-neighbors.json\"))\n",
    "        self.block_mapping = json.load(open(\"./geo-data/blocks-neighbors.json\"))\n",
    "\n",
    "    def tract_to_neighborhood(self, tract):\n",
    "        # given a census tract return the boston neighborhood it is in \n",
    "        return self.tract_mapping[tract]\n",
    "\n",
    "    def block_to_neighborhood(self, block):\n",
    "        # given a census block return the boston neighborhood it is in \n",
    "        return self.block_mapping(block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75025940-c34b-4a2b-85a1-28bdda898938",
   "metadata": {},
   "source": [
    "### Geography Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3afa56b4-13f8-4b95-95d7-7198f5041830",
   "metadata": {},
   "outputs": [],
   "source": [
    "class geography():\n",
    "    def __init__(self):\n",
    "        self.load_geographies()\n",
    "        self.load_org_entities()\n",
    "        self.saved_geocodes = json.load(open(\"./saved-geocodes.json\"))\n",
    "    \n",
    "    def load_geographies(self):\n",
    "        # static data, states, towns, orgs in entity output format to filter out of geocoding results \n",
    "        self.load_state_entities()\n",
    "        self.load_mass_town_entities()\n",
    "    \n",
    "    def load_state_entities(self):\n",
    "        states = pd.read_csv(\"./geo-data/states.csv\")\n",
    "        states_set = set()\n",
    "        for idx in range(len(states)):\n",
    "            tup = (states['state'][idx], 'LOC')\n",
    "            states_set.add(tup)\n",
    "        self.state_entities = states_set\n",
    "    \n",
    "    def load_mass_town_entities(self):\n",
    "        towns = pd.read_csv(\"./geo-data/mass-towns.csv\")\n",
    "        towns_set = set()\n",
    "        for idx in range(len(towns)):\n",
    "            tup = (towns['town'][idx], 'LOC')\n",
    "            towns_set.add(tup)\n",
    "        self.mass_town_entities = towns_set\n",
    "    \n",
    "    def load_org_entities(self):\n",
    "        self.org_entities = (('GBH News', 'ORG'), ('Boston Public Radio', 'ORG'), \n",
    "                             ('Supreme Court', 'ORG'), ('New York Times', 'ORG'), \n",
    "                             ('Washington Post', 'ORG'), ('CNN', 'ORG'), \n",
    "                             ('NPR', 'ORG'), ('Associated', 'ORG'), \n",
    "                             ('Press', 'ORG'), ('Senate', 'ORG'), \n",
    "                             ('Associated Press', 'ORG'), ('AP', 'ORG'), \n",
    "                             ('ABC News', 'ORG'),('CSS', 'ORG'), \n",
    "                             ('Philadelphia Inquirer', 'ORG'), ('House', 'ORG'),\n",
    "                             ('Congress', 'ORG'), ('Worcester', 'ORG'),\n",
    "                             ('FBI', 'ORG'), ('Homeland Security Department', 'ORG'),\n",
    "                             ('CDC', 'ORG'),('Fox News', 'ORG'),('The Washington Post', 'ORG'),\n",
    "                             ('States', 'LOC'), ('S.', 'LOC'), ('Massachusetts', 'ORG'),\n",
    "                             ('White House', 'ORG'), ('High School', 'ORG'),\n",
    "                             ('MIT', 'ORG'), ('Harvard University', 'ORG'),\n",
    "                             ('White House', 'LOC'),('Greater Boston', 'LOC'),\n",
    "                             ('New England', 'LOC'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc08f9d6-a377-42d9-93ed-ee8447e1eaf4",
   "metadata": {},
   "source": [
    "### Some Additional Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b3eed21-e4e4-4ae2-9563-2efd617df6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_locations(article_text):\n",
    "    \"\"\"\n",
    "    get location names from article using NER - spacy \n",
    "    input: article_text as a string, aggregate of h1, h2, lede, and body\n",
    "    returns: locations - set of tuples of (NAME, 'GPE')\n",
    "    \"\"\"\n",
    "    # get locations using NER  \n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    doc = nlp(article_text)\n",
    "    print(doc.ents)\n",
    "    # get the locations only, remove duplicates from results \n",
    "    locations = set([(X.text, X.label_) for X in doc.ents if X.label_ == 'GPE' or X.label_ == 'FAC' or X.label_ == 'ORG']) # or X.label_ == 'LOC' or X.label_ == 'FAC' or X.label_ == 'ORG'\n",
    "\n",
    "    return locations\n",
    "    \n",
    "def load_bert():\n",
    "    # loading bert model \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
    "    nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"max\")\n",
    "\n",
    "    return nlp\n",
    "\n",
    "def get_locations_bert(article_text, nlp):\n",
    "    \"\"\"\n",
    "    get location names from article using NER - bert model \n",
    "    https://huggingface.co/dslim/bert-base-NER\n",
    "    input: article_text as a string, aggregate of h1, h2, lede, and body\n",
    "    returns: locations - set of tuples of (NAME, 'LOC') and organizations - set of tuples (NAME, 'ORG) mentioned in the article\n",
    "    \"\"\"\n",
    "    \n",
    "    ner_results = nlp(article_text)\n",
    "    locations = set([(X['word'],X['entity_group']) for X in ner_results if X['entity_group'] == 'LOC'])\n",
    "    orgs = set([(X['word'], X['entity_group']) for X in ner_results if X['entity_group'] == 'ORG'])\n",
    "\n",
    "    return locations, orgs\n",
    "\n",
    "def get_snippet(sentences, num_sent, lede=True, remaining_text=False):\n",
    "    \"\"\"\n",
    "    get the snippet of text from the article_text, replace single quotes\n",
    "    input: article text, and num_sent - number of sentences to return, default lede is true will return first x sentences\n",
    "           reamaining_text then must be False \n",
    "    returns: first x (num_sent) sentences\n",
    "    \"\"\"\n",
    "    #clean_text = clean_article_text(text)\n",
    "    #clean_text = \". \".join(clean_text.split(\".\")) # adding a space after period so nltk can do a better job recognizing sentences\n",
    "    #lede = nltk.sent_tokenize(clean_text)[:num_sent] # returns a list\n",
    "    \n",
    "    if lede: # get the first num_sent \n",
    "        lede_text = sentences[:num_sent]\n",
    "        result_text = \" \".join(lede_text)\n",
    "    elif remaining_text: # get rest of article num_sent * 2 until the end\n",
    "        result_text = sentences[num_sent*2:]\n",
    "        result_text = \" \".join(result_text)\n",
    "    else: # get sentences num_sent to num_sent * 2\n",
    "       result_text = sentences[num_sent:num_sent*2]\n",
    "       result_text = \" \".join(result_text) \n",
    "    \n",
    "    singleq = result_text.replace('’', \"'\")\n",
    "\n",
    "    return singleq\n",
    "\n",
    "def get_sentences(text):\n",
    "    # return article text as a list of its sentences \n",
    "\n",
    "    clean_text = clean_article_text(text)\n",
    "    clean_text = \". \".join(clean_text.split(\".\")) # adding a space after period so nltk can do a better job recognizing sentences\n",
    "    sentences = nltk.sent_tokenize(clean_text)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def clean_article_text(text):\n",
    "    # get text, removing html tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    clean_text = soup.get_text()\n",
    "    return clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88cab79f-b615-4aff-980b-d9aec7b6a2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location_geocode(API_KEY, locations):\n",
    "    \"\"\"\n",
    "    getting coordinates from location names in articles \n",
    "    input: google maps platform API KEY, locations article \n",
    "    return: dictionary of location names (key) with coordinates (value as a dictionary with lat and lon as keys)\n",
    "    \"\"\"\n",
    "    gmaps = googlemaps.Client(key=API_KEY)\n",
    "    results = {}\n",
    "\n",
    "    # getting coordinates\n",
    "    for place in locations:\n",
    "        # we can constrain google geocode api search to massachusetts or us - census geocoder will not work for places outside of U.S \n",
    "        #geocode_result = gmaps.geocode(place[0] + \", Suffok County, MA, USA\") # place is a tuple, where first value is the location name \n",
    "        geocode_result = gmaps.geocode(place[0] + \", Suffolk County\",  components={\"administrative_area_level\": \"MA\", \n",
    "                                                                                   \"country\": \"US\"})\n",
    "        #print(geocode_result)\n",
    "        #print()\n",
    "        temp = {}\n",
    "        try:\n",
    "            geocode_components = geocode_result[0]['address_components']\n",
    "            for i, addr_comp in enumerate(geocode_components):\n",
    "                if 'administrative_area_level_2' in addr_comp['types']:\n",
    "                    if \"Suffolk County\" == addr_comp['short_name'] and i != 0:\n",
    "                        temp['lat'] = geocode_result[0]['geometry']['location']['lat']\n",
    "                        temp['lon'] = geocode_result[0]['geometry']['location']['lng']\n",
    "                        results[place[0]] = temp\n",
    "                    \"\"\"\n",
    "                    else: # location outside of boston \n",
    "                        geocode_result = gmaps.geocode(place[0],  components={\"administrative_area_level\": \"MA\", \n",
    "                                                                                   \"country\": \"US\"})\n",
    "                        new_loc = {}\n",
    "                        temp['lat'] = geocode_result[0]['geometry']['location']['lat']\n",
    "                        temp['lon'] = geocode_result[0]['geometry']['location']['lng']\n",
    "                        new_loc[place[0]] = temp\n",
    "                        #save_geocodes(new_loc, False)\n",
    "                        print(geocode_result)\n",
    "                        print(place[0])\n",
    "                    \"\"\"\n",
    "                        \n",
    "        except IndexError: # unable to get coordinates for location\n",
    "            print(\"Unable to locate \" + place[0])\n",
    "\n",
    "    return results \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e375baa-8b84-4a71-b0ad-f51e49087f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_census_geos(geocode_results):\n",
    "    \"\"\"\n",
    "    get census geographies - tract, block group, block by coordinates\n",
    "    input: google maps geocode_results as a dictionary\n",
    "    return: block, block_group, tract, county for each location\n",
    "    \"\"\"\n",
    "    census_geos = {}\n",
    "    for place in geocode_results:\n",
    "        # building the geocoding url\n",
    "        base_url = f'https://geocoding.geo.census.gov/geocoder/geographies/coordinates?'\n",
    "        survey_ver = f'&benchmark=4&vintage=4&layers=2020 Census Blocks&format=json'\n",
    "        lon = geocode_results[place]['lon']\n",
    "        lat = geocode_results[place]['lat']\n",
    "        census_geo_url = f'{base_url}x={lon}&y={lat}{survey_ver}'\n",
    "\n",
    "        # getting the census geographies \n",
    "        response = requests.get(census_geo_url)\n",
    "        response_json = response.json()\n",
    "\n",
    "        try:\n",
    "            block = response_json['result']['geographies']['2020 Census Blocks'][0]['BLOCK']\n",
    "            block_group = response_json['result']['geographies']['2020 Census Blocks'][0]['BLKGRP']\n",
    "            tract = response_json['result']['geographies']['2020 Census Blocks'][0]['TRACT']\n",
    "            county = response_json['result']['geographies']['2020 Census Blocks'][0]['COUNTY']\n",
    "            census_geos[place] = {'block': block,\n",
    "                                  'blkgrp': block_group,\n",
    "                                  'tract': tract,\n",
    "                                  'county': county}\n",
    "        except IndexError:\n",
    "            print(\"Unable to retrieve census geography for: \" + place)\n",
    "        except KeyError:\n",
    "            print(\"Location is outside of the United States: \" + place)\n",
    "    return census_geos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50738030-4fff-40d2-9b91-b5a193632d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_geocodes(new_data, boston=True):\n",
    "    # save new geocodes to json file \n",
    "    # one for boston locations \n",
    "    # one for locations outside of boston\n",
    "    \n",
    "    if boston:\n",
    "        filename = \"./saved-geocodes.json\"\n",
    "        with open(filename, 'r+') as f:\n",
    "            # load existing data \n",
    "            file_data = json.load(f)\n",
    "            for name in new_data:\n",
    "                file_data[name] = new_data[name]\n",
    "            f.seek(0)\n",
    "            # convert back to json\n",
    "            json.dump(file_data, f, indent=4)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1daa5bd9-60b2-4b81-8893-1c30b5ebea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# CENSUS DATA API \n",
    "# https://www.census.gov/content/dam/Census/library/publications/2020/acs/acs_api_handbook_2020_ch02.pdf\n",
    "# any user can query small quantities of data with minimal restrictions - up to 50 variables in a single query, up to 500 queries per IP address per day \n",
    "# more than 500 queries per IP address per day requires you to register for API key - www.census.gov/developers\n",
    "# https://www.census.gov/data/developers/data-sets/decennial-census.html \n",
    "\"\"\"\n",
    "def get_census_demographics(year, dsource, dname, tract, county, state):\n",
    "    # input: census year, data source, survey name, tract, county, state\n",
    "    # return: demographic data for tract mentioned\n",
    "    \n",
    "    # census variables: https://api.census.gov/data/2020/dec/pl/variables.html \n",
    "    cols = 'NAME,P2_001N,P2_002N,P2_003N,P2_004N,P2_005N,P2_006N,P2_007N,P2_008N,P2_009N,P2_010N'\n",
    "    base_url = f\"https://api.census.gov/data/{year}/{dsource}/{dname}\"\n",
    "\n",
    "    # to get tract demographics \n",
    "    census_url = f\"{base_url}?get={cols}&for=tract:{tract}&in=county:{county}&in=state:{state}\"\n",
    "\n",
    "    # to get block demographics \n",
    "    # census_url = f\"{base_url}?get={cols}&for=block:{block}&in=tract:{tract}&in=county:{county}&in=state:{state}\"\n",
    "\n",
    "    census_response = requests.get(census_url)\n",
    "    census_response_json = census_response.json()\n",
    "\n",
    "    return census_response_json\n",
    "\n",
    "def clean_entity_results(extracted_loc, extracted_orgs, drop_geos):\n",
    "    # cleaning extracted entities from bert \n",
    "    # removing state names, and mass town names since the demographics data is too broad\n",
    "    # return cleaned set of entities\n",
    "    entity_result = extracted_loc | extracted_orgs\n",
    "\n",
    "    for tup in extracted_loc | extracted_orgs:\n",
    "        if len(tup[0]) <= 1:\n",
    "            entity_result.remove(tup)\n",
    "        elif tup in drop_geos.state_entities:\n",
    "            entity_result.remove(tup)\n",
    "        elif tup in drop_geos.mass_town_entities:\n",
    "            entity_result.remove(tup)\n",
    "        elif tup in drop_geos.org_entities:\n",
    "            entity_result.remove(tup)\n",
    "    return entity_result\n",
    "\n",
    "def remove_existing_geocodes(entity_result, saved_geocodes):\n",
    "    # check if any locations or organizations were recognized\n",
    "    # check if the geocodes already exist in dictionary\n",
    "    existing_loc_geocode = {}\n",
    "    new_loc_geocode = set()\n",
    "    for ent in entity_result:\n",
    "        try:\n",
    "            existing_loc_geocode[ent[0]] = saved_geocodes[ent[0]]\n",
    "        except KeyError:\n",
    "            new_loc_geocode.add(ent)\n",
    "    return existing_loc_geocode, new_loc_geocode\n",
    "\n",
    "def run_entity_recognition(text, nlp, drop_geos, saved_geocodes):\n",
    "    # running entity recogntion on text\n",
    "    # parse existing geocoded entities and new geocoded entities\n",
    "    try:\n",
    "        extracted_loc, extracted_orgs = get_locations_bert(text, nlp)\n",
    "        print(\"Extracted_locs:\", extracted_loc)\n",
    "        ent_result = clean_entity_results(extracted_loc, extracted_orgs, drop_geos)\n",
    "        existing_loc_geocode, new_loc_geocode = remove_existing_geocodes(ent_result, saved_geocodes)\n",
    "    except TypeError as e:\n",
    "        print(\"No entities\")\n",
    "        existing_loc_geocode = {}\n",
    "        new_loc_geocode = set()\n",
    "\n",
    "    return existing_loc_geocode, new_loc_geocode\n",
    "\n",
    "def run_location_geocode(API_KEY, new_loc_geocode):\n",
    "    # get geocodes for NEW locations and saving them to json\n",
    "    # returns new location geocodes as dictionary \n",
    "    location_geocode = {}\n",
    "    if new_loc_geocode:\n",
    "        location_geocode = get_location_geocode(API_KEY, new_loc_geocode)\n",
    "        save_geocodes(location_geocode)\n",
    "    return location_geocode\n",
    "\n",
    "def check_snippets(API_KEY, new_entities, existing_entities):\n",
    "    location_geocode = run_location_geocode(API_KEY, new_entities)\n",
    "    existing_loc_geocode = existing_entities\n",
    "    combined_geocodes = location_geocode | existing_loc_geocode # if this is empty, then try the next snippet of text \n",
    "    return (not combined_geocodes), location_geocode, existing_loc_geocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23e9fb62-809b-4a59-bd40-fc9cedb49b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(year, dsource, dname, state, existing_loc_geocode, location_geocode, mappings):\n",
    "    #location_geocode = {'Boston': {'lat': 42.3600825, 'lon': -71.0588801}, 'Massachusetts': {'lat': 42.4072107, 'lon': -71.3824374}, 'Boston city': {'lat': 42.3600825, 'lon': -71.0588801}, 'Roxbury': {'lat': 42.3125672, 'lon': -71.0898796}, 'Fitchburg': {'lat': 42.5834228, 'lon': -71.8022955}, 'Medford': {'lat': 42.4184296, 'lon': -71.1061639}}\n",
    "    #location_geocode = {'Massachusetts': {'lat': 42.4072107, 'lon': -71.3824374}, 'Salem': {'lat': 42.5197473, 'lon': -70.8954626}, 'Salem City Hall': {'lat': 42.5218851, 'lon': -70.8956157}}\n",
    "    #location_geocode = {'Salem': {'lat': 42.5197473, 'lon': -70.8954626}, 'Massachusetts': {'lat': 42.4072107, 'lon': -71.3824374}, 'Salem City Hall': {'lat': 42.5218851, 'lon': -70.8956157}}\n",
    "\n",
    "    print(location_geocode | existing_loc_geocode)\n",
    "    \n",
    "    census_geos = get_census_geos(location_geocode | existing_loc_geocode)\n",
    "\n",
    "    result = []\n",
    "    for place_name in census_geos:\n",
    "        place_info = {}\n",
    "        county = census_geos[place_name]['county']\n",
    "        tract = census_geos[place_name]['tract']\n",
    "        \n",
    "        try:\n",
    "            demographic_results = get_census_demographics(year, dsource, dname, tract, county, state)\n",
    "\n",
    "            # build result dictionary \n",
    "            place_info[place_name] = {'county_code': county} \n",
    "            place_info[place_name] = {'county_name': demographic_results[1][0]}\n",
    "            place_info[place_name]['tract'] = tract\n",
    "            geoid_tract = state + county + tract # this includes the state and county and tract number\n",
    "            place_info[place_name]['geoid_tract'] = geoid_tract\n",
    "\n",
    "            if mappings.tract_mapping.get(geoid_tract): # get corresponding boston neighborhood \n",
    "                place_info[place_name]['neighborhood'] = mappings.tract_mapping[state + county + tract]\n",
    "            \n",
    "            place_info[place_name]['demographics'] = {\n",
    "                'p2_001n': demographic_results[1][1], # total population \n",
    "                'p2_002n': demographic_results[1][2], # total hispanic or latino \n",
    "                'p2_003n': demographic_results[1][3], # total not hispanic or latino \n",
    "                'p2_004n': demographic_results[1][4], # total not hispanic or latino - pop of one race\n",
    "                'p2_005n': demographic_results[1][5], # total not hispanic or latino - pop of one race - white alone \n",
    "                'p2_006n': demographic_results[1][6], # total not hispanic or latino - pop of one race - black or african american alone\n",
    "                'p2_007n': demographic_results[1][7], # total not hispanic or latino - pop of one race - american indian and alaska native alone\n",
    "                'p2_008n': demographic_results[1][8], # total not hispanic or latino - pop of one race - asian alone \n",
    "                'p2_009n': demographic_results[1][9], # total not hispanic or latino - pop of one race - native hawaiian and other pacific islander alone\n",
    "                'p2_010n': demographic_results[1][10] # total not hispanic or latino - pop of one race - some other race alone \n",
    "            } \n",
    "            \n",
    "            result.append(place_info)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Unable to get census demographics for: \" + place_name)\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d44ece4-6e78-43ef-8393-24131cfe05df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Label</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Byline</th>\n",
       "      <th>Section Navigation</th>\n",
       "      <th>Section</th>\n",
       "      <th>Tagging</th>\n",
       "      <th>Title</th>\n",
       "      <th>Paths</th>\n",
       "      <th>Publish Date</th>\n",
       "      <th>Has Path?</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Article</td>\n",
       "      <td>A Celtic Playlist For Easter</td>\n",
       "      <td>A Celtic Playlist For Easter</td>\n",
       "      <td>Brian O'Donovan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Celtic</td>\n",
       "      <td>0000016a-3bcb-d661-af7b-7bff0d200001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/music/celtic/2019/04/20/a-celtic-playlist-for...</td>\n",
       "      <td>Mon Mar 29 15:22:35 EDT 2021</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>The above is a continuous stream.  &lt;br/&gt;&lt;br/&gt;I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Article</td>\n",
       "      <td>Songs Of War And Remembrance: Memorial Day</td>\n",
       "      <td>Songs Of War And Remembrance: Memorial Day</td>\n",
       "      <td>Brian O'Donovan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Celtic</td>\n",
       "      <td>0000016a-f0d5-dbfd-a56f-f4dfc34c0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/music/celtic/2019/05/25/songs-of-war-and-reme...</td>\n",
       "      <td>Sat May 29 01:00:45 EDT 2021</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Click above for the audio of a special segment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Article</td>\n",
       "      <td>Words And Music: Father's Day</td>\n",
       "      <td>Words And Music: Father's Day</td>\n",
       "      <td>Brian O'Donovan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Celtic</td>\n",
       "      <td>0000016b-539d-d757-adef-f7bd4a5f0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/music/celtic/2019/06/13/words-and-music-fathe...</td>\n",
       "      <td>Fri Jun 18 01:00:28 EDT 2021</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>In honor of Father&amp;#39;s Day, this segment of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Article</td>\n",
       "      <td>Celebrating The Birthday Of Robert Burns — Jan...</td>\n",
       "      <td>Celebrating The Birthday Of Robert Burns — Jan...</td>\n",
       "      <td>Brian O'Donovan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Celtic</td>\n",
       "      <td>0000016f-c48a-d14c-a57f-e59b02310001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/music/celtic/2020/01/21/celebrating-the-birth...</td>\n",
       "      <td>Sat Jan 23 08:47:27 EST 2021</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Robert Burns is known as &amp;quot;Scotland&amp;#39;s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Article</td>\n",
       "      <td>Ten Celtic Love Songs For St. Valentine's Day</td>\n",
       "      <td>Ten Celtic Love Songs For St. Valentine's Day</td>\n",
       "      <td>Brian O'Donovan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Celtic</td>\n",
       "      <td>00000170-273a-d4d2-a378-677e83f60001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/celtic/ValentinesDay (Permalink)</td>\n",
       "      <td>Wed Feb 10 12:09:13 EST 2021</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Folk music generally, and Celtic music, in par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12790</th>\n",
       "      <td>Article</td>\n",
       "      <td>5 key takeaways from the Trump indictment news</td>\n",
       "      <td>5 key takeaways from the Trump indictment news</td>\n",
       "      <td>Emily Olson, Emma Bowman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>National News</td>\n",
       "      <td>00000187-3753-da17-afc7-f77b1ea40001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/national-news/2023/03/31/5-key-takeaways-from...</td>\n",
       "      <td>Fri Mar 31 06:26:00 EDT 2023</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Former president Donald Trump has been indicte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12791</th>\n",
       "      <td>Article</td>\n",
       "      <td>Harvard professor says government should pause...</td>\n",
       "      <td>Harvard professor says government should pause...</td>\n",
       "      <td>Alexi Cohan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Science and Technology</td>\n",
       "      <td>00000187-37f6-d65f-a1f7-bffee1410001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/science-and-technology/2023/03/31/harvard-pro...</td>\n",
       "      <td>Fri Mar 31 11:06:14 EDT 2023</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Artificial intelligence has advanced rapidly i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12792</th>\n",
       "      <td>Article</td>\n",
       "      <td>Why Trump isn't the first president to face ar...</td>\n",
       "      <td>Why Trump isn't the first president to face ar...</td>\n",
       "      <td>Dustin Jones, Kaitlyn Radde</td>\n",
       "      <td>NaN</td>\n",
       "      <td>National News</td>\n",
       "      <td>00000187-37f7-da17-afc7-f7ffec2a0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/national-news/2023/03/31/why-trump-isnt-the-f...</td>\n",
       "      <td>Fri Mar 31 09:05:00 EDT 2023</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Former President Donald Trump was indicted Thu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12793</th>\n",
       "      <td>Article</td>\n",
       "      <td>These cockroaches tweaked their mating rituals...</td>\n",
       "      <td>These cockroaches tweaked their mating rituals...</td>\n",
       "      <td>Ari Daniel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>News</td>\n",
       "      <td>00000187-382e-da17-afc7-f96fcf4c0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/news/2023/03/31/these-cockroaches-tweaked-the...</td>\n",
       "      <td>Fri Mar 31 10:02:00 EDT 2023</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Human attempts to kill cockroaches with sugary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12794</th>\n",
       "      <td>Article</td>\n",
       "      <td>U.S. Capitol rioter the 'QAnon Shaman' is rele...</td>\n",
       "      <td>U.S. Capitol rioter the 'QAnon Shaman' is rele...</td>\n",
       "      <td>Juliana Kim</td>\n",
       "      <td>NaN</td>\n",
       "      <td>National News</td>\n",
       "      <td>00000187-3865-da17-afc7-f96fb8210001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/national-news/2023/03/31/u-s-capitol-rioter-t...</td>\n",
       "      <td>Fri Mar 31 11:19:00 EDT 2023</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Jacob Chansley, who received one of the longes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12795 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Type                                              Label  \\\n",
       "0      Article                      A Celtic Playlist For Easter    \n",
       "1      Article        Songs Of War And Remembrance: Memorial Day    \n",
       "2      Article                      Words And Music: Father's Day   \n",
       "3      Article  Celebrating The Birthday Of Robert Burns — Jan...   \n",
       "4      Article      Ten Celtic Love Songs For St. Valentine's Day   \n",
       "...        ...                                                ...   \n",
       "12790  Article     5 key takeaways from the Trump indictment news   \n",
       "12791  Article  Harvard professor says government should pause...   \n",
       "12792  Article  Why Trump isn't the first president to face ar...   \n",
       "12793  Article  These cockroaches tweaked their mating rituals...   \n",
       "12794  Article  U.S. Capitol rioter the 'QAnon Shaman' is rele...   \n",
       "\n",
       "                                                Headline  \\\n",
       "0                          A Celtic Playlist For Easter    \n",
       "1            Songs Of War And Remembrance: Memorial Day    \n",
       "2                          Words And Music: Father's Day   \n",
       "3      Celebrating The Birthday Of Robert Burns — Jan...   \n",
       "4          Ten Celtic Love Songs For St. Valentine's Day   \n",
       "...                                                  ...   \n",
       "12790     5 key takeaways from the Trump indictment news   \n",
       "12791  Harvard professor says government should pause...   \n",
       "12792  Why Trump isn't the first president to face ar...   \n",
       "12793  These cockroaches tweaked their mating rituals...   \n",
       "12794  U.S. Capitol rioter the 'QAnon Shaman' is rele...   \n",
       "\n",
       "                            Byline Section Navigation                 Section  \\\n",
       "0                  Brian O'Donovan                NaN                  Celtic   \n",
       "1                  Brian O'Donovan                NaN                  Celtic   \n",
       "2                  Brian O'Donovan                NaN                  Celtic   \n",
       "3                  Brian O'Donovan                NaN                  Celtic   \n",
       "4                  Brian O'Donovan                NaN                  Celtic   \n",
       "...                            ...                ...                     ...   \n",
       "12790     Emily Olson, Emma Bowman                NaN           National News   \n",
       "12791                  Alexi Cohan                NaN  Science and Technology   \n",
       "12792  Dustin Jones, Kaitlyn Radde                NaN           National News   \n",
       "12793                   Ari Daniel                NaN                    News   \n",
       "12794                  Juliana Kim                NaN           National News   \n",
       "\n",
       "                                    Tagging Title  \\\n",
       "0      0000016a-3bcb-d661-af7b-7bff0d200001   NaN   \n",
       "1      0000016a-f0d5-dbfd-a56f-f4dfc34c0001   NaN   \n",
       "2      0000016b-539d-d757-adef-f7bd4a5f0001   NaN   \n",
       "3      0000016f-c48a-d14c-a57f-e59b02310001   NaN   \n",
       "4      00000170-273a-d4d2-a378-677e83f60001   NaN   \n",
       "...                                     ...   ...   \n",
       "12790  00000187-3753-da17-afc7-f77b1ea40001   NaN   \n",
       "12791  00000187-37f6-d65f-a1f7-bffee1410001   NaN   \n",
       "12792  00000187-37f7-da17-afc7-f7ffec2a0001   NaN   \n",
       "12793  00000187-382e-da17-afc7-f96fcf4c0001   NaN   \n",
       "12794  00000187-3865-da17-afc7-f96fb8210001   NaN   \n",
       "\n",
       "                                                   Paths  \\\n",
       "0      /music/celtic/2019/04/20/a-celtic-playlist-for...   \n",
       "1      /music/celtic/2019/05/25/songs-of-war-and-reme...   \n",
       "2      /music/celtic/2019/06/13/words-and-music-fathe...   \n",
       "3      /music/celtic/2020/01/21/celebrating-the-birth...   \n",
       "4                      /celtic/ValentinesDay (Permalink)   \n",
       "...                                                  ...   \n",
       "12790  /national-news/2023/03/31/5-key-takeaways-from...   \n",
       "12791  /science-and-technology/2023/03/31/harvard-pro...   \n",
       "12792  /national-news/2023/03/31/why-trump-isnt-the-f...   \n",
       "12793  /news/2023/03/31/these-cockroaches-tweaked-the...   \n",
       "12794  /national-news/2023/03/31/u-s-capitol-rioter-t...   \n",
       "\n",
       "                       Publish Date Has Path?  \\\n",
       "0      Mon Mar 29 15:22:35 EDT 2021      TRUE   \n",
       "1      Sat May 29 01:00:45 EDT 2021      TRUE   \n",
       "2      Fri Jun 18 01:00:28 EDT 2021      TRUE   \n",
       "3      Sat Jan 23 08:47:27 EST 2021      TRUE   \n",
       "4      Wed Feb 10 12:09:13 EST 2021      TRUE   \n",
       "...                             ...       ...   \n",
       "12790  Fri Mar 31 06:26:00 EDT 2023      TRUE   \n",
       "12791  Fri Mar 31 11:06:14 EDT 2023      TRUE   \n",
       "12792  Fri Mar 31 09:05:00 EDT 2023      TRUE   \n",
       "12793  Fri Mar 31 10:02:00 EDT 2023      TRUE   \n",
       "12794  Fri Mar 31 11:19:00 EDT 2023      TRUE   \n",
       "\n",
       "                                                    Body  \n",
       "0      The above is a continuous stream.  <br/><br/>I...  \n",
       "1      Click above for the audio of a special segment...  \n",
       "2      In honor of Father&#39;s Day, this segment of ...  \n",
       "3      Robert Burns is known as &quot;Scotland&#39;s ...  \n",
       "4      Folk music generally, and Celtic music, in par...  \n",
       "...                                                  ...  \n",
       "12790  Former president Donald Trump has been indicte...  \n",
       "12791  Artificial intelligence has advanced rapidly i...  \n",
       "12792  Former President Donald Trump was indicted Thu...  \n",
       "12793  Human attempts to kill cockroaches with sugary...  \n",
       "12794  Jacob Chansley, who received one of the longes...  \n",
       "\n",
       "[12795 rows x 12 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./Articles Nov 2020 - March 2023.csv\", low_memory=False)\n",
    "df = df.iloc[:, : 12]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fe349e1-8a7a-40fd-9504-08187543f6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Celtic', 'Programs', nan,\n",
       "       '&quot;cms.content.publishUser&quot;:{&quot;_ref&quot;:&quot;00000160-dc6b-dd0e-abfa-fc7bd7c80000&quot;',\n",
       "       ' I’m just here because it’s better than hanging out in my hotel room. I’m here to visit my sister.<br/><br/>Reader',\n",
       "       'Education', 'Politics', 'Specials', 'Digital Mural', 'Local News',\n",
       "       'Commentary', 'Arts & Culture', 'International News', 'News',\n",
       "       'National News', 'Arts', 'Front Row Boston', 'Local Music',\n",
       "       'Science and Technology', ' however', 'Dining In', ' in fact',\n",
       "       'Lifestyle', 'GBH Events', ' it makes us look petty',\n",
       "       'Boston Public Library Studio', 'Support',\n",
       "       ' Barnaby Race &amp; Nathan Tysen', 'WATCH HIGH SCHOOL QUIZ SHOW',\n",
       "       ' so',\n",
       "       ' I’m gonna. Clergy should marry to be a good example. It’ll make me happy. And also',\n",
       "       ' weirdly flirty: </b>So',\n",
       "       ' and we’re treated to sweeping vistas that will do nicely if you’re looking for a picture to put next to &quot;Romanticism&quot; in the dictionary. <br/><br/><b>Gardinerette: </b>And we’re close to my hometown',\n",
       "       ' popping up from behind the garden wall:</b> So he’s gone? What a relief! Oh snap',\n",
       "       ' we should do it again some time!<br/><b>GirlBoss:</b> Look',\n",
       "       ' Ringmaster Grumpy',\n",
       "       ' like those working in <u><a href=\"\"https://www.wgbh.org/news/local-news/2020/05/14/new-bedford-mayor-jon-mitchell-for-an-industrial-city-zoom-doesnt-do-you-much-good\"\" target=\"\"_blank\"\">industrial settings</a></u>. On top of work calls',\n",
       "       ' gave a short sermon. <br/><br/>Dean has presided so far over ten funerals for people who died from the virus',\n",
       "       'Help',\n",
       "       '&quot;_type&quot;:&quot;0000015f-fed7-db18-a37f-fef72a090000&quot;}',\n",
       "       ' she runs', '&quot;cms.content.publishDate&quot;:1611773188630',\n",
       "       '&quot;cms.content.updateDate&quot;:1612212428045',\n",
       "       '&quot;theme.default.:core:enhancement:Enhancement.hbs._template&quot;:null',\n",
       "       '&quot;_type&quot;:&quot;0000015f-fed7-db18-a37f-fef72a0a0001&quot;}\"\">A man laughs so hard he takes off his glasses to wipe his eyes</bsp-image>Sure',\n",
       "       ' and she a poor woman', '&quot;hideCaption&quot;:false',\n",
       "       ' now totals 36',\n",
       "       ' they’ll work for me longer. <br/><b>Country Dad:</b> Yeah',\n",
       "       ' but surely you can understand that he’s reacting to the situation. <br/><b>Random Factory Owner:</b> If he’s so invested in the strike I’m surprised he accepted your charity.<br/><b>Mags:</b> It wasn’t for him',\n",
       "       ' Buffalo Philharmonic Orchestra &amp; Buffalo Philharmonic Chorus)<br/><ul><li>Gershwin: <i>Porgy and Bess</i></li></ul>David Frost &amp; John Kerswell',\n",
       "       'Jazz',\n",
       "       ' so you know I mean it. <br/><b>Country Mom:</b> God bless.<br/><b>Ol’ Battleaxe:</b> <bsp-image data-state=\"\"{&quot;cms.site.owner&quot;:{&quot;_ref&quot;:&quot;00000160-0889-d9cd-ab69-3d9d91cd0000&quot;',\n",
       "       ' but my snooty aunt was trying to hustle me out of here. I’m really gonna miss you two. Please',\n",
       "       '&quot;_type&quot;:&quot;0000015f-fed7-db18-a37f-fef72a320004&quot;}\"\">Instagram</bsp-instagram><br/><b>Calvin Zabo — <i>Agents of S.H.I.E.L.D</i></b><br/>While MacLachlan turned <i>Showgirls’</i> sleezy Zack Carey into a truly reprehensible character',\n",
       "       ' whether motivated by a criminal element or motivated by geopolitical conditions',\n",
       "       '&quot;_type&quot;:&quot;0000015f-fed7-db18-a37f-fef72a4c0002&quot;}',\n",
       "       ' Healthy Acadia',\n",
       "       ' let’s go back in and party. <br/><br/>And party they do. Knock-Off Jack meets the young fella Third Wheel had been chatting to',\n",
       "       ' his third', '.org Homepage',\n",
       "       ' and he’s not in a good mood. He parks the van full of dodgy cargo where requested',\n",
       "       '&quot;cms.content.updateDate&quot;:1626967483705',\n",
       "       '&quot;cms.content.updateDate&quot;:1626730521212',\n",
       "       ' he’s right. There’s nothing concrete',\n",
       "       '&quot;_id&quot;:&quot;0000017b-0858-d5be-a57b-bdd827a30000&quot;',\n",
       "       ' unfortunately',\n",
       "       ' but you gave me hope again. I want you to stay.<br/><b>L’Americain:</b> Yup. I’m staying.<br/><br/>Oh',\n",
       "       ' is eating dinner. Maybe Rear Window’s not a great cook',\n",
       "       '&quot;_type&quot;:&quot;0000015f-fed7-db18-a37f-fef72a4e0001&quot;}',\n",
       "       ' of course',\n",
       "       '&quot;_type&quot;:&quot;0000015f-fed7-db18-a37f-fef72a0a0003&quot;}',\n",
       "       ' interrupted by her mom', ' pictured left',\n",
       "       ' and we’ll see many more in the future unless the law changes.<br/><b>Boots',\n",
       "       ' a jerk. We left things in a bad place yesterday and I want to apologize.<br/><b>Trixie:</b> To the people who live in Miserable Disaster Apartments? Because they’re the people who deserve your apology',\n",
       "       ' Jones said.<br/><br/>“We would urge them to register',\n",
       "       ' and the revolutionaries return the volley. Upstairs',\n",
       "       '&quot;image&quot;:{&quot;image&quot;:{&quot;_ref&quot;:&quot;0000017e-59e2-db0e-ab7f-79e2890e0001&quot;',\n",
       "       '&quot;cms.content.updateDate&quot;:1642795091579',\n",
       "       ' but yikes: this is not a classically good situation! Hilariously',\n",
       "       ' he’ll ask! All in good time!<br/><b>M. Master Key: </b>We. Don’t. Have. Time. <br/><b>Customs Officer:</b> Look',\n",
       "       ' but it can take years to find the answer.<br/><b>Phyllis:</b> Can I sit with her while we wait for her lawyer at least? It might keep her calm.<br/><b>Proto-Cassie Stuart:</b> Yes',\n",
       "       ' and that she’s very good at childbirth.<br/><br/><b>Lucille:</b> You are brave and resourceful. No one is going to take this baby away. Contrite Teen',\n",
       "       '&quot;theme.default.:core:figure:Figure.hbs._template&quot;:null',\n",
       "       ' at Nonnatus',\n",
       "       ' dude. Dad can’t afford for us to stay at home.<br/><b>Charlotte:</b> Aha',\n",
       "       '&quot;captionOverride&quot;:&quot;&quot;',\n",
       "       ' but it&#39;s an expression of approval.</bsp-image>And then',\n",
       "       ' you’re literally at a ball',\n",
       "       '&quot;cms.content.updateDate&quot;:1648664511267',\n",
       "       ' just now realizing the implications of what she’s agreed to:</b> A whole week! Wow.<br/><br/>I could not be happier for Phyllis! What a great opportunity! Outside the office',\n",
       "       '&quot;cms.content.updateDate&quot;:1650296101404',\n",
       "       ' saving the day:</b> What about Benedict?<br/><b>Loose Hips:</b> Perfect!<br/><br/>Look',\n",
       "       ' activist filmmaker Lauren Pespisa',\n",
       "       '&quot;_type&quot;:&quot;0000015f-fed7-db18-a37f-fef72a3c0002&quot;}\"\">Tweet</bsp-tweet><bsp-tweet data-state=\"\"{&quot;cms.site.owner&quot;:{&quot;_ref&quot;:&quot;00000160-2ca4-dda9-abf8-ffb4eba00002&quot;',\n",
       "       ' a polite young lad', ' Bright is',\n",
       "       ' they guy fell down the stairs disembarking!<br/><br/>Yikes',\n",
       "       ' as management has focused on returning riders to the system. “What we are left with is a very small amount of dollars that can be impacted by the T.”<br/><bsp-image data-state=\"\"{&quot;cms.site.owner&quot;:{&quot;_ref&quot;:&quot;00000160-2ca4-dda9-abf8-ffb4eba00002&quot;',\n",
       "       '&quot;cms.content.updateDate&quot;:1662644627407',\n",
       "       ' cutting to the chase:</b> I felt bad about what happened to your family.<br/><b>Dog Lady:</b> Not as bad as I did.<br/><b>Floral Mum:</b> I always said to Menacing Dad that it’d be you.<br/><b>Dog Lady:</b> I hope you get a chance to tell him that. <br/><b>Floral Mum',\n",
       "       ' and in this instance only', ' Artemis Huang',\n",
       "       ' simple as that. He also pretty quickly figures out that she’s asking because of Jacob’s whole “if she’d been in Amsterdam she’d be alive today” theory. No',\n",
       "       ' given what he’s been accused of I don’t want anyone knowing I have dealings with him. Looks like your date’s arrived.<br/><b>Hassell',\n",
       "       ' birth mark on his forehead.<br/><b>Dutch Insurance Salesman:</b> No clue who that is',\n",
       "       ' after making a token show of protest:</b> Yeah ok let’s do it.<br/><br/>Back at Scotland Yard',\n",
       "       '&quot;image&quot;:{&quot;image&quot;:{&quot;_ref&quot;:&quot;00000184-5842-dc09-a3a7-5f46cba40001&quot;',\n",
       "       ' at the morgue',\n",
       "       '&quot;cms.content.updateDate&quot;:1668985488662',\n",
       "       '&quot;_id&quot;:&quot;00000185-159a-d153-afcd-7fdf24390000&quot;',\n",
       "       '&quot;cms.content.publishUser&quot;:{&quot;_ref&quot;:&quot;0000017c-94bc-d208-a97c-dfff23d00000&quot;',\n",
       "       '&quot;cms.content.publishDate&quot;:1671641296466',\n",
       "       ' what those news stories are. We have to do those jobs. But then we also have to hear from the people that are impacted by these stories',\n",
       "       ' and you twisted what I said and made it mean!<br/><b>True Crime Reporter:</b> I run an old-timey British tabloid',\n",
       "       ' Victoria Mars goes to Arabella’s to say “I knew it! I knew you were up to no good!” in person. Arabella',\n",
       "       ' anyway? <br/><br/><b>Victoria Mars',\n",
       "       ' later that night Ivy returns home to find Victoria Mars attempting to repair her bag. And reader',\n",
       "       '&quot;_id&quot;:&quot;00000186-1969-d0c8-adc7-5ff98d4c0000&quot;',\n",
       "       ' nah: I like simple and small. It’s the unwrapping that I like; best part of Christmas!<br/><b>Duke Silver:</b> Good to know.<br/><b>Victoria Mars:</b> Arabella',\n",
       "       ' is really the responsibility of the director solely. That&#39;s the real delineation there.&quot;<br/><bsp-video data-state=\"\"{&quot;cms.site.owner&quot;:{&quot;_ref&quot;:&quot;00000160-2ca4-dda9-abf8-ffb4eba00002&quot;',\n",
       "       ' and she did another surgery which resulted in the altering of my entire life.”<br/><br/><i>Research for this story was launched in a Boston University College of Communication investigative journalism class. Contributors include then-students Hannah Green',\n",
       "       ' who are dismayed to learn that she will be leaving tomorrow',\n",
       "       ' Dastardly Duke tracks down Arthur to see how he’s doing.<br/><br/><b>Dastardly Duke:</b> You seem… down?<br/><b>Arthur:</b> <bsp-image data-state=\"\"{&quot;cms.content.publishDate&quot;:1679070760604'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Section'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "938b87a3-a42f-4bf9-a944-face2bf94498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-large-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#set variables \n",
    "year='2020'\n",
    "dsource='dec' # which survey are we interested in ? decennial \n",
    "dname='pl' # a dataset within a survey, pl - redistricting data \n",
    "state='25' # state code \n",
    "\n",
    "result = {}\n",
    "dumped_articles = []\n",
    "drop_geos = geography()\n",
    "mappings = neighborhood_mapping()\n",
    "saved_geocodes = drop_geos.saved_geocodes \n",
    "nlp = load_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "310e7734-ef9a-4943-a0e5-7463e4f74379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted_locs: {('Mass', 'LOC')}\n",
      "Extracted_locs: set()\n",
      "Extracted_locs: set()\n",
      "Extracted_locs: set()\n",
      "\n",
      "Prediction time: 6.803230047225952\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "ignore_article_types = [\"National News\", \"International News\", \"Programs\", \"Digital Mural\", \"Jazz\", \"Celtic\"]\n",
    "\n",
    "start = time.time()\n",
    "for idx in range(1500, 1508): #12,024 (- 500 articles -) 10,567 (- 500 articles -) 9,176 8,647 7810 7477 7303\n",
    "    # not running on articles in national or international categories because mentions will most likely be outside of boston\n",
    "    #if df['category'][idx] != \"National\" and df['category'][idx] != \"International News\":\n",
    "    if df['Section'][idx] not in ignore_article_types and df['Type'][idx] == 'Article':\n",
    "        headline = str(df['Label'][idx])\n",
    "        text = str(df['Body'][idx])\n",
    "        \n",
    "        sentences = get_sentences(text)\n",
    "\n",
    "        # get lede first 5 sentences, can change the number of sentences\n",
    "        text_5 = get_snippet(sentences, 5)\n",
    "        text_10 = get_snippet(sentences, 5, False) # get sentences 5-10\n",
    "        text_remain = get_snippet(sentences, 5, False, True)\n",
    "\n",
    "        run_entity_recognition(headline, nlp, drop_geos, saved_geocodes)\n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print('Prediction time:', str(end - start))\n",
    "\n",
    "        #get entities, returns existing entities that have been seen before and new entities as sets \n",
    "        check_order = [\n",
    "            (run_entity_recognition(headline, nlp, drop_geos, saved_geocodes), \"headline\"), \n",
    "            (run_entity_recognition(text_5, nlp, drop_geos, saved_geocodes), \"first 5 sentences\"), \n",
    "            (run_entity_recognition(text_10, nlp, drop_geos, saved_geocodes), \"next 5 sentences\"),\n",
    "            (run_entity_recognition(text_remain, nlp, drop_geos, saved_geocodes), \"remaining text\")\n",
    "        ]\n",
    "\n",
    "        for (entities, method) in check_order:\n",
    "            print(df['Tagging'][idx] + \" \" + method)\n",
    "            check_text, location_geocode, existing_loc_geocode = check_snippets(secret.API_KEY, entities[1], entities[0])\n",
    "            print(entities[1], entities[0])\n",
    "            if not check_text:\n",
    "                break \n",
    "\n",
    "        print(\"====== CONFIGURATIONS FOR TEMP ======\")\n",
    "        print(f\"existing_loc_geocode: {existing_loc_geocode}\")\n",
    "        print(f\"location_geocode: {location_geocode}\")\n",
    "        print()\n",
    "        \n",
    "        temp = run_pipeline(year, dsource, dname, state, existing_loc_geocode, location_geocode, mappings)\n",
    "\n",
    "        print()\n",
    "        print(f\"====== ARTICLE ======: {idx}\")\n",
    "        print(f\"Tag: {df['Tagging'][idx]}\") # Indexed by\n",
    "        print(f\"Author: {df['Byline'][idx]}\")\n",
    "        print(\"Body is Omitted\")\n",
    "        print(f\"content_id: {df['Tagging'][idx]}\")\n",
    "        print(f\"hl1: {clean_article_text(df['Label'][idx])}\")\n",
    "        print(f\"hl2: {clean_article_text(df['Headline'][idx])}\")\n",
    "        print(f\"topic: {df['Section'][idx]}\")\n",
    "        print(f\"pub_date: {df['Publish Date'][idx]}\")\n",
    "        print(\"pub_name: GBH\")\n",
    "        print(f\"link: {df['Paths'][idx]}\")\n",
    "        print(f\"census_tracts: {temp}\") # IMPORTANT!\n",
    "        print(f\"method: {method}\")\n",
    "        print(f\"ent_geocodes: {existing_loc_geocode | location_geocode}\")\n",
    "        print(\"===========\")\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        result[df['UID'][idx]] = {\"author\": \"\",\n",
    "                                  \"body\": clean_article_text(df['content'][idx]),\n",
    "                                  \"content_id\": df['UID'][idx],\n",
    "                                  \"hl1\": clean_article_text(df['title'][idx]),\n",
    "                                  \"hl2\": clean_article_text(df['description'][idx]),\n",
    "                                  \"meta\": {\"copyright\": \"\",\n",
    "                                           \"issue_number\": \"\", \n",
    "                                           \"volume\": \"\"},\n",
    "                                  \"topic\": df['category'][idx],\n",
    "                                  \"pub_date\": df['pubDate'][idx],\n",
    "                                  \"pub_name\": \"GBH\",\n",
    "                                  \"link\": df['link'][idx],\n",
    "                                  \"census_tracts\": temp,\n",
    "                                  \"method\": method,\n",
    "                                  \"ent_geocodes\": existing_loc_geocode | location_geocode}\n",
    "        \"\"\"\n",
    "        modified for gbh rss feed dump \n",
    "        with open('./gbh-sample-test.json', 'r+') as f:\n",
    "            # load existing data \n",
    "            file_data = json.load(f)\n",
    "            file_data[df['Tagging'][idx]] = {\"author\": df['Byline'][idx],\n",
    "                                             \"body\": clean_article_text(df['Body'][idx]),\n",
    "                                             \"content_id\": df['Tagging'][idx],\n",
    "                                             \"hl1\": clean_article_text(df['Label'][idx]),\n",
    "                                             \"hl2\": clean_article_text(df['Headline'][idx]),\n",
    "                                             \"meta\": {\"copyright\": \"\",\n",
    "                                                      \"issue_number\": \"\", \n",
    "                                                      \"volume\": \"\"},\n",
    "                                             \"topic\": df['Section'][idx],\n",
    "                                             \"pub_date\": df['Publish Date'][idx],\n",
    "                                             \"pub_name\": \"GBH\",\n",
    "                                             \"link\": df['Paths'][idx], # will need to edit this in postprocessing \n",
    "                                             \"census_tracts\": temp,\n",
    "                                             \"method\": method,\n",
    "                                             \"ent_geocodes\": existing_loc_geocode | location_geocode}\n",
    "            f.seek(0)\n",
    "            # convert back to json\n",
    "            json.dump(file_data, f, indent=4)\n",
    "    else:\n",
    "       dumped_articles.append(df['Tagging'][idx])\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e734fc-3380-4618-8945-4e9fc47f7365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80b8f0e-3f4d-4d40-9cf1-d8ebc20db1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d41cb85-877e-489b-ab63-a0b68e985b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29842d44-66ad-4d56-ad63-45ae1f366ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba57b0b9-b7e3-4649-806f-7f1aa5c67757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73541bf5-4004-4b44-a06e-a634edbccdbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b030fb2-3e10-415a-8c30-874b186a82a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6c4d01-97d7-4835-a5f3-3194ed972da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0967a5-d55f-42fa-984b-cca600e4c036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ec6e9f-2016-40ee-89ef-2e5a64e96c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef60c1d9-4908-4982-9c60-95113110b218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
